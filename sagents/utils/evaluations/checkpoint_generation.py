import json
from typing import List, Dict, Any

from .base_agent_processor import BaseAgentProcessor
from ..logger import logger

class CheckpointGenerationAgent(BaseAgentProcessor):
    def __init__(
        self,
        api_key: str,
        base_url: str,
    ):
        super().__init__(api_key, base_url)     

    async def task_generation(self, prompt: str, messages: List[Dict[str, Any]], model_name: str):
        messages.append({"role": "user", "content": prompt})
        response = await self.call_qianxun(messages, model_name=model_name)
        messages.append({"role": "assistant", "content": response})
        return response

    def system_prompt(self, system: str, messages: List[Dict[str, Any]]):
        messages.append({"role": "system", "content": system})
        
    async def workflow(self, user_messages: list, agent_config: str, tools_description: str, model_name: str):
        messages: List[Dict[str, Any]] = []

        self.system_prompt(system="""
你是一个专业的智能 Agent 能力测评专家，负责根据给定的 Agent 配置和可用工具集，为指定的多轮对话场景生成高质量的测试用例与结构化评估方案。

📌 核心评估原则（必须严格遵守）：
> **宁可少一个检查点，也不可多一个错误检查点。**
> - 所有生成的评估检查点必须 100% 确信其必要且正确。
> - 若某个步骤或判断存在不确定性（例如：Agent 可能通过其他路径达成目标），则不应生成该检查点。
> - 禁止基于猜测、假设或常见模式强行添加检查点。
> - 评估目标是**高精确率（Precision）优先**，而非高覆盖率。

📌 历史对话上下文理解：
> - 你会收到一个包含历史对话记录的完整上下文，其中包括用户的历史提问和助手的历史回复。
> - 历史对话中可能包含已执行的工具调用、已获取的数据结果、以及用户与Agent的交互过程。
> - 你需要理解整个对话的演进过程，识别上下文中的**关键信息**（如：已查询的数据、已明确的筛选条件、用户关注的焦点等）。
> - **最新用户提问可能会引用历史上下文**：如使用代词("这些客户"、"他们")、省略已说明的条件、或在历史基础上提出新要求。
> - 如果上下文只包含一轮对话，则不需要考虑历史上下文，本轮对话即为最新一轮用户提问。

📌 基于历史上下文的检查点生成原则：
> - **检查点仅针对最新一轮用户提问**：评估Agent如何回答最后一个用户问题。
> - **必须考虑历史上下文的影响**：
>   - 如果历史对话中已经获取了相关数据，Agent应能复用这些数据而非重新查询。
>   - 如果最新提问包含指代性表达，Agent必须正确理解其指向的历史实体或信息。
>   - 如果历史对话已明确某些参数（如时间范围、筛选条件），最新提问省略时应能正确继承。
> - **工具调用结果复用原则**：
>   - 如果历史对话中已调用的工具返回的数据仍然有效且满足最新提问的需求，Agent应直接复用，不应重复调用。
>   - 只有在以下情况才需要重新调用工具：
>     - 查询参数发生实质性变化（时间范围、筛选条件、查询对象不同）
>     - 历史数据不完整或不包含最新提问所需的字段
>     - 数据存在时效性要求且历史数据已过期

你的任务是基于完整的历史对话上下文，模拟一个理想 Agent 如何回答最新一轮用户提问，并据此构建一份**精确、可验证、无幻觉**的评分卡，用于后续对实际 Agent 的行为进行可信评测。
""", messages=messages)

        await self.task_generation(prompt=f"""
### 🔹 第一步：理解历史对话上下文并模拟理想Agent的回答过程

请基于以下信息，分析历史对话上下文，并模拟一个理想 Agent 如何回答**最新一轮用户提问**：

#### 【Agent 配置】
{agent_config}

#### 【可用工具详情】
{tools_description}

#### 【完整对话历史】（包含历史问答和最新提问）
{user_messages}

#### 【最新用户提问】
{user_messages[-1]}

请严格按照以下结构输出，不得省略任何部分：

1. **历史对话上下文分析**  
- 概述历史对话的主题和已经讨论的内容
- 识别历史对话中已经执行的工具调用及其返回的关键数据
- 分析历史对话中已经明确的参数、筛选条件或用户关注点
- 识别历史数据是否可以被最新提问复用

2. **最新提问理解**  
- 最新用户提问的核心意图是什么
- 该提问与历史对话的关系（独立新问题 / 基于历史上下文的延续 / 包含指代性表达）
- 如果包含指代（如"这些客户"、"他们"），明确指代的是历史对话中的哪些实体或数据
- 如果省略了某些条件，说明是否可以从历史对话中继承这些条件

3. **任务拆解逻辑**  
针对最新用户提问说明：
- 需要完成什么目标
- 需要分解为哪些逻辑步骤
- 每个步骤是否可以复用历史对话中的数据，还是需要重新调用工具
- 所有步骤必须依赖至少一个【可用工具】完成，禁止纯知识推理或静态回答

4. **所需调用的工具及参数说明**  
列出回答最新提问需要调用的工具：
- 工具名称、输入参数及其来源说明
- 参数来源类型包括：`最新用户输入`、`历史对话上下文`、`历史工具调用结果`、`系统配置`、`常量/默认值`
- **特别说明**：
  - 如果历史对话中已经调用过某工具且结果可以直接复用，说明为何不需要重复调用
  - 如果确需重新调用，明确说明原因（如：查询参数不同、历史数据不完整、数据已过期等）

5. **预期输出结果**  
说明回答最新提问的过程：
- 最终结论是如何从工具返回数据（或历史数据）中推导得出的
- 是否需要结合历史对话的结果进行综合分析
- 强调最终结论必须基于工具返回的真实数据或历史对话中已获取的数据，不能凭空生成

6. **预期输出形式**  
明确回答最新提问时：
- **查询返回的必要信息**应以何种形式呈现（表格 / 列表 / 文本描述）
- 允许整体响应为混合格式，但查询返回的必要信息必须满足指定结构要求
- 参考Agent_config的availableWorkflows字段信息

7. **模拟执行过程（针对最新提问）**  
按时间顺序描述回答最新提问的执行流程：

- 步骤1：分析历史对话上下文，提取相关信息 `{{...}}`
- 步骤2：（如需要）调用 `[工具名]`，参数为 `{{...}}`（说明参数来源），目的：解释该步骤目标
- 步骤3：（如需要）处理上一步返回结果，提取 `{{字段}}` 作为输入，调用 `[工具名]`，参数为 `{{...}}`
- 步骤4：基于工具返回数据（或历史数据）生成最终回复
- ……

> ⚠️ 重要约束：
> - **评估对象是针对最新一轮用户提问的回答**，不是评估整个历史对话。
> - 所有工具调用必须来自【可用工具详情】列表，禁止虚构工具。
> - 工具参数必须准确，来源清晰，特别注意区分是来自最新提问还是历史对话。
> - **优先复用历史数据**：如果历史对话中已经获取了所需数据且仍然有效，应直接复用，不应重复调用工具。
> - 只有在以下情况才需要重新调用工具：
>   - 查询参数发生实质性变化（时间范围、筛选条件、查询对象不同）
>   - 历史数据不完整或不包含最新提问所需的字段
>   - 数据存在时效性要求且历史数据已过期
> - 最终结论必须严格基于工具返回内容或历史对话中已获取的数据生成。
> - 生成的检查点可以少于实际步骤，但每个检查点必须准确无误。
""", messages=messages, model_name=model_name)

        await self.task_generation(prompt="""
### 🔹 第二步：生成结构化评估方案（评分卡设计）

根据上述分析，构建一份用于评测实际 Agent 回答**最新一轮用户提问**表现的 **结构化评分卡（Evaluation Checklist）**。

📌 检查点生成守则（必须遵守）：
1. **只保留确定性高的检查点**：
- 仅当某个工具调用或参数传递是**唯一合理路径**或**配置强制要求**时，才可生成检查点。
- 若存在替代路径（如：Agent 可从历史对话获取数据、使用默认值、跳过某步、一次调用工具中已包含该步骤的实现、**可直接复用历史对话中的数据**），则不应强制要求该步骤。

2. **拒绝模糊依赖**：
- 禁止添加如"应该考虑天气"、"建议使用位置信息"等非强制性判断。
- 所有依赖必须明确来自工具输出或历史对话上下文，且在模拟执行中真实使用。

3. **格式检查仅针对关键结论**：
- 仅当问题明确要求结构化输出（如"列出"、"对比"、"表格"），或工具返回天然结构化时，才可添加格式检查点。
- 否则，默认允许自由文本描述。

4. **结论来源必须可追溯**：
- 若最终结论引用了数值、名称、状态等，必须能追溯到是哪个工具返回的信息或历史对话中的哪些数据。（当你明确知道工具的返回字段，否则只说明工具名称）
- 否则，不得添加"必须基于工具输出"的检查点。

📌 基于历史对话上下文的特殊检查点规则：
1. **历史上下文理解检查**：
- 如果最新提问包含指代性表达（"这些客户"、"他们"等），且必须正确理解历史对话中的实体，可生成上下文理解检查点。
- 但要注意：仅当这种指代消解是唯一合理解释时才生成。

2. **历史数据复用检查（重要）**：
- **禁止生成要求重复调用的检查点**：如果历史对话中已经获取了所需数据且可以直接复用，则不应生成要求重新调用该工具的检查点。
- **仅在必须重新调用时才生成检查点**：只有当查询参数发生实质性变化（时间范围、筛选条件、查询对象不同）或历史数据不包含当前所需信息时，才可生成要求重新调用的检查点。
- 例如：历史对话中已查询了"最近1个月的客户"，最新提问询问"这些客户的跟进记录"时：
  - ❌ 不应生成："必须调用客户查询工具"（因为可以复用历史数据）
  - ✅ 可以生成："必须调用跟进记录查询工具"（因为历史对话中没有跟进记录数据）

3. **评估范围限定**：
- **检查点仅针对最新一轮用户提问的回答**，不评估历史对话的质量。
- 所有检查点应明确是针对"回答最新提问"的行为，而非历史对话行为。

✅ 输出原则：**精确优先，保守生成**。宁可只生成 X 个高置信检查点，也不生成 X + N(N为不确信检查点数量) 个含不确定项的检查点。
            """, messages=messages, model_name=model_name)

        three_response = await self.task_generation(prompt="""
### 🔹 第三步：输出标准 JSON 格式的评分卡

请严格按照以下 JSON Schema 输出最终结果。**不得添加注释、额外字段或解释性文字**。

```json
{
    "evaluation_checkpoints": [
        {
            "checkpoint_id": "CP1",
            "hit_point": "字符串，具体评估行为点（如：调用了 search_customers 工具）",
            "judgement_standard": "字符串，明确的判断标准（如：必须调用 search_customers 工具，且 time_range 参数覆盖近1个月）"
        },
        {
            "checkpoint_id": "CP2",
            "hit_point": "字符串，如：最终结果的结论部分是否依据工具调用返回的结果或历史对话数据",
            "judgement_standard": "字符串，如：判断输出的客户信息必须来自工具返回数据或历史对话中已获取的数据"
        },
        {
            "checkpoint_id": "CP3",
            "hit_point": "字符串，如：最终输出中查询返回的必要信息输出格式是否为表格",
            "judgement_standard": "字符串，如：判断输出内容中客户信息是否以表格形式呈现"
        },
        {
            "checkpoint_id": "CP4",
            "hit_point": "字符串，如：正确理解了历史对话上下文中的指代对象",
            "judgement_standard": "字符串，如：如果提问中包含'这些客户'等指代，必须正确识别为历史对话中查询的客户"
        }
    ],
    "difficulty_level": {
        "level": "字符串，三选一：'简单' / '中等' / '困难'",
        "难点说明": "字符串，说明任务挑战性（如：需理解历史对话上下文、包含指代消解、需复用历史数据、多工具串联、参数依赖复杂、需数据聚合等）"
    },
    "required_tools": [
        "字符串数组，列出回答最新提问需要调用的所有工具名称（去重）"
    ]
}
    
📌 重要提醒：
- 在填充 evaluation_checkpoints 前，请再次审查每个检查点是否满足：
  1. 是否 100% 确信该行为是必要且唯一的？
  2. 是否存在 Agent 不经过此步也能正确回答的可能性？
  3. 参数来源是否明确且不可替代？
  4. **对于工具调用检查点**：历史对话中的数据是否不可复用？如果可以复用，则不应生成要求重新调用的检查点。
  5. 所有检查点是否都是针对"回答最新提问"的行为，而非历史对话行为？
- 如有任何疑问，请直接删除该检查点。
- 记住：**一个错误的检查点会污染整个评测体系，其危害远大于遗漏一个非关键点。**
- checkpoint_id 命名规范：
  - 检查点：CP{序号}，如 CP1、CP2、CP3
""", messages=messages, model_name=model_name)


        three_response = self.parse_json_response(three_response)
        if isinstance(three_response, dict):
            three_response["messages"] = user_messages
            three_response = json.dumps(three_response, ensure_ascii=False)
        elif isinstance(three_response, str):
            three_response = three_response + f"\nmessages: {user_messages}"
            
        logger.info("workflow messages: " + json.dumps(messages, ensure_ascii=False, indent=4))
        
        logger.info("workflow result: " + three_response)

        return three_response
import os
import time
import hashlib
import asyncio
import concurrent.futures
from typing import Dict, Any, List, Optional

from .tool_base import ToolBase
from sagents.utils.logger import logger
from sagents.utils.file_parser import FileParser

class FileParserTool(ToolBase):
    """æ–‡ä»¶è§£æå·¥å…·é›†"""

    def __init__(self):
        logger.debug("Initializing FileParserTool")
        super().__init__()

    @ToolBase.tool(
        description_i18n={
            "zh": "ä»éæ–‡æœ¬æ–‡ä»¶ä¸­æå–Markdownæ–‡æœ¬ï¼ˆå«PDF/Office/CSVç­‰ï¼‰",
            "en": "Extract Markdown text from non-text files (PDF/Office/CSV)",
            "pt": "Extrai texto Markdown de arquivos nÃ£o textuais (PDF/Office/CSV)"
        },
        param_description_i18n={
            "input_file_path": {"zh": "è¾“å…¥æ–‡ä»¶çš„ç»å¯¹è·¯å¾„", "en": "Absolute path of input file", "pt": "Caminho absoluto do arquivo de entrada"},
            "start_index": {"zh": "å¼€å§‹å­—ç¬¦ä½ç½®ï¼Œé»˜è®¤0", "en": "Start character index, default 0", "pt": "Ãndice inicial de caractere, padrÃ£o 0"},
            "max_length": {"zh": "æœ€å¤§æå–é•¿åº¦ï¼Œé»˜è®¤5000ï¼Œæœ€å¤§5000", "en": "Max extract length, default 5000, max 5000", "pt": "Comprimento mÃ¡ximo de extraÃ§Ã£o, padrÃ£o 5000, mÃ¡ximo 5000"},
            "include_metadata": {"zh": "æ˜¯å¦åŒ…å«æ–‡ä»¶å…ƒæ•°æ®", "en": "Whether to include file metadata", "pt": "Se deve incluir metadados do arquivo"}
        }
    )
    def extract_text_from_non_text_file(
        self, 
        input_file_path: str, 
        start_index: int = 0, 
        max_length: int = 500000,
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """è¯»å–æœ¬åœ°å­˜å‚¨ä¸‹çš„éæ–‡æœ¬æ–‡ä»¶ï¼Œä¾‹å¦‚pdfï¼Œdocxï¼Œdocï¼Œpptï¼Œpptxï¼Œxlsxï¼Œxlsï¼Œcsvç­‰æ–‡ä»¶ï¼Œè¿”å›Markdownçš„æ–‡æœ¬æ•°æ®

        Args:
            input_file_path (str): è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæœ¬åœ°çš„ç»å¯¹è·¯å¾„
            start_index (int): å¼€å§‹æå–çš„å­—ç¬¦ä½ç½®ï¼Œé»˜è®¤0
            max_length (int): å•æ¬¡æœ€å¤§æå–é•¿åº¦ï¼Œé»˜è®¤5000å­—ç¬¦ï¼Œæœ€å¤§5000å­—ç¬¦
            include_metadata (bool): æ˜¯å¦åŒ…å«æ–‡ä»¶å…ƒæ•°æ®ï¼Œé»˜è®¤True

        Returns:
            Dict[str, Any]: åŒ…å«æå–æ–‡æœ¬å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        if max_length > 500000:
            max_length = 500000
        start_time = time.time()
        operation_id = hashlib.md5(f"extract_{input_file_path}_{time.time()}".encode()).hexdigest()[:8]
        logger.info(f"ğŸ“„ extract_text_from_fileå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {input_file_path}")
        logger.info(f"ğŸ”§ å‚æ•°: start_index={start_index}, max_length={max_length}, include_metadata={include_metadata}")

        try:
            # ä½¿ç”¨ Core Utils ä¸­çš„ FileParser
            parser = FileParser()

            # FileParser.extract_text_from_file è¿”å›çš„æ ¼å¼:
            # {
            #     "text": "...",
            #     "metadata": {...},
            #     "success": True,
            #     "error": None
            # }
            # æˆ–è€…é”™è¯¯æ—¶:
            # {
            #     "success": False,
            #     "error": "..."
            # }

            def run_async_in_thread():
                # åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        parser.extract_text_from_file(
                            input_file_path,
                            start_index=start_index,
                            max_length=max_length,
                            timeout=120, # ç¨å¾®ç»™å¤šç‚¹æ—¶é—´å¤„ç†å¤§æ–‡ä»¶
                            enable_text_cleaning=True
                        )
                    )
                finally:
                    new_loop.close()

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_async_in_thread)
                result = future.result()

            if not result.get("success"):
                error_time = time.time() - start_time
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                logger.error(f"âŒ æ–‡ä»¶è§£æå¤±è´¥ [{operation_id}] - é”™è¯¯: {error_msg}, è€—æ—¶: {error_time:.2f}ç§’")
                return {
                    "success": False,
                    "error": error_msg,
                    "file_path": input_file_path,
                    "execution_time": error_time,
                    "operation_id": operation_id
                }

            # æˆåŠŸ
            extracted_text = result.get("text", "")
            metadata = result.get("metadata", {})

            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„å…ƒæ•°æ®ï¼Œå¦‚æœ include_metadata ä¸º False
            if not include_metadata:
                metadata = {}

            execution_time = time.time() - start_time
            logger.info(f"âœ… æ–‡ä»¶è§£ææˆåŠŸ [{operation_id}] - åŸå§‹æ–‡æœ¬é•¿åº¦: {len(extracted_text)}, è§£æè€—æ—¶: {execution_time:.2f}ç§’")

            return {
                "success": True,
                "text": extracted_text,
                "metadata": metadata,
                "file_path": input_file_path,
                "execution_time": execution_time,
                "operation_id": operation_id
            }

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"âŒ ç³»ç»Ÿå¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’")
            return {
                "success": False,
                "error": f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}",
                "file_path": input_file_path,
                "execution_time": error_time,
                "operation_id": operation_id
            }

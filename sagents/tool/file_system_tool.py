import os
import hashlib
import mimetypes
import time
from datetime import datetime
from pathlib import Path
import urllib.parse
import httpx
import platform
import re
import chardet
from typing import Dict, Any, Optional, List
import asyncio

from .tool_base import ToolBase
from sagents.utils.logger import logger
class FileSystemError(Exception):
    """æ–‡ä»¶ç³»ç»Ÿå¼‚å¸¸"""
    pass

class SecurityValidator:
    """å®‰å…¨éªŒè¯å™¨"""
    
    # å±é™©çš„æ–‡ä»¶æ‰©å±•å
    DANGEROUS_EXTENSIONS = {
        '.exe', '.bat', '.cmd', '.com', '.pif', '.scr', '.vbs', '.js',
        '.jar', '.app', '.deb', '.pkg', '.rpm', '.dmg', '.iso'
    }
    
    # ç³»ç»Ÿå…³é”®ç›®å½•ï¼ˆç¦æ­¢æ“ä½œï¼‰
    PROTECTED_PATHS = {
        '/System', '/usr/bin', '/usr/sbin', '/bin', '/sbin',
        '/Windows/System32', '/Windows/SysWOW64', '/Program Files',
        '/Program Files (x86)'
    }
    
    @staticmethod
    def validate_path(file_path: str, allow_dangerous: bool = False) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶è·¯å¾„çš„å®‰å…¨æ€§"""
        try:
            # æ£€æŸ¥è·¯å¾„éå†æ”»å‡»ï¼ˆåœ¨è§£æå‰æ£€æŸ¥ï¼‰
            if '..' in file_path:
                return {"valid": False, "error": "è·¯å¾„åŒ…å«å±é™©çš„éå†å­—ç¬¦"}
            
            path = Path(file_path).resolve()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
            if not path.is_absolute():
                return {"valid": False, "error": "å¿…é¡»æä¾›ç»å¯¹è·¯å¾„"}
            
            # æ£€æŸ¥ç³»ç»Ÿä¿æŠ¤ç›®å½•
            path_str = str(path)
            for protected in SecurityValidator.PROTECTED_PATHS:
                if path_str.startswith(protected):
                    return {"valid": False, "error": f"ç¦æ­¢è®¿é—®ç³»ç»Ÿä¿æŠ¤ç›®å½•: {protected}"}
            
            # æ£€æŸ¥å±é™©æ–‡ä»¶æ‰©å±•å
            if not allow_dangerous and path.suffix.lower() in SecurityValidator.DANGEROUS_EXTENSIONS:
                return {"valid": False, "error": f"å±é™©çš„æ–‡ä»¶ç±»å‹: {path.suffix}"}
            
            return {"valid": True, "resolved_path": str(path)}
            
        except Exception as e:
            return {"valid": False, "error": f"è·¯å¾„éªŒè¯å¤±è´¥: {str(e)}"}

class FileMetadata:
    """æ–‡ä»¶å…ƒæ•°æ®ç®¡ç†å™¨"""
    
    @staticmethod
    def get_file_info(file_path: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯"""
        try:
            path = Path(file_path)
            
            if not path.exists():
                return {"exists": False}
            
            stat_info = path.stat()
            
            # åŸºç¡€ä¿¡æ¯
            info = {
                "exists": True,
                "name": path.name,
                "absolute_path": str(path.absolute()),
                "size_bytes": stat_info.st_size,
                "size_mb": round(stat_info.st_size / (1024 * 1024), 2),
                "is_file": path.is_file(),
                "is_dir": path.is_dir(),
                "is_symlink": path.is_symlink(),
                "created_time": datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                "accessed_time": datetime.fromtimestamp(stat_info.st_atime).isoformat(),
            }
            
            # æ–‡ä»¶ç‰¹å®šä¿¡æ¯
            if path.is_file():
                info.update({
                    "extension": path.suffix.lower(),
                    "mime_type": mimetypes.guess_type(str(path))[0] or "unknown",
                    "encoding": FileMetadata._detect_encoding(file_path) if path.suffix.lower() in ['.txt', '.py', '.js', '.css', '.html', '.md'] else None
                })
            
            # æƒé™ä¿¡æ¯
            info["permissions"] = {
                "readable": os.access(file_path, os.R_OK),
                "writable": os.access(file_path, os.W_OK),
                "executable": os.access(file_path, os.X_OK),
                "mode": oct(stat_info.st_mode)[-3:] if platform.system() != 'Windows' else None
            }
            
            return info
            
        except Exception as e:
            return {"exists": False, "error": str(e)}
    
    @staticmethod
    def _detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)
                result = chardet.detect(raw_data)
                return result.get('encoding') or 'utf-8'
        except Exception:
            return 'utf-8'

class FileSystemTool(ToolBase):
    """æ–‡ä»¶ç³»ç»Ÿæ“ä½œå·¥å…·é›†"""
    
    def __init__(self):
        logger.debug("Initializing FileSystemTool")
        super().__init__()
        self.default_upload_url = "http://36.133.44.114:20034/askonce/api/v1/doc/upload"
        self.default_headers = {"User-Source": 'AskOnce_bakend'}

    @ToolBase.tool(
        description_i18n={
            "zh": "è¯»å–æ–‡æœ¬æ–‡ä»¶æŒ‡å®šè¡ŒèŒƒå›´å†…å®¹",
            "en": "Read text file within a line range",
            "pt": "LÃª conteÃºdo do arquivo em intervalo de linhas"
        },
        param_description_i18n={
            "file_path": {"zh": "æ–‡ä»¶ç»å¯¹è·¯å¾„", "en": "Absolute file path", "pt": "Caminho absoluto do arquivo"},
            "start_line": {"zh": "å¼€å§‹è¡Œå·ï¼Œé»˜è®¤0", "en": "Start line number, default 0", "pt": "Linha inicial, padrÃ£o 0"},
            "end_line": {"zh": "ç»“æŸè¡Œå·ï¼ˆä¸åŒ…å«ï¼‰", "en": "End line number (exclusive)", "pt": "Linha final (exclusiva)"},
            "encoding": {"zh": "æ–‡ä»¶ç¼–ç ï¼Œautoè‡ªåŠ¨æ£€æµ‹", "en": "File encoding, 'auto' for detection", "pt": "CodificaÃ§Ã£o, 'auto' para detectar"},
            "max_size_mb": {"zh": "æœ€å¤§è¯»å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰", "en": "Maximum file size to read (MB)", "pt": "Tamanho mÃ¡ximo do arquivo para leitura (MB)"}
        }
    )
    async def file_read(self, file_path: str, start_line: int = 0, end_line: Optional[int] = 20, 
                  encoding: str = "auto", max_size_mb: float = 10.0) -> Dict[str, Any]:
        """é«˜çº§æ–‡ä»¶è¯»å–å·¥å…·ï¼Œè¯»å–æ–‡æœ¬æ–‡ä»¶ï¼Œä¾‹å¦‚txtï¼Œä»¥åŠé…ç½®æ–‡ä»¶å’Œä»£ç æ–‡ä»¶

        Args:
            file_path (str): æ–‡ä»¶ç»å¯¹è·¯å¾„
            start_line (int): å¼€å§‹è¡Œå·ï¼Œé»˜è®¤0
            end_line (int): ç»“æŸè¡Œå·ï¼ˆä¸åŒ…å«ï¼‰ï¼Œé»˜è®¤20
            encoding (str): æ–‡ä»¶ç¼–ç ï¼Œ'auto'è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
            max_size_mb (float): æœ€å¤§è¯»å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤10MB

        Returns:
            Dict[str, Any]: åŒ…å«æ–‡ä»¶å†…å®¹å’Œå…ƒä¿¡æ¯
        """
        
        start_time = time.time()
        operation_id = hashlib.md5(f"read_{file_path}_{time.time()}".encode()).hexdigest()[:8]
        logger.info(f"ğŸ“– file_readå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}")
        
        try:
            # å®‰å…¨éªŒè¯
            validation = SecurityValidator.validate_path(file_path)
            if not validation["valid"]:
                return {"status": "error", "message": validation["error"]}
            
            file_path = validation["resolved_path"]
            
            # è·å–æ–‡ä»¶ä¿¡æ¯ (run in thread because it might detect encoding)
            file_info = await asyncio.to_thread(FileMetadata.get_file_info, file_path)
            if not file_info["exists"]:
                return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
            if not file_info["is_file"]:
                return {"status": "error", "message": "æŒ‡å®šè·¯å¾„ä¸æ˜¯æ–‡ä»¶"}
            
            if not file_info["permissions"]["readable"]:
                return {"status": "error", "message": "æ–‡ä»¶æ— è¯»å–æƒé™"}
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if file_info["size_mb"] > max_size_mb:
                return {"status": "error", "message": f"æ–‡ä»¶è¿‡å¤§: {file_info['size_mb']:.2f}MB > {max_size_mb}MB"}
            
            # æ£€æµ‹ç¼–ç 
            if encoding == "auto":
                encoding = file_info.get("encoding", "utf-8")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            def read_file_lines():
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.readlines()

            lines = await asyncio.to_thread(read_file_lines)
            
            # å¤„ç†è¡ŒèŒƒå›´
            total_lines = len(lines)
            if end_line is None:
                end_line = total_lines
            
            start_line = max(0, start_line)
            end_line = min(total_lines, end_line)
            
            if start_line >= total_lines:
                content = ""
            else:
                content = ''.join(lines[start_line:end_line])
            
            total_time = time.time() - start_time
            
            # è¿”å›ç»“æœå¤„ç†æ‰å‰ç½®è·¯å¾„
            return {
                "status": "success",
                "message": f"æˆåŠŸè¯»å–æ–‡ä»¶ (è¡Œ {start_line}-{end_line})",
                "content": content,
                "file_info": {
                    "path": file_path,
                    "total_lines": total_lines,
                    "read_lines": end_line - start_line,
                    "encoding": encoding,
                    "size_mb": file_info["size_mb"]
                },
                "line_range": {
                    "start": start_line,
                    "end": end_line,
                    "total": total_lines
                },
                "execution_time": total_time,
                "operation_id": operation_id
            }
            
        except UnicodeDecodeError as e:
            return {"status": "error", "message": f"æ–‡ä»¶ç¼–ç é”™è¯¯: {str(e)}ï¼Œè¯·å°è¯•æŒ‡å®šæ­£ç¡®çš„ç¼–ç "}
        except Exception as e:
            logger.error(f"ğŸ’¥ è¯»å–æ–‡ä»¶å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}")
            return {"status": "error", "message": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"}

    @ToolBase.tool(
        description_i18n={
            "zh": "æŒ‰æ¨¡å¼å†™å…¥æ–‡æœ¬åˆ°æ–‡ä»¶",
            "en": "Write text to file with mode",
            "pt": "Grava texto no arquivo com modo"
        },
        param_description_i18n={
            "file_path": {"zh": "æ–‡ä»¶ç»å¯¹è·¯å¾„", "en": "Absolute file path", "pt": "Caminho absoluto do arquivo"},
            "content": {"zh": "è¦å†™å…¥çš„æ–‡æœ¬å†…å®¹", "en": "Text content to write", "pt": "ConteÃºdo de texto a gravar"},
            "mode": {"zh": "å†™å…¥æ¨¡å¼ overwrite/append/prepend", "en": "Write mode overwrite/append/prepend", "pt": "Modo de gravaÃ§Ã£o overwrite/append/prepend"},
            "encoding": {"zh": "æ–‡ä»¶ç¼–ç ", "en": "File encoding", "pt": "CodificaÃ§Ã£o do arquivo"}
        }
    )
    async def file_write(self, file_path: str, content: str, mode: str = "overwrite", 
                   encoding: str = "utf-8") -> Dict[str, Any]:
        """æ™ºèƒ½æ–‡ä»¶å†™å…¥å·¥å…·

        Args:
            file_path (str): æ–‡ä»¶ç»å¯¹è·¯å¾„
            content (str): è¦å†™å…¥çš„å†…å®¹
            mode (str): å†™å…¥æ¨¡å¼ - 'overwrite', 'append', 'prepend'
            encoding (str): æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤utf-8
            auto_upload (bool): æ˜¯å¦è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘ç«¯ï¼Œé»˜è®¤True
            
        Returns:
            Dict[str, Any]: æ“ä½œç»“æœå’Œæ–‡ä»¶ä¿¡æ¯
        """
        
        start_time = time.time()
        operation_id = hashlib.md5(f"write_{file_path}_{time.time()}".encode()).hexdigest()[:8]
        logger.info(f"âœï¸ file_writeå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}")
        
        try:
            # å®‰å…¨éªŒè¯
            validation = SecurityValidator.validate_path(file_path)
            if not validation["valid"]:
                return {"status": "error", "message": validation["error"]}
            
            file_path = validation["resolved_path"]
            path = Path(file_path)
            
            # åˆ›å»ºç›®å½•ç»“æ„
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤„ç†å†™å…¥æ¨¡å¼
            if mode == "overwrite":
                write_mode = 'w'
                final_content = content
            elif mode == "append":
                write_mode = 'a'
                final_content = content
            elif mode == "prepend":
                write_mode = 'w'
                if path.exists():
                    def read_existing():
                        with open(file_path, 'r', encoding=encoding) as f:
                            return f.read()
                    existing_content = await asyncio.to_thread(read_existing)
                    final_content = content + existing_content
                else:
                    final_content = content
            else:
                return {"status": "error", "message": f"ä¸æ”¯æŒçš„å†™å…¥æ¨¡å¼: {mode}"}
            
            # å†™å…¥æ–‡ä»¶
            write_start_time = time.time()
            
            def write_file():
                with open(file_path, write_mode, encoding=encoding) as f:
                    f.write(final_content)
            
            await asyncio.to_thread(write_file)
            
            write_time = time.time() - write_start_time
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = await asyncio.to_thread(FileMetadata.get_file_info, file_path)
            
            result: Dict[str, Any] = {
                "status": "success",
                "message": f"æ–‡ä»¶å†™å…¥æˆåŠŸ ({mode}æ¨¡å¼)",
                "file_info": {
                    "path": file_path,
                    "size_mb": file_info["size_mb"],
                    "encoding": encoding
                },
                "operation": {
                    "mode": mode,
                    "content_length": len(content),
                    "final_content_length": len(final_content),
                    "write_time": write_time,
                    "timestamp": datetime.now().isoformat()
                },
                "operation_id": operation_id
            }
            
            # è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘ç«¯
            # if auto_upload:
            #     try:
            #         upload_result = self.upload_file_to_cloud(file_path)
            #         if upload_result["status"] == "success":
            #             result["cloud_url"] = upload_result["url"]
            #             result["file_id"] = upload_result.get("file_id")
            #             result["message"] += "ï¼Œå·²ä¸Šä¼ åˆ°äº‘ç«¯"
            #         else:
            #             result["upload_error"] = upload_result["message"]
            #     except Exception as e:
            #         result["upload_error"] = f"äº‘ç«¯ä¸Šä¼ å¤±è´¥: {str(e)}"
            
            total_time = time.time() - start_time
            result["execution_time"] = total_time
            
            return result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ æ–‡ä»¶å†™å…¥å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}")
            return {"status": "error", "message": f"æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}", "operation_id": operation_id}

    # @ToolBase.tool()
    # def upload_file_to_cloud(self, file_path: str) -> Dict[str, Any]:
    #     """ä¸Šä¼ æ–‡ä»¶åˆ°äº‘ç«¯å­˜å‚¨
        
    #     Args:
    #         file_path (str): è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
    #     Returns:
    #         Dict[str, Any]: ä¸Šä¼ ç»“æœï¼ŒåŒ…å«çŠ¶æ€å’Œæ–‡ä»¶URL
    #     """
    #     start_time = time.time()
    #     operation_id = hashlib.md5(f"upload_cloud_{file_path}_{time.time()}".encode()).hexdigest()[:8]
    #     logger.info(f"â˜ï¸ upload_file_to_cloudå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}")
        
    #     try:
    #         # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    #         if not os.path.exists(file_path):
    #             return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
    #         # è·å–æ–‡ä»¶ä¿¡æ¯
    #         file_name = os.path.basename(file_path)
    #         file_size = os.path.getsize(file_path)
    #         file_size_mb = file_size / (1024 * 1024)
            
    #         # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶100MBï¼‰
    #         if file_size > 100 * 1024 * 1024:
    #             return {"status": "error", "message": "æ–‡ä»¶è¿‡å¤§ï¼Œè¶…è¿‡100MBé™åˆ¶"}
            
    #         # å‡†å¤‡ä¸Šä¼ 
    #         url = self.default_upload_url
    #         headers = self.default_headers
            
    #         # å‘èµ·ä¸Šä¼ è¯·æ±‚
    #         upload_start_time = time.time()
    #         with open(file_path, 'rb') as f:
    #             files = {'file': (file_name, f, 'application/octet-stream')}
    #             response = requests.post(url, headers=headers, files=files, timeout=60)
            
    #         upload_time = time.time() - upload_start_time
    #         response.raise_for_status()
            
    #         # å¤„ç†å“åº”
    #         json_data = response.json()
    #         file_url = json_data.get('data', {}).get('url')
    #         file_id = json_data.get('data', {}).get('fileId')
            
    #         if not file_url:
    #             return {
    #                 "status": "error", 
    #                 "message": "APIè¿”å›æˆåŠŸä½†ç¼ºå°‘æ–‡ä»¶URL",
    #                 "response": json_data
    #             }
            
    #         total_time = time.time() - start_time
            
    #         return {
    #             "status": "success", 
    #             "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ", 
    #             "url": file_url,
    #             "file_id": file_id,
    #             "file_name": file_name,
    #             "file_size": file_size,
    #             "file_size_mb": file_size_mb,
    #             "upload_time": upload_time,
    #             "total_time": total_time,
    #             "operation_id": operation_id
    #         }
                
    #     except requests.exceptions.Timeout:
    #         return {"status": "error", "message": "ä¸Šä¼ è¶…æ—¶"}
    #     except requests.exceptions.RequestException as e:
    #         return {"status": "error", "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    #     except Exception as e:
    #         logger.error(f"ğŸ’¥ ä¸Šä¼ å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}")
    #         return {"status": "error", "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}

    @ToolBase.tool(
        description_i18n={
            "zh": "æŒ‰å…³é”®è¯æ£€ç´¢æ–‡ä»¶å¹¶è¿”å›ä¸Šä¸‹æ–‡",
            "en": "Search file by keywords and return context",
            "pt": "Pesquisa por palavras-chave e retorna contexto"
        },
        param_description_i18n={
            "file_path": {"zh": "è¦æœç´¢çš„æ–‡ä»¶è·¯å¾„", "en": "File path to search", "pt": "Caminho do arquivo para buscar"},
            "keywords": {"zh": "å…³é”®è¯åˆ—è¡¨", "en": "List of keywords", "pt": "Lista de palavras-chave"},
            "return_search_item": {"zh": "è¿”å›çš„åŒ¹é…æ¡ç›®æ•°é‡", "en": "Number of matched items to return", "pt": "Quantidade de resultados retornados"}
        }
    )
    def search_content_in_file(self, file_path: str, keywords:list[str],return_search_item=5) -> Dict[str, Any]:
        
        """åœ¨æ–‡ä»¶ä¸­é€šè¿‡å…³é”®è¯åŒ¹é…ï¼Œæœç´¢ç›¸å…³çš„å†…å®¹çš„ä¸Šä¸‹æ–‡å†…å®¹
        Args:
            file_path (str): è¦æœç´¢çš„æ–‡ä»¶è·¯å¾„
            keywords (list[str]): è¦æœç´¢çš„å…³é”®è¯åˆ—è¡¨
            return_search_item (int, optional): è¿”å›çš„æœç´¢ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª. Defaults to 5.
        Returns:
            Dict[str, Any]: æœç´¢ç»“æœï¼ŒåŒ…å«åŒ¹é…çš„å†…å®¹å’Œä¸Šä¸‹æ–‡
        """
        context_size = 800
        return_search_item = int(return_search_item)
        start_time = time.time()
        operation_id = hashlib.md5(f"search_file_{file_path}_{time.time()}".encode()).hexdigest()[:8]
        logger.info(f"ğŸ” search_content_in_fileå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}")
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
            # è¯»å–æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                file_content = f.read()

            # å­˜å‚¨æœç´¢ç»“æœ
            search_results = []
            file_content_lower = file_content.lower()
            
            # æ‰¾åˆ°æ‰€æœ‰å…³é”®è¯çš„åŒ¹é…ä½ç½®
            keyword_positions = {}
            for keyword in keywords:
                keyword_lower = keyword.lower()
                positions = []
                start = 0
                while True:
                    pos = file_content_lower.find(keyword_lower, start)
                    if pos == -1:
                        break
                    positions.append(pos)
                    start = pos + 1
                keyword_positions[keyword] = positions
            
            # æ”¶é›†æ‰€æœ‰åŒ¹é…ä½ç½®å¹¶è®¡ç®—ä¸Šä¸‹æ–‡
            all_positions = []
            for keyword, positions in keyword_positions.items():
                for pos in positions:
                    all_positions.append((pos, keyword))
            
            # æŒ‰ä½ç½®æ’åº
            all_positions.sort()
            
            # åˆå¹¶ç›¸è¿‘çš„åŒ¹é…ä½ç½®ï¼Œé¿å…é‡å¤çš„ä¸Šä¸‹æ–‡
            merged_results: List[Dict[str, Any]] = []
            for pos, keyword in all_positions:
                # è®¡ç®—ä¸Šä¸‹æ–‡èŒƒå›´
                start_char = max(0, pos - context_size // 2)
                end_char = min(len(file_content), pos + context_size // 2)
                
                # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰ç»“æœé‡å 
                overlapped = False
                for existing in merged_results:
                    if (start_char < existing['end_char'] and end_char > existing['start_char']):
                        # åˆå¹¶é‡å åŒºåŸŸ
                        existing['start_char'] = min(existing['start_char'], start_char)
                        existing['end_char'] = max(existing['end_char'], end_char)
                        if keyword not in existing['matched_keywords']:
                            existing['matched_keywords'].append(keyword)
                            existing['score'] += 1
                        overlapped = True
                        break
                
                if not overlapped:
                    # æå–ä¸Šä¸‹æ–‡å†…å®¹
                    context = file_content[start_char:end_char]
                    merged_results.append({
                        'score': 1,
                        'matched_keywords': [keyword],
                        'context': context.strip(),
                        'start_char': start_char,
                        'end_char': end_char,
                        'match_position': pos
                    })
            
            # æŒ‰åˆ†æ•°é™åºæ’åºï¼Œåˆ†æ•°ç›¸åŒæ—¶æŒ‰åŒ¹é…ä½ç½®å‡åº
            merged_results.sort(key=lambda x: (-x['score'], x['match_position']))
            
            # é™åˆ¶è¿”å›ç»“æœæ•°é‡
            search_results = merged_results[:return_search_item]
            
            execution_time = time.time() - start_time
            logger.info(f"âœ… search_content_in_fileæ‰§è¡Œå®Œæˆ [{operation_id}] - è€—æ—¶: {execution_time:.2f}s, æ‰¾åˆ° {len(search_results)} ä¸ªåŒ¹é…é¡¹")
            
            return {
                "status": "success",
                "message": f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªåŒ¹é…é¡¹",
                "results": search_results,
                "total_matches": len(search_results),
                "keywords": keywords,
                "execution_time": execution_time,
                "operation_id": operation_id
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"æœç´¢æ–‡ä»¶å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(f"âŒ search_content_in_fileæ‰§è¡Œå¤±è´¥ [{operation_id}] - {error_msg} - è€—æ—¶: {execution_time:.2f}s")
            return {
                "status": "error",
                "message": error_msg,
                "execution_time": execution_time,
                "operation_id": operation_id
            }

    
    @ToolBase.tool(
        description_i18n={
            "zh": "ä»URLä¸‹è½½æ–‡ä»¶åˆ°ç›®å½•",
            "en": "Download file from URL to directory",
            "pt": "Baixa arquivo da URL para diretÃ³rio"
        },
        param_description_i18n={
            "url": {"zh": "è¦ä¸‹è½½çš„æ–‡ä»¶URL", "en": "File URL to download", "pt": "URL do arquivo para download"},
            "working_dir": {"zh": "ä¿å­˜æ–‡ä»¶çš„ç›®å½•", "en": "Directory to save the file", "pt": "DiretÃ³rio para salvar o arquivo"}
        }
    )
    async def download_file_from_url(self, url: str, working_dir: str) -> Dict[str, Any]:
        """ä»URLä¸‹è½½æ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•

        Args:
            url (str): è¦ä¸‹è½½çš„æ–‡ä»¶URL
            working_dir (str): ä¿å­˜æ–‡ä»¶çš„å·¥ä½œç›®å½•

        Returns:
            Dict[str, Any]: ä¸‹è½½ç»“æœï¼ŒåŒ…å«ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """

        start_time = time.time()
        operation_id = hashlib.md5(f"download_{url}_{time.time()}".encode()).hexdigest()[:8]
        logger.info(f"ğŸ“¥ download_file_from_urlå¼€å§‹æ‰§è¡Œ [{operation_id}] - URL: {url}")
        
        try:
            # æ£€æŸ¥å·¥ä½œç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(working_dir):
                return {"status": "error", "message": f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {working_dir}"}
            
            # å‘èµ·HTTPè¯·æ±‚ä¸‹è½½æ–‡ä»¶
            download_start_time = time.time()
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.get(url)
                response.raise_for_status()
                content = response.content
            
            download_time = time.time() - download_start_time
            
            # è·å–æ–‡ä»¶å¤§å°
            content_length = len(content)
            content_size_mb = content_length / (1024 * 1024)
            
            # è§£ç URLå¹¶è·å–æ–‡ä»¶å
            decoded_url = urllib.parse.unquote(url)
            file_name = os.path.basename(decoded_url)
            
            # å¦‚æœæ–‡ä»¶åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
            if not file_name or file_name in ['/', '\\']:
                file_name = f"downloaded_file_{operation_id}"
            
            # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(working_dir, file_name)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ åç¼€
            original_file_path = file_path
            counter = 1
            while os.path.exists(file_path):
                name, ext = os.path.splitext(original_file_path)
                file_path = f"{name}_{counter}{ext}"
                counter += 1
            
            # ä¿å­˜æ–‡ä»¶
            save_start_time = time.time()
            await asyncio.to_thread(lambda: open(file_path, 'wb').write(content))
            save_time = time.time() - save_start_time
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
            if not os.path.exists(file_path):
                return {"status": "error", "message": f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {file_path}"}
            
            saved_size = os.path.getsize(file_path)
            if saved_size != content_length:
                logger.warning(f"âš ï¸ æ–‡ä»¶å¤§å°ä¸åŒ¹é… [{operation_id}] - ä¸‹è½½: {content_length}, ä¿å­˜: {saved_size}")
            
            total_time = time.time() - start_time
            
            return {
                "status": "success",
                "message": "æ–‡ä»¶ä¸‹è½½æˆåŠŸ",
                "file_path": file_path,
                "file_name": file_name,
                "file_size": saved_size,
                "file_size_mb": content_size_mb,
                "download_time": download_time,
                "save_time": save_time,
                "total_time": total_time,
                "operation_id": operation_id
            }
            
        except httpx.HTTPStatusError as e:
            return {"status": "error", "message": f"HTTPé”™è¯¯: {e.response.status_code}"}
        except httpx.RequestError as e:
            return {"status": "error", "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"ğŸ’¥ ä¸‹è½½å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}")
            return {"status": "error", "message": f"ä¸‹è½½å¤±è´¥: {str(e)}"}

    @ToolBase.tool(
        description_i18n={
            "zh": "åœ¨æ–‡ä»¶ä¸­æœç´¢å¹¶æ›¿æ¢æ–‡æœ¬",
            "en": "Search and replace text in file",
            "pt": "Busca e substitui texto no arquivo"
        },
        param_description_i18n={
            "file_path": {"zh": "æ–‡ä»¶ç»å¯¹è·¯å¾„", "en": "Absolute file path", "pt": "Caminho absoluto do arquivo"},
            "search_pattern": {"zh": "è¦æœç´¢çš„æ¨¡å¼æˆ–æ–‡æœ¬", "en": "Pattern or text to search", "pt": "PadrÃ£o ou texto a buscar"},
            "replacement": {"zh": "æ›¿æ¢æ–‡æœ¬", "en": "Replacement text", "pt": "Texto de substituiÃ§Ã£o"},
            "use_regex": {"zh": "æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼", "en": "Use regular expression", "pt": "Usar expressÃ£o regular"},
            "case_sensitive": {"zh": "æ˜¯å¦åŒºåˆ†å¤§å°å†™", "en": "Case sensitive", "pt": "Diferenciar maiÃºsculas/minÃºsculas"}
        }
    )
    def update_file(self, file_path: str, search_pattern: str, replacement: str, 
                          use_regex: bool = False, case_sensitive: bool = True) -> Dict[str, Any]:
        """æ›´æ–°æ–‡ä»¶ä¸­åŒ¹é…çš„æ–‡æœ¬å†…å®¹

        Args:
            file_path (str): æ–‡ä»¶ç»å¯¹è·¯å¾„
            search_pattern (str): è¦æœç´¢çš„æ¨¡å¼
            replacement (str): æ›¿æ¢æ–‡æœ¬
            use_regex (bool): æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            case_sensitive (bool): æ˜¯å¦åŒºåˆ†å¤§å°å†™

        Returns:
            Dict[str, Any]: æ›¿æ¢ç»“æœç»Ÿè®¡
        """
        
        try:
            # å®‰å…¨éªŒè¯
            validation = SecurityValidator.validate_path(file_path)
            if not validation["valid"]:
                return {"status": "error", "message": validation["error"]}
            
            file_path = validation["resolved_path"]
            
            # æ£€æŸ¥æ–‡ä»¶
            file_info = FileMetadata.get_file_info(file_path)
            if not file_info["exists"] or not file_info["is_file"]:
                return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶"}
            
            # è¯»å–æ–‡ä»¶
            encoding = file_info.get("encoding", "utf-8")
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                original_content = f.read()
            
            # æ‰§è¡Œæœç´¢æ›¿æ¢
            if use_regex:
                flags = 0 if case_sensitive else re.IGNORECASE
                pattern = re.compile(search_pattern, flags)
                new_content, replace_count = pattern.subn(replacement, original_content)
            else:
                if case_sensitive:
                    new_content = original_content.replace(search_pattern, replacement)
                    replace_count = original_content.count(search_pattern)
                else:
                    pattern = re.compile(re.escape(search_pattern), re.IGNORECASE)
                    new_content, replace_count = pattern.subn(replacement, original_content)
            
            # å†™å…¥ä¿®æ”¹åçš„å†…å®¹
            with open(file_path, 'w', encoding=encoding, errors='ignore') as f:
                f.write(new_content)
            
            return {
                "status": "success",
                "message": f"æˆåŠŸæ›¿æ¢ {replace_count} å¤„åŒ¹é…é¡¹",
                "statistics": {
                    "replacements": replace_count,
                    "original_length": len(original_content),
                    "new_length": len(new_content),
                    "length_change": len(new_content) - len(original_content)
                }
            }
            
        except re.error as e:
            return {"status": "error", "message": f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {str(e)}"}
        except Exception as e:
            return {"status": "error", "message": f"æœç´¢æ›¿æ¢å¤±è´¥: {str(e)}"} 
    
    @ToolBase.tool(
        description_i18n={
            "zh": "å°†CSVè½¬æ¢ä¸ºExcel",
            "en": "Convert CSV to Excel",
            "pt": "Converte CSV para Excel"
        },
        param_description_i18n={
            "csv_file_path": {"zh": "è¾“å…¥CSVæ–‡ä»¶è·¯å¾„", "en": "Input CSV file path", "pt": "Caminho do arquivo CSV de entrada"},
            "excel_file_path": {"zh": "è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„", "en": "Output Excel file path", "pt": "Caminho do arquivo Excel de saÃ­da"}
        }
    )
    def convert_csv_to_excel(self, csv_file_path: str, excel_file_path: str) -> Dict[str, Any]:
        """å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºExcelæ–‡ä»¶

        Args:
            csv_file_path (str): è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„
            excel_file_path (str): è¾“å‡ºçš„Excelæ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, Any]: è½¬æ¢ç»“æœ
        """
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(csv_file_path):
            return {"status": "error", "message": "è¾“å…¥çš„CSVæ–‡ä»¶ä¸å­˜åœ¨"}
        
        # è¯»å–CSVæ–‡ä»¶
        import pandas as pd
        df = pd.read_csv(csv_file_path)
        
        # å†™å…¥Excelæ–‡ä»¶
        df.to_excel(excel_file_path, index=False)
        
        return {
            "status": "success",
            "message": "CSVæ–‡ä»¶å·²æˆåŠŸè½¬æ¢ä¸ºExcelæ–‡ä»¶",
            "excel_file_path": excel_file_path
        }

"""
File Parser Tool for MCP Server

A tool for extracting text content from various file formats via network URLs.
Only supports network URLs for security reasons - no local file access.
"""

import hashlib
import os
import re
import tempfile
import time
import traceback
import urllib.parse
from pathlib import Path
from typing import Any, Dict, List, Optional

import chardet
import requests

# ç¬¬ä¸‰æ–¹åº“ï¼ˆç°åœ¨ç”±å„ä¸ªè§£æå™¨å­ç±»å¤„ç†ï¼‰
# å¯¼å…¥æ–°çš„è§£æå™¨å­ç±»
from .parsers import (
    BaseFileParser,
    DOCXParser,
    EMLParser,
    ExcelParser,
    HTMLParser,
    ParseResult,
    PDFParser,
    PPTXParser,
    TextParser,
)


class FileParserError(Exception):
    """æ–‡ä»¶è§£æå¼‚å¸¸"""

    pass


class FileValidator:
    """æ–‡ä»¶éªŒè¯å™¨ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œç½‘ç»œURL"""

    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹å’Œå¯¹åº”çš„MIMEç±»å‹
    SUPPORTED_FORMATS = {
        ".pdf": "application/pdf",
        ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        ".doc": "application/msword",
        ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
        ".ppt": "application/vnd.ms-powerpoint",
        ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        ".xls": "application/vnd.ms-excel",
        ".txt": "text/plain",
        ".csv": "text/csv",
        ".json": "application/json",
        ".xml": "application/xml",
        ".html": "text/html",
        ".htm": "text/html",
        ".md": "text/markdown",
        ".markdown": "text/markdown",
        ".rtf": "application/rtf",
        ".eml": "message/rfc822",
    }

    # æ–‡ä»¶å¤§å°é™åˆ¶ (MB)
    MAX_FILE_SIZE = {
        ".pdf": 50,
        ".docx": 25,
        ".doc": 25,
        ".pptx": 100,
        ".ppt": 100,
        ".xlsx": 25,
        ".xls": 25,
        ".txt": 10,
        ".csv": 50,
        ".json": 10,
        ".xml": 10,
        ".html": 5,
        ".htm": 5,
        ".md": 5,
        ".markdown": 5,
        ".rtf": 10,
        ".eml": 25,
    }

    @staticmethod
    def validate_file_path_or_url(file_path_or_url: str) -> Dict[str, Any]:
        """éªŒè¯æœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ–ç½‘ç»œURLçš„æœ‰æ•ˆæ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºURL
            is_url = file_path_or_url.startswith(("http://", "https://"))

            if is_url:
                # URLéªŒè¯é€»è¾‘
                parsed = urllib.parse.urlparse(file_path_or_url)
                if not parsed.netloc:
                    return {"valid": False, "error": "æ— æ•ˆçš„URLæ ¼å¼"}

                # ä»URLè·å–æ–‡ä»¶æ‰©å±•å
                path = parsed.path.lower()
                file_extension = None

                for ext in FileValidator.SUPPORTED_FORMATS.keys():
                    if path.endswith(ext):
                        file_extension = ext
                        break

                if not file_extension:
                    # å°è¯•ä»URLçš„æœ€åéƒ¨åˆ†è·å–æ‰©å±•å
                    filename = os.path.basename(path)
                    if "." in filename:
                        file_extension = "." + filename.split(".")[-1].lower()
                        if file_extension not in FileValidator.SUPPORTED_FORMATS:
                            return {
                                "valid": False,
                                "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}",
                            }
                    else:
                        return {"valid": False, "error": "æ— æ³•ä»URLç¡®å®šæ–‡ä»¶ç±»å‹"}

                return {
                    "valid": True,
                    "is_url": True,
                    "file_extension": file_extension,
                    "mime_type": FileValidator.SUPPORTED_FORMATS[file_extension],
                    "source": file_path_or_url,
                }
            else:
                # æœ¬åœ°æ–‡ä»¶è·¯å¾„éªŒè¯é€»è¾‘
                file_path = Path(file_path_or_url)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not file_path.exists():
                    return {"valid": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path_or_url}"}

                if not file_path.is_file():
                    return {
                        "valid": False,
                        "error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path_or_url}",
                    }

                # è·å–æ–‡ä»¶æ‰©å±•å
                file_extension = file_path.suffix.lower()
                if file_extension not in FileValidator.SUPPORTED_FORMATS:
                    return {
                        "valid": False,
                        "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}",
                    }

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                max_size = FileValidator.MAX_FILE_SIZE.get(file_extension, 10)
                if file_size_mb > max_size:
                    return {
                        "valid": False,
                        "error": f"æ–‡ä»¶è¿‡å¤§: {file_size_mb:.1f}MBï¼Œæœ€å¤§å…è®¸: {max_size}MB",
                    }

                return {
                    "valid": True,
                    "is_url": False,
                    "file_extension": file_extension,
                    "mime_type": FileValidator.SUPPORTED_FORMATS[file_extension],
                    "source": str(file_path.absolute()),
                    "file_size_mb": file_size_mb,
                }

        except Exception as e:
            return {"valid": False, "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}"}


class FileHandler:
    """æ–‡ä»¶å¤„ç†å™¨ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œç½‘ç»œæ–‡ä»¶ä¸‹è½½"""

    @staticmethod
    def get_file_path(file_path_or_url: str, is_url: bool, timeout: int = 30) -> str:
        """è·å–æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ˜¯URLåˆ™ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•ï¼Œä¿æŒåŸæ–‡ä»¶å"""
        try:
            if is_url:
                # åˆ›å»ºä¸´æ—¶ç›®å½•
                temp_dir = tempfile.mkdtemp()

                # ä»URLè·å–æ–‡ä»¶åï¼Œä¿æŒåŸæ–‡ä»¶å
                parsed_url = urllib.parse.urlparse(file_path_or_url)
                filename = os.path.basename(parsed_url.path)

                # å¦‚æœæ— æ³•ä»URLè·å–æ–‡ä»¶åï¼Œç”Ÿæˆä¸€ä¸ª
                if not filename or "." not in filename:
                    # å°è¯•ä»Content-Dispositionå¤´è·å–æ–‡ä»¶å
                    try:
                        response = requests.head(file_path_or_url, timeout=timeout)
                        content_disposition = response.headers.get(
                            "Content-Disposition", ""
                        )
                        if "filename=" in content_disposition:
                            filename = content_disposition.split("filename=")[1].strip(
                                '"'
                            )
                        else:
                            filename = f"downloaded_file_{int(time.time())}"
                    except:
                        filename = f"downloaded_file_{int(time.time())}"

                temp_file_path = os.path.join(temp_dir, filename)

                # ä¸‹è½½æ–‡ä»¶
                response = requests.get(file_path_or_url, timeout=timeout, stream=True)
                response.raise_for_status()

                with open(temp_file_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                return temp_file_path
            else:
                # æœ¬åœ°æ–‡ä»¶ï¼Œç›´æ¥è¿”å›è·¯å¾„
                return file_path_or_url

        except Exception as e:
            raise Exception(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")


class TextProcessor:
    """æ–‡æœ¬å¤„ç†å™¨"""

    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r"\n\s*\n", "\n\n", text)  # å¤šä¸ªè¿ç»­æ¢è¡Œç¬¦åˆå¹¶ä¸ºåŒæ¢è¡Œ
        text = re.sub(r"[ \t]+", " ", text)  # å¤šä¸ªè¿ç»­ç©ºæ ¼åˆå¹¶ä¸ºå•ä¸ªç©ºæ ¼
        text = text.strip()

        return text

    @staticmethod
    def truncate_text(text: str, start_index: int = 0, max_length: int = 5000) -> str:
        """å®‰å…¨åœ°æˆªå–æ–‡æœ¬"""
        if not text:
            return ""

        start_index = max(0, start_index)
        end_index = min(len(text), start_index + max_length)

        return text[start_index:end_index]

    @staticmethod
    def get_text_stats(text: str) -> Dict[str, int]:
        """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        if not text:
            return {"characters": 0, "words": 0, "lines": 0, "paragraphs": 0}

        return {
            "characters": len(text),
            "words": len(text.split()),
            "lines": text.count("\n") + 1,
            "paragraphs": len([p for p in text.split("\n\n") if p.strip()]),
        }

    @staticmethod
    def replace_wrong_char(text: str, correct_dict: Dict[str, str] = None) -> str:
        """æ›¿æ¢é”™è¯¯å­—ç¬¦"""
        if not text:
            return ""

        # é»˜è®¤çš„å­—ç¬¦æ›¿æ¢å­—å…¸
        default_correct_dict = {
            '"': '"',
            '"': '"',
            """: "'",
            """: "'",
            "â€¦": "...",
            "â€”": "-",
            "â€“": "-",
            "ã€€": " ",  # å…¨è§’ç©ºæ ¼æ›¿æ¢ä¸ºåŠè§’ç©ºæ ¼
        }

        if correct_dict:
            default_correct_dict.update(correct_dict)

        for wrong_char, correct_char in default_correct_dict.items():
            text = text.replace(wrong_char, correct_char)

        return text

    @staticmethod
    def remove_duplicate_char(text: str, is_remove_wrap: bool = False) -> str:
        """ç§»é™¤é‡å¤å­—ç¬¦"""
        if not text:
            return ""

        # ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦ï¼ˆè¿ç»­çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ªæ¢è¡Œç¬¦ï¼‰
        text = re.sub(r"\n+", "\n", text)

        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼ï¼ˆè¿ç»­çš„ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼ï¼‰
        text = re.sub(r" +", " ", text)

        if is_remove_wrap:
            # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå¥å·
            text = text.replace("\n", "ã€‚")
            # ç§»é™¤å¤šä½™çš„å¥å·
            text = re.sub(r"ã€‚+", "ã€‚", text)

        return text


class EncodingDetector:
    """æ–‡ä»¶ç¼–ç æ£€æµ‹å™¨"""

    @staticmethod
    def detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, "rb") as f:
                raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºæ£€æµ‹ç¼–ç 
                result = chardet.detect(raw_data)
                return result["encoding"] or "utf-8"
        except Exception:
            return "utf-8"


class ParserFactory:
    """è§£æå™¨å·¥å‚ç±»"""

    def __init__(self):
        self.parsers = {
            ".pdf": PDFParser(),
            ".docx": DOCXParser(),
            ".eml": EMLParser(),
            ".pptx": PPTXParser(),
            ".xlsx": ExcelParser(),
            ".xls": ExcelParser(),
            ".html": HTMLParser(),
            ".htm": HTMLParser(),
            ".txt": TextParser(),
            ".csv": TextParser(),
            ".json": TextParser(),
            ".xml": TextParser(),
            ".md": TextParser(),
            ".markdown": TextParser(),
            ".rtf": TextParser(),
        }

    def get_parser(self, file_extension: str) -> Optional[BaseFileParser]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–å¯¹åº”çš„è§£æå™¨"""
        return self.parsers.get(file_extension.lower())

    def is_supported(self, file_extension: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ"""
        return file_extension.lower() in self.parsers

    def detect_file_type(self, file_path: str) -> str:
        """
        æ£€æµ‹æ–‡ä»¶çš„å®é™…ç±»å‹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            str: æ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»å‹æ‰©å±•å
        """
        try:
            # ä½¿ç”¨magic numberæ£€æµ‹æ–‡ä»¶ç±»å‹
            import magic

            mime_type = magic.from_file(file_path, mime=True)

            # æ ¹æ®MIMEç±»å‹æ˜ å°„åˆ°æ‰©å±•å
            mime_to_ext = {
                "application/pdf": ".pdf",
                "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
                "application/vnd.openxmlformats-officedocument.presentationml.presentation": ".pptx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx",
                "text/plain": ".txt",
                "text/html": ".html",
                "application/json": ".json",
                "text/csv": ".csv",
                "text/markdown": ".md",
                "message/rfc822": ".eml",
            }

            detected_ext = mime_to_ext.get(mime_type, ".txt")
            print(
                f"ğŸ” æ–‡ä»¶ç±»å‹æ£€æµ‹: {file_path} -> MIME: {mime_type} -> æ‰©å±•å: {detected_ext}"
            )
            return detected_ext

        except ImportError:
            print("âš ï¸ python-magicæœªå®‰è£…ï¼Œä½¿ç”¨æ–‡ä»¶å¤´æ£€æµ‹")
            return self._detect_by_header(file_path)
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            traceback.print_exc()
            return self._detect_by_header(file_path)

    def _detect_by_header(self, file_path: str) -> str:
        """
        é€šè¿‡æ–‡ä»¶å¤´æ£€æµ‹æ–‡ä»¶ç±»å‹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            str: æ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»å‹æ‰©å±•å
        """
        try:
            with open(file_path, "rb") as f:
                header = f.read(1024)  # è¯»å–å‰1KB

            # æ£€æµ‹å¸¸è§æ–‡ä»¶å¤´
            if header.startswith(b"%PDF"):
                return ".pdf"
            elif header.startswith(b"PK\x03\x04"):
                # ZIPæ ¼å¼ï¼Œå¯èƒ½æ˜¯DOCX, PPTX, XLSXç­‰
                # è¿›ä¸€æ­¥æ£€æµ‹
                try:
                    import zipfile

                    with zipfile.ZipFile(file_path, "r") as zip_file:
                        file_list = zip_file.namelist()
                        if "word/document.xml" in file_list:
                            return ".docx"
                        elif "ppt/presentation.xml" in file_list:
                            return ".pptx"
                        elif "xl/workbook.xml" in file_list:
                            return ".xlsx"
                except:
                    pass
                return ".txt"  # å¦‚æœæ— æ³•ç¡®å®šï¼Œé»˜è®¤ä¸ºæ–‡æœ¬
            elif header.startswith(b"<!DOCTYPE html") or header.startswith(b"<html"):
                return ".html"
            elif header.startswith(b"{") or header.startswith(b"["):
                return ".json"
            else:
                # å°è¯•æ£€æµ‹æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡ä»¶
                try:
                    header.decode("utf-8")
                    return ".txt"
                except UnicodeDecodeError:
                    try:
                        header.decode("gbk")
                        return ".txt"
                    except UnicodeDecodeError:
                        return ".txt"  # é»˜è®¤ä¸ºæ–‡æœ¬

        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶å¤´æ£€æµ‹å¤±è´¥: {e}")
            traceback.print_exc()
            return ".txt"

    def get_smart_parser(
        self, file_path: str, file_extension: str
    ) -> tuple[Optional[BaseFileParser], bool]:
        """
        æ™ºèƒ½è·å–è§£æå™¨ï¼Œå…ˆæ£€æµ‹å®é™…æ–‡ä»¶ç±»å‹ï¼Œå¦‚æœä¸æ‰©å±•åä¸åŒ¹é…åˆ™ä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»å‹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            file_extension: æ–‡ä»¶æ‰©å±•å

        Returns:
            tuple[Optional[BaseFileParser], bool]: (è§£æå™¨å®ä¾‹, æ˜¯å¦ä½¿ç”¨äº†fallback)
        """
        # é¦–å…ˆæ£€æµ‹å®é™…æ–‡ä»¶ç±»å‹
        detected_ext = self.detect_file_type(file_path)
        print(f"ğŸ” æ–‡ä»¶æ‰©å±•å: {file_extension}, æ£€æµ‹åˆ°çš„ç±»å‹: {detected_ext}")

        # å¦‚æœæ£€æµ‹åˆ°çš„ç±»å‹ä¸æ‰©å±•åä¸åŒï¼Œä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»å‹
        if detected_ext != file_extension:
            # å¦‚æœæ‹“å±•åæ˜¯eml ä¸”æ£€æµ‹åˆ°çš„ç±»å‹ä¸æ˜¯emlï¼Œä½¿ç”¨æ‹“å±•å
            if file_extension == ".eml" and detected_ext != ".eml":
                print(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹ä¸åŒ¹é…ï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»å‹: {file_extension}")
                fallback_parser = self.get_parser(file_extension)
                if fallback_parser:
                    print(f"ğŸ”„ ä½¿ç”¨çš„è§£æå™¨: {file_extension} (è·³è¿‡can_parseæ£€æŸ¥)")
                    return fallback_parser, True

            print(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹ä¸åŒ¹é…ï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»å‹: {detected_ext}")
            fallback_parser = self.get_parser(detected_ext)
            if fallback_parser:
                print(f"ğŸ”„ ä½¿ç”¨æ£€æµ‹åˆ°çš„è§£æå™¨: {detected_ext} (è·³è¿‡can_parseæ£€æŸ¥)")
                return fallback_parser, True

        # å¦‚æœç±»å‹åŒ¹é…æˆ–æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨åŸºäºæ‰©å±•åçš„è§£æå™¨
        primary_parser = self.get_parser(file_extension)
        if primary_parser:
            print(f"ğŸ” ä½¿ç”¨ä¸»è§£æå™¨: {file_extension}")
            return primary_parser, False

        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æå™¨ä½œä¸ºæœ€åçš„fallback
        print(f"ğŸ”„ ä½¿ç”¨æ–‡æœ¬è§£æå™¨ä½œä¸ºæœ€åçš„fallback")
        return self.get_parser(".txt"), True


class FileParser:
    """æ–‡ä»¶è§£æå·¥å…·"""

    def __init__(self):
        self.parser_factory = ParserFactory()

    async def extract_text_from_file(
        self,
        file_path_or_url: str,
        start_index: int = 0,
        max_length: int = 500000,
        timeout: int = 60,
        enable_text_cleaning: bool = True,
        correct_dict: Dict[str, str] = None,
        is_remove_wrap: bool = False,
    ) -> Dict[str, Any]:
        """ä»æœ¬åœ°æ–‡ä»¶æˆ–ç½‘ç»œæ–‡ä»¶æå–æ–‡æœ¬å†…å®¹

        Args:
            file_path_or_url: æœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ–ç½‘ç»œURLåœ°å€
            start_index: å¼€å§‹æå–çš„å­—ç¬¦ä½ç½®
            max_length: æœ€å¤§æå–é•¿åº¦
            timeout: ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            enable_text_cleaning: æ˜¯å¦å¯ç”¨æ–‡æœ¬æ¸…æ´—
            correct_dict: è‡ªå®šä¹‰å­—ç¬¦æ›¿æ¢å­—å…¸
            is_remove_wrap: æ˜¯å¦ç§»é™¤æ¢è¡Œç¬¦

        Returns:
            åŒ…å«æå–ç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        operation_id = hashlib.md5(
            f"extract_{file_path_or_url}_{time.time()}".encode()
        ).hexdigest()[:8]
        temp_file_path = None

        try:
            # éªŒè¯æ–‡ä»¶è·¯å¾„æˆ–URL
            validation_result = FileValidator.validate_file_path_or_url(
                file_path_or_url
            )
            if not validation_result["valid"]:
                return {
                    "success": False,
                    "error": validation_result["error"],
                    "source": file_path_or_url,
                    "execution_time": time.time() - start_time,
                    "operation_id": operation_id,
                }

            file_extension = validation_result["file_extension"]
            is_url = validation_result["is_url"]

            # è·å–æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ˜¯URLåˆ™ä¸‹è½½ï¼‰
            file_path = FileHandler.get_file_path(file_path_or_url, is_url, timeout)
            if is_url:
                temp_file_path = file_path  # è®°å½•ä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºæ¸…ç†

            # ä½¿ç”¨æ™ºèƒ½è·¯ç”±è·å–è§£æå™¨
            parser, is_fallback = self.parser_factory.get_smart_parser(
                file_path, file_extension
            )

            if parser is None:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§£æå™¨ï¼Œå°è¯•ä½¿ç”¨pandoc
                try:
                    import pypandoc

                    extracted_text = pypandoc.convert_file(file_path, "markdown")
                    parse_result = ParseResult(
                        text=extracted_text,
                        metadata={
                            "file_type": "unknown",
                            "parser": "pandoc_fallback",
                            "file_size": (
                                os.path.getsize(file_path)
                                if os.path.exists(file_path)
                                else 0
                            ),
                        },
                        success=True,
                    )
                except Exception as e:
                    print(f"Pandocè§£æå¤±è´¥: {e}")
                    traceback.print_exc()
                    raise FileParserError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æˆ–è§£æå¤±è´¥: {str(e)}")
            else:
                # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©çš„è§£æå™¨
                try:
                    # å¦‚æœæ˜¯fallbackè§£æå™¨ï¼Œè·³è¿‡æ ¼å¼éªŒè¯
                    if is_fallback:
                        print(f"ğŸ”„ ä½¿ç”¨fallbackè§£æå™¨è·³è¿‡æ ¼å¼éªŒè¯: {file_path}")
                        parse_result = parser.parse(file_path, skip_validation=True)
                    else:
                        parse_result = parser.parse(file_path)

                    if not parse_result.success:
                        print(f"âš ï¸ è§£æå™¨å¤±è´¥: {parse_result.error}")
                        # å¦‚æœæ‰€æœ‰è§£æå™¨éƒ½å¤±è´¥ï¼Œå°è¯•pypandoc
                        try:
                            import pypandoc

                            print("ğŸ”„ å°è¯•ä½¿ç”¨pypandocä½œä¸ºæœ€åçš„fallback")
                            extracted_text = pypandoc.convert_file(
                                file_path, "markdown"
                            )
                            parse_result = ParseResult(
                                text=extracted_text,
                                metadata={
                                    "file_type": "unknown",
                                    "parser": "pypandoc_emergency_fallback",
                                    "file_size": (
                                        os.path.getsize(file_path)
                                        if os.path.exists(file_path)
                                        else 0
                                    ),
                                },
                                success=True,
                            )
                        except Exception as pypandoc_error:
                            raise FileParserError(
                                f"æ‰€æœ‰è§£æå™¨éƒ½å¤±è´¥äº†ã€‚è§£æå™¨é”™è¯¯: {parse_result.error}, pypandocé”™è¯¯: {str(pypandoc_error)}"
                            )

                    extracted_text = parse_result.text
                except Exception as e:
                    print(f"è§£æå™¨å¼‚å¸¸: {e}")
                    traceback.print_exc()

                    # æ™ºèƒ½è·¯ç”±å·²ç»å¤„ç†äº†æ–‡ä»¶ç±»å‹æ£€æµ‹ï¼Œç›´æ¥å°è¯•pypandocä½œä¸ºæœ€åçš„fallback
                    try:
                        import pypandoc

                        print("ğŸ”„ å°è¯•ä½¿ç”¨pypandocä½œä¸ºæœ€åçš„fallback")
                        extracted_text = pypandoc.convert_file(file_path, "markdown")
                        parse_result = ParseResult(
                            text=extracted_text,
                            metadata={
                                "file_type": "unknown",
                                "parser": "pypandoc_exception_fallback",
                                "file_size": (
                                    os.path.getsize(file_path)
                                    if os.path.exists(file_path)
                                    else 0
                                ),
                            },
                            success=True,
                        )
                    except Exception as pypandoc_error:
                        raise FileParserError(
                            f"æ‰€æœ‰è§£æå™¨éƒ½å¤±è´¥äº†ã€‚è§£æå™¨å¼‚å¸¸: {str(e)}, pypandocé”™è¯¯: {str(pypandoc_error)}"
                        )

            # å¤„ç†æ–‡æœ¬
            processed_text = extracted_text

            if enable_text_cleaning:
                # åº”ç”¨æ–‡æœ¬æ¸…æ´—
                processed_text = TextProcessor.replace_wrong_char(
                    processed_text, correct_dict
                )
                processed_text = TextProcessor.remove_duplicate_char(
                    processed_text, is_remove_wrap
                )

            # åŸºæœ¬æ¸…ç†å’Œæˆªå–
            cleaned_text = TextProcessor.clean_text(processed_text)
            truncated_text = TextProcessor.truncate_text(
                cleaned_text, start_index, max_length
            )
            text_stats = TextProcessor.get_text_stats(cleaned_text)

            total_time = time.time() - start_time

            # æ„å»ºè¿”å›ç»“æœï¼ŒåŒ…å«è§£æå™¨çš„å…ƒæ•°æ®
            result = {
                "success": True,
                "text": truncated_text,
                "file_info": {
                    "source": file_path_or_url,
                    "is_url": is_url,
                    "file_extension": file_extension,
                    "mime_type": validation_result["mime_type"],
                },
                "text_info": {
                    "original_length": len(extracted_text),
                    "processed_length": len(processed_text),
                    "cleaned_length": len(cleaned_text),
                    "extracted_length": len(truncated_text),
                    "start_index": start_index,
                    "max_length": max_length,
                    "text_cleaning_enabled": enable_text_cleaning,
                    **text_stats,
                },
                "execution_time": total_time,
                "operation_id": operation_id,
            }

            # æ·»åŠ è§£æå™¨çš„å…ƒæ•°æ®
            if "parse_result" in locals() and parse_result.metadata:
                result["metadata"] = parse_result.metadata

            return result

        except Exception as e:
            print(f"æ–‡ä»¶å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            traceback.print_exc()
            error_time = time.time() - start_time
            return {
                "success": False,
                "error": str(e),
                "source": file_path_or_url,
                "is_url": is_url,
                "execution_time": error_time,
                "operation_id": operation_id,
            }

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                except Exception:
                    pass

    async def get_supported_formats(self) -> Dict[str, Any]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼åˆ—è¡¨"""
        return {
            "success": True,
            "supported_formats": FileValidator.SUPPORTED_FORMATS,
            "max_file_sizes": FileValidator.MAX_FILE_SIZE,
            "total_formats": len(FileValidator.SUPPORTED_FORMATS),
            "note": "æ”¯æŒç½‘ç»œURLå’Œæœ¬åœ°æ–‡ä»¶è·¯å¾„",
        }

    def get_supported_file_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰"""
        return list(FileValidator.SUPPORTED_FORMATS.keys())

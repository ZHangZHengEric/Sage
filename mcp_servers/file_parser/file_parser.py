import argparse
import hashlib
import logging
import os
import re
import subprocess
import tempfile
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List

import aspose.slides as slides
import chardet
import html2text
import pdfplumber
import pypandoc
import requests
import uvicorn
from mcp.server.fastmcp import FastMCP
from openpyxl import load_workbook
from pptx import Presentation
from starlette.applications import Starlette
from starlette.routing import Mount

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

mcp = FastMCP("Advanced File Parser")

parser = argparse.ArgumentParser(description='å¯åŠ¨é«˜çº§æ–‡ä»¶è§£æ MCP Server')
args = parser.parse_args()


class FileParserError(Exception):
    """æ–‡ä»¶è§£æå¼‚å¸¸"""
    pass


class FileValidator:
    """æ–‡ä»¶éªŒè¯å™¨"""

    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹å’Œå¯¹åº”çš„MIMEç±»å‹
    SUPPORTED_FORMATS = {
        '.pdf': 'application/pdf',
        '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        '.doc': 'application/msword',
        '.pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        '.ppt': 'application/vnd.ms-powerpoint',
        '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        '.xls': 'application/vnd.ms-excel',
        '.txt': 'text/plain',
        '.csv': 'text/csv',
        '.json': 'application/json',
        '.xml': 'application/xml',
        '.html': 'text/html',
        '.htm': 'text/html',
        '.md': 'text/markdown',
        '.markdown': 'text/markdown',
        '.rtf': 'application/rtf',
        '.odt': 'application/vnd.oasis.opendocument.text',
        '.epub': 'application/epub+zip',
        '.latex': 'application/x-latex',
        '.tex': 'application/x-tex'
    }

    # æ–‡ä»¶å¤§å°é™åˆ¶ (MB)
    MAX_FILE_SIZE = {
        '.pdf': 50,
        '.docx': 25,
        '.doc': 25,
        '.pptx': 100,
        '.ppt': 100,
        '.xlsx': 25,
        '.xls': 25,
        '.txt': 10,
        '.csv': 50,
        '.json': 10,
        '.xml': 10,
        '.html': 5,
        '.htm': 5,
        '.md': 5,
        '.markdown': 5,
        '.rtf': 10,
        '.odt': 25,
        '.epub': 50,
        '.latex': 10,
        '.tex': 10
    }

    @staticmethod
    def validate_file(file_path: str) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶çš„æœ‰æ•ˆæ€§"""
        try:
            path = Path(file_path)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not path.exists():
                return {"valid": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆéç›®å½•ï¼‰
            if not path.is_file():
                return {"valid": False, "error": f"è·¯å¾„ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {file_path}"}

            # è·å–æ–‡ä»¶æ‰©å±•å
            file_extension = path.suffix.lower()

            # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
            if file_extension not in FileValidator.SUPPORTED_FORMATS:
                return {"valid": False, "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}"}

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size_mb = path.stat().st_size / (1024 * 1024)
            max_size = FileValidator.MAX_FILE_SIZE.get(file_extension, 10)

            if file_size_mb > max_size:
                return {"valid": False, "error": f"æ–‡ä»¶è¿‡å¤§: {file_size_mb:.1f}MB > {max_size}MB"}

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è¯»
            try:
                with open(file_path, 'rb') as f:
                    f.read(1024)  # å°è¯•è¯»å–å‰1KB
            except PermissionError:
                return {"valid": False, "error": "æ–‡ä»¶æ— è¯»å–æƒé™"}
            except Exception as e:
                return {"valid": False, "error": f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"}

            return {
                "valid": True,
                "file_size_mb": file_size_mb,
                "file_extension": file_extension,
                "mime_type": FileValidator.SUPPORTED_FORMATS[file_extension]
            }

        except Exception as e:
            return {"valid": False, "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}"}


class TextProcessor:
    """æ–‡æœ¬å¤„ç†å™¨"""

    @staticmethod
    def detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # è¯»å–å‰10KBè¿›è¡Œæ£€æµ‹
                result = chardet.detect(raw_data)
                return result.get('encoding', 'utf-8')
        except Exception:
            return 'utf-8'

    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\n\s*\n', '\n\n', text)  # å¤šä¸ªè¿ç»­æ¢è¡Œç¬¦åˆå¹¶ä¸ºåŒæ¢è¡Œ
        text = re.sub(r'[ \t]+', ' ', text)      # å¤šä¸ªè¿ç»­ç©ºæ ¼åˆå¹¶ä¸ºå•ä¸ªç©ºæ ¼
        text = text.strip()

        return text

    @staticmethod
    def truncate_text(text: str, start_index: int = 0, max_length: int = 5000) -> str:
        """å®‰å…¨åœ°æˆªå–æ–‡æœ¬"""
        if not text:
            return ""

        start_index = max(0, start_index)
        end_index = min(len(text), start_index + max_length)

        return text[start_index:end_index]

    @staticmethod
    def get_text_stats(text: str) -> Dict[str, int]:
        """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        if not text:
            return {"characters": 0, "words": 0, "lines": 0, "paragraphs": 0}

        return {
            "characters": len(text),
            "words": len(text.split()),
            "lines": text.count('\n') + 1,
            "paragraphs": len([p for p in text.split('\n\n') if p.strip()])
        }


class PDFParser:
    """PDFè§£æå™¨"""

    @staticmethod
    def extract_text(pdf_path: str) -> str:
        """ä»PDFæå–æ–‡æœ¬"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                text_parts = []
                for page_num, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(f"=== ç¬¬ {page_num + 1} é¡µ ===\n{page_text}")
                    except Exception as e:
                        logger.warning(f"PDFç¬¬{page_num + 1}é¡µè§£æå¤±è´¥: {e}")
                        text_parts.append(f"=== ç¬¬ {page_num + 1} é¡µ ===\n[é¡µé¢è§£æå¤±è´¥: {str(e)}]")

                return "\n\n".join(text_parts)

        except Exception as e:
            raise FileParserError(f"PDFè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def get_pdf_info(pdf_path: str) -> Dict[str, Any]:
        """è·å–PDFä¿¡æ¯"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                return {
                    "pages": len(pdf.pages),
                    "metadata": pdf.metadata or {}
                }
        except Exception:
            return {"pages": 0, "metadata": {}}


class OfficeParser:
    """Officeæ–‡æ¡£è§£æå™¨"""

    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """ä»DOCXæå–æ–‡æœ¬"""
        try:
            return pypandoc.convert_file(file_path, 'markdown', extra_args=['--extract-media=.'])
        except Exception as e:
            raise FileParserError(f"DOCXè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def extract_text_from_doc(file_path: str) -> str:
        """ä»DOCæå–æ–‡æœ¬ï¼ˆéœ€è¦antiwordï¼‰"""
        try:
            result = subprocess.run(
                ['antiword', file_path],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                return result.stdout
            else:
                raise FileParserError(f"DOCè½¬æ¢å¤±è´¥: {result.stderr}")
        except subprocess.TimeoutExpired:
            raise FileParserError("DOCè§£æè¶…æ—¶")
        except FileNotFoundError:
            raise FileParserError("æœªæ‰¾åˆ°antiwordå·¥å…·ï¼Œè¯·å®‰è£…: sudo apt-get install antiword")
        except Exception as e:
            raise FileParserError(f"DOCè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def extract_text_from_pptx(file_path: str) -> str:
        """ä»PPTXæå–æ–‡æœ¬"""
        try:
            prs = Presentation(file_path)
            slides_text = []

            for slide_num, slide in enumerate(prs.slides):
                slide_content = [f"=== å¹»ç¯ç‰‡ {slide_num + 1} ==="]

                # æŒ‰ä½ç½®æ’åºå½¢çŠ¶
            shapes = sorted(slide.shapes, key=lambda x: (x.top, x.left))

            for shape in shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    slide_content.append(shape.text.strip())

                slides_text.append('\n'.join(slide_content))

            return '\n\n'.join(slides_text)

        except Exception as e:
            raise FileParserError(f"PPTXè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def extract_text_from_ppt(file_path: str) -> str:
        """ä»PPTæå–æ–‡æœ¬ï¼ˆéœ€è¦aspose.slidesï¼‰"""
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_pptx = os.path.join(temp_dir, "temp.pptx")

                with slides.Presentation(file_path) as presentation:
                    presentation.save(temp_pptx, slides.export.SaveFormat.PPTX)

                return OfficeParser.extract_text_from_pptx(temp_pptx)

        except Exception as e:
            raise FileParserError(f"PPTè§£æå¤±è´¥: {str(e)}")


class ExcelParser:
    """Excelè§£æå™¨"""

    @staticmethod
    def extract_text_from_xlsx(file_path: str) -> str:
        """ä»Excelæå–æ–‡æœ¬å¹¶è½¬æ¢ä¸ºMarkdown"""
        try:
            excel_data = ExcelParser._read_excel_to_dict(file_path)
            markdown_tables = []

            for sheet_name, sheet_data in excel_data.items():
                # é™åˆ¶è¡Œæ•°
                if len(sheet_data) > 100:
                    sheet_data = sheet_data[:100]

                sheet_md = ExcelParser._sheet_data_to_markdown(sheet_data, sheet_name)
                markdown_tables.append(sheet_md)

            return '\n\n'.join(markdown_tables)

        except Exception as e:
            raise FileParserError(f"Excelè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def _read_excel_to_dict(file_path: str) -> Dict[str, List[List[str]]]:
        """è¯»å–Excelæ–‡ä»¶åˆ°å­—å…¸ï¼Œæ­£ç¡®å¤„ç†åˆå¹¶å•å…ƒæ ¼"""
        # éœ€è¦å…³é—­read_onlyæ¨¡å¼æ‰èƒ½è®¿é—®åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯
        workbook = load_workbook(file_path, data_only=True, read_only=False)
        excel_data = {}

        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]

            # åˆ›å»ºåˆå¹¶å•å…ƒæ ¼å€¼æ˜ å°„
            merged_cell_values = {}
            for merged_range in sheet.merged_cells.ranges:
                # è·å–åˆå¹¶å•å…ƒæ ¼å·¦ä¸Šè§’çš„å€¼
                top_left_cell = sheet.cell(merged_range.min_row, merged_range.min_col)
                value = top_left_cell.value

                # ä¸ºåˆå¹¶èŒƒå›´å†…çš„æ‰€æœ‰å•å…ƒæ ¼è®¾ç½®ç›¸åŒçš„å€¼
                for row in range(merged_range.min_row, merged_range.max_row + 1):
                    for col in range(merged_range.min_col, merged_range.max_col + 1):
                        merged_cell_values[(row, col)] = value

            # è¯»å–æ•°æ®ï¼Œè€ƒè™‘åˆå¹¶å•å…ƒæ ¼
            sheet_data = []
            max_row = sheet.max_row
            max_col = sheet.max_column

            if max_row and max_col:
                for row_idx in range(1, max_row + 1):
                    row_data = []
                    for col_idx in range(1, max_col + 1):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶å•å…ƒæ ¼
                        if (row_idx, col_idx) in merged_cell_values:
                            cell_value = merged_cell_values[(row_idx, col_idx)]
                        else:
                            cell = sheet.cell(row_idx, col_idx)
                            cell_value = cell.value

                        cell_str = str(cell_value).replace('\n', '\\n') if cell_value is not None else ''
                        row_data.append(cell_str)

                    sheet_data.append(row_data)

                if not sheet_data:
                    continue

                # æ¸…ç†ç©ºè¡Œå’Œç©ºåˆ—
                sheet_data = ExcelParser._clean_empty_rows_cols(sheet_data)

                if sheet_data:
                    excel_data[sheet_name] = sheet_data

        workbook.close()
        return excel_data

    @staticmethod
    def _clean_empty_rows_cols(data: List[List[str]]) -> List[List[str]]:
        """æ¸…ç†ç©ºè¡Œå’Œç©ºåˆ—"""
        if not data:
            return data

        # ç§»é™¤ç©ºè¡Œ
        data = [row for row in data if any(cell.strip() for cell in row)]

        if not data:
            return data

        # ç§»é™¤ç©ºåˆ—
        cols_to_keep = []
        for col_idx in range(len(data[0])):
            if any(row[col_idx].strip() for row in data):
                cols_to_keep.append(col_idx)

        if cols_to_keep:
            data = [[row[i] for i in cols_to_keep] for row in data]

        return data

    @staticmethod
    def _sheet_data_to_markdown(sheet_data: List[List[str]], sheet_name: str) -> str:
        """è½¬æ¢å·¥ä½œè¡¨æ•°æ®ä¸ºMarkdown"""
        if not sheet_data:
            return f'## {sheet_name}\n\n(ç©ºå·¥ä½œè¡¨)'

        markdown_lines = [f'## {sheet_name}', '']

        # å¦‚æœæœ‰æ•°æ®ï¼Œç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
        if sheet_data:
            # è¡¨å¤´
            header = '| ' + ' | '.join(cell if cell else ' ' for cell in sheet_data[0]) + ' |'
            markdown_lines.append(header)

            # åˆ†éš”çº¿
            separator = '| ' + ' | '.join('---' for _ in sheet_data[0]) + ' |'
            markdown_lines.append(separator)

            # æ•°æ®è¡Œ
            for row in sheet_data[1:]:
                row_md = '| ' + ' | '.join(cell if cell else ' ' for cell in row) + ' |'
                markdown_lines.append(row_md)

        return '\n'.join(markdown_lines)


class WebParser:
    """ç½‘é¡µè§£æå™¨"""

    @staticmethod
    def extract_text_from_html(file_path: str) -> str:
        """ä»HTMLæ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            encoding = TextProcessor.detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding) as file:
                html_content = file.read()

            return WebParser._html_to_text(html_content)

        except Exception as e:
            raise FileParserError(f"HTMLè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def extract_text_from_url(url: str, timeout: int = 30) -> str:
        """ä»URLæå–æ–‡æœ¬"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()

            return WebParser._html_to_text(response.text)

        except requests.RequestException as e:
            raise FileParserError(f"URLè®¿é—®å¤±è´¥: {str(e)}")
        except Exception as e:
            raise FileParserError(f"URLè§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def _html_to_text(html_content: str) -> str:
        """HTMLè½¬æ–‡æœ¬"""
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False
        h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
        return h.handle(html_content)


class PlainTextParser:
    """çº¯æ–‡æœ¬è§£æå™¨"""

    @staticmethod
    def extract_text_from_plain_file(file_path: str) -> str:
        """ä»çº¯æ–‡æœ¬æ–‡ä»¶æå–å†…å®¹"""
        try:
            encoding = TextProcessor.detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding) as file:
                return file.read()
        except Exception as e:
            raise FileParserError(f"æ–‡æœ¬æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")

    @staticmethod
    def extract_text_with_pandoc(file_path: str, input_format: str = None) -> str:
        """ä½¿ç”¨Pandocæå–æ–‡æœ¬"""
        try:
            if input_format:
                return pypandoc.convert_file(file_path, 'markdown', format=input_format)
            else:
                return pypandoc.convert_file(file_path, 'markdown')
        except Exception as e:
            raise FileParserError(f"Pandocè§£æå¤±è´¥: {str(e)}")

# ==================== MCP å·¥å…·å‡½æ•° ====================


@mcp.tool()
async def extract_text_from_file(
    input_file_path: str,
    start_index: int = 0,
    max_length: int = 5000,
    include_metadata: bool = False
) -> Dict[str, Any]:
    """
    ä»æœ¬åœ°çš„å„ç§æ ¼å¼çš„æ–‡ä»¶ä¸­æå–æ–¹ä¾¿é˜…è¯»çš„markdownæ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæ˜¯ç½‘ç»œä¸Šçš„æ–‡ä»¶ï¼Œè¯·å…ˆè¿›è¡Œä¸‹è½½åˆ°æœ¬åœ°ç£ç›˜ã€‚

    æ”¯æŒçš„æ ¼å¼ï¼šPDF, DOCX, DOC, PPTX, PPT, XLSX, XLS, TXT, CSV, JSON, XML, HTML, MDç­‰

    Args:
        input_file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæœ¬åœ°çš„ç»å¯¹è·¯å¾„
        start_index: å¼€å§‹æå–çš„å­—ç¬¦ä½ç½®ï¼ˆé»˜è®¤0ï¼‰
        max_length: æœ€å¤§æå–é•¿åº¦ï¼ˆé»˜è®¤5000å­—ç¬¦ï¼‰
        include_metadata: æ˜¯å¦åŒ…å«æ–‡ä»¶å…ƒæ•°æ®ï¼ˆé»˜è®¤Falseï¼‰

    Returns:
        åŒ…å«æå–æ–‡æœ¬å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
    """
    start_time = time.time()
    operation_id = hashlib.md5(f"extract_{input_file_path}_{time.time()}".encode()).hexdigest()[:8]
    logger.info(f"ğŸ“„ extract_text_from_fileå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {input_file_path}")
    logger.info(f"ğŸ”§ å‚æ•°: start_index={start_index}, max_length={max_length}, include_metadata={include_metadata}")

    try:
        # éªŒè¯æ–‡ä»¶
        logger.debug("ğŸ” å¼€å§‹æ–‡ä»¶éªŒè¯")
        validation_result = FileValidator.validate_file(input_file_path)

        if not validation_result["valid"]:
            error_time = time.time() - start_time
            logger.error(f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥ [{operation_id}] - é”™è¯¯: {validation_result['error']}, è€—æ—¶: {error_time:.2f}ç§’")
            return {
                "success": False,
                "error": validation_result["error"],
                "file_path": input_file_path,
                "execution_time": error_time,
                "operation_id": operation_id
            }

        file_extension = validation_result["file_extension"]
        file_size_mb = validation_result["file_size_mb"]
        logger.info(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡ [{operation_id}] - æ ¼å¼: {file_extension}, å¤§å°: {file_size_mb:.2f}MB")

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£æå™¨
        parse_start_time = time.time()
        logger.info(f"ğŸ”§ å¼€å§‹æ–‡ä»¶è§£æ - æ ¼å¼: {file_extension}")

        extracted_text = ""
        metadata = {}

        try:
            if file_extension == '.pdf':
                logger.debug("ğŸ“• ä½¿ç”¨PDFè§£æå™¨")
                extracted_text = PDFParser.extract_text(input_file_path)
                if include_metadata:
                    metadata = PDFParser.get_pdf_info(input_file_path)

            elif file_extension in ['.docx', '.doc']:
                logger.debug("ğŸ“ ä½¿ç”¨Wordè§£æå™¨")
                if file_extension == '.docx':
                    extracted_text = OfficeParser.extract_text_from_docx(input_file_path)
                else:
                    extracted_text = OfficeParser.extract_text_from_doc(input_file_path)

            elif file_extension in ['.pptx', '.ppt']:
                logger.debug("ğŸ“Š ä½¿ç”¨PowerPointè§£æå™¨")
                if file_extension == '.pptx':
                    extracted_text = OfficeParser.extract_text_from_pptx(input_file_path)
                else:
                    extracted_text = OfficeParser.extract_text_from_ppt(input_file_path)

            elif file_extension in ['.xlsx', '.xls']:
                logger.debug("ğŸ“ˆ ä½¿ç”¨Excelè§£æå™¨")
                extracted_text = ExcelParser.extract_text_from_xlsx(input_file_path)

            elif file_extension in ['.html', '.htm']:
                logger.debug("ğŸŒ ä½¿ç”¨HTMLè§£æå™¨")
                extracted_text = WebParser.extract_text_from_html(input_file_path)

            elif file_extension in ['.txt', '.csv', '.json', '.xml', '.md', '.markdown']:
                logger.debug("ğŸ“„ ä½¿ç”¨çº¯æ–‡æœ¬è§£æå™¨")
                extracted_text = PlainTextParser.extract_text_from_plain_file(input_file_path)

            else:
                # å°è¯•ä½¿ç”¨Pandocè§£æ
                logger.debug("ğŸ”§ å°è¯•ä½¿ç”¨Pandocè§£æå™¨")
                extracted_text = PlainTextParser.extract_text_with_pandoc(input_file_path)

            parse_time = time.time() - parse_start_time
            logger.info(f"âœ… æ–‡ä»¶è§£ææˆåŠŸ [{operation_id}] - åŸå§‹æ–‡æœ¬é•¿åº¦: {len(extracted_text)}, è§£æè€—æ—¶: {parse_time:.2f}ç§’")

        except Exception as parse_error:
            parse_time = time.time() - parse_start_time
            logger.warning(f"âš ï¸ ä¸»è§£æå™¨å¤±è´¥ [{operation_id}] - é”™è¯¯: {str(parse_error)}, è€—æ—¶: {parse_time:.2f}ç§’")
            logger.debug("ğŸ”§ å°è¯•Pandocå¤‡ç”¨è§£æå™¨")

            try:
                extracted_text = PlainTextParser.extract_text_with_pandoc(input_file_path)
                logger.info(f"âœ… Pandocå¤‡ç”¨è§£ææˆåŠŸ [{operation_id}] - æ–‡æœ¬é•¿åº¦: {len(extracted_text)}")
            except Exception as fallback_error:
                error_time = time.time() - start_time
                logger.error(f"âŒ æ‰€æœ‰è§£æå™¨éƒ½å¤±è´¥ [{operation_id}] - ä¸»é”™è¯¯: {str(parse_error)}, å¤‡ç”¨é”™è¯¯: {str(fallback_error)}, è€—æ—¶: {error_time:.2f}ç§’")
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶è§£æå¤±è´¥ - ä¸»é”™è¯¯: {str(parse_error)}, å¤‡ç”¨é”™è¯¯: {str(fallback_error)}",
                    "file_path": input_file_path,
                    "execution_time": error_time,
                    "operation_id": operation_id
                }

        # æ¸…ç†å’Œå¤„ç†æ–‡æœ¬
        logger.debug("ğŸ§¹ å¼€å§‹æ–‡æœ¬æ¸…ç†å’Œå¤„ç†")
        cleaned_text = TextProcessor.clean_text(extracted_text)
        truncated_text = TextProcessor.truncate_text(cleaned_text, start_index, max_length)
        text_stats = TextProcessor.get_text_stats(cleaned_text)

        total_time = time.time() - start_time

        logger.info(f"âœ… æ–‡æœ¬æå–å®Œæˆ [{operation_id}] - æ¸…ç†åé•¿åº¦: {len(cleaned_text)}, æˆªå–é•¿åº¦: {len(truncated_text)}, æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.debug(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡: {text_stats}")

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "text": truncated_text,
            "file_info": {
                "file_path": input_file_path,
                "file_extension": file_extension,
                "file_size_mb": round(file_size_mb, 2),
                "mime_type": validation_result["mime_type"]
            },
            "text_info": {
                "original_length": len(extracted_text),
                "cleaned_length": len(cleaned_text),
                "extracted_length": len(truncated_text),
                "start_index": start_index,
                "max_length": max_length,
                **text_stats
            },
            "execution_time": total_time,
            "operation_id": operation_id
        }

        if include_metadata and metadata:
            result["metadata"] = metadata
            logger.debug(f"ğŸ“‹ åŒ…å«å…ƒæ•°æ®: {len(metadata)} é¡¹")

        return result

    except Exception as e:
        error_time = time.time() - start_time
        logger.error(f"ğŸ’¥ æ–‡æœ¬æå–å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’")
        logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

        return {
            "success": False,
            "error": str(e),
            "file_path": input_file_path,
            "execution_time": error_time,
            "operation_id": operation_id
        }


@mcp.tool()
async def extract_text_from_url(
    url: str,
    start_index: int = 0,
    max_length: int = 5000,
    timeout: int = 30
) -> Dict[str, Any]:
    """
    ä»URLæå–ç½‘é¡µhtmlæ–‡æœ¬å†…å®¹

    Args:
        url: ç›®æ ‡URLåœ°å€
        start_index: å¼€å§‹æå–çš„å­—ç¬¦ä½ç½®ï¼ˆé»˜è®¤0ï¼‰
        max_length: æœ€å¤§æå–é•¿åº¦ï¼ˆé»˜è®¤5000å­—ç¬¦ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤30ç§’ï¼‰

    Returns:
        åŒ…å«æå–æ–‡æœ¬å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
    """
    start_time = time.time()
    operation_id = hashlib.md5(f"url_{url}_{time.time()}".encode()).hexdigest()[:8]
    logger.info(f"ğŸŒ extract_text_from_urlå¼€å§‹æ‰§è¡Œ [{operation_id}] - URL: {url}")
    logger.info(f"ğŸ”§ å‚æ•°: start_index={start_index}, max_length={max_length}, timeout={timeout}ç§’")

    try:
        # éªŒè¯URLæ ¼å¼
        logger.debug("ğŸ” éªŒè¯URLæ ¼å¼")
        if not url.startswith(('http://', 'https://')):
            error_time = time.time() - start_time
            logger.error(f"âŒ URLæ ¼å¼æ— æ•ˆ [{operation_id}] - URL: {url}, è€—æ—¶: {error_time:.2f}ç§’")
            return {
                "success": False,
                "error": "URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´",
                "url": url,
                "execution_time": error_time,
                "operation_id": operation_id
            }

        # æå–ç½‘é¡µå†…å®¹
        fetch_start_time = time.time()
        logger.info("ğŸŒ å¼€å§‹è·å–ç½‘é¡µå†…å®¹")

        extracted_text = WebParser.extract_text_from_url(url, timeout)

        fetch_time = time.time() - fetch_start_time
        logger.info(f"âœ… ç½‘é¡µå†…å®¹è·å–æˆåŠŸ [{operation_id}] - åŸå§‹æ–‡æœ¬é•¿åº¦: {len(extracted_text)}, è·å–è€—æ—¶: {fetch_time:.2f}ç§’")

        # æ¸…ç†å’Œå¤„ç†æ–‡æœ¬
        logger.debug("ğŸ§¹ å¼€å§‹æ–‡æœ¬æ¸…ç†å’Œå¤„ç†")
        cleaned_text = TextProcessor.clean_text(extracted_text)
        truncated_text = TextProcessor.truncate_text(cleaned_text, start_index, max_length)
        text_stats = TextProcessor.get_text_stats(cleaned_text)

        total_time = time.time() - start_time

        logger.info(f"âœ… URLæ–‡æœ¬æå–å®Œæˆ [{operation_id}] - æ¸…ç†åé•¿åº¦: {len(cleaned_text)}, æˆªå–é•¿åº¦: {len(truncated_text)}, æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.debug(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡: {text_stats}")

        return {
            "success": True,
            "text": truncated_text,
            "url_info": {
                "url": url,
                "timeout": timeout,
                "fetch_time": fetch_time
            },
            "text_info": {
                "original_length": len(extracted_text),
                "cleaned_length": len(cleaned_text),
                "extracted_length": len(truncated_text),
                "start_index": start_index,
                "max_length": max_length,
                **text_stats
            },
            "execution_time": total_time,
            "operation_id": operation_id
        }

    except Exception as e:
        error_time = time.time() - start_time
        logger.error(f"ğŸ’¥ URLæ–‡æœ¬æå–å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’")
        logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

        return {
            "success": False,
            "error": str(e),
            "url": url,
            "execution_time": error_time,
            "operation_id": operation_id
        }


@mcp.tool()
async def batch_extract_text(
    file_paths: List[str],
    max_length: int = 3000,
    include_metadata: bool = False
) -> Dict[str, Any]:
    """æ‰¹é‡æå–å¤šä¸ªæ–‡ä»¶çš„æ–‡æœ¬

    Args:
        file_paths (List[str]): æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        max_length (int): æ¯ä¸ªæ–‡ä»¶çš„æœ€å¤§æå–é•¿åº¦ï¼Œé»˜è®¤3000
        include_metadata (bool): æ˜¯å¦åŒ…å«å…ƒæ•°æ®ï¼Œé»˜è®¤False

    Returns:
        Dict: æ‰¹é‡å¤„ç†ç»“æœ
    """
    start_time = time.time()
    results = []
    successful = 0
    failed = 0

    for file_path in file_paths:
        try:
            result = await extract_text_from_file(
                input_file_path=file_path,
                start_index=0,
                max_length=max_length,
                include_metadata=include_metadata
            )

            if result["status"] == "success":
                successful += 1
            else:
                failed += 1

            results.append({
                "file_path": file_path,
                "result": result
            })

        except Exception as e:
            failed += 1
            results.append({
                "file_path": file_path,
                "result": {
                    "status": "error",
                    "message": f"å¤„ç†å¤±è´¥: {str(e)}",
                    "text": "",
                    "length": 0
                }
            })

    processing_time = time.time() - start_time

    return {
        "status": "success",
        "message": f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ{successful}ä¸ªï¼Œå¤±è´¥{failed}ä¸ª",
        "summary": {
            "total_files": len(file_paths),
            "successful": successful,
            "failed": failed,
            "processing_time": round(processing_time, 2)
        },
        "results": results
    }

if __name__ == "__main__":
    app = Starlette(
        routes=[
            Mount('/', app=mcp.sse_app()),
        ]
    )

    uvicorn.run(app, host="0.0.0.0", port=34001)

import httpx
from mcp.server.fastmcp import FastMCP
from starlette.applications import Starlette
from starlette.routing import Mount, Host
import uvicorn
from typing import List, Dict, Any, Union, Optional, Tuple
import argparse
import json
import os
import shutil
import tempfile
import hashlib
import mimetypes
import logging
import time
from datetime import datetime
from pathlib import Path
import urllib.parse
import requests
import asyncio
import stat
import platform
import zipfile
import tarfile
import re
import chardet
import traceback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

mcp = FastMCP("File System")

parser = argparse.ArgumentParser(description="å¯åŠ¨æ–‡ä»¶ç³»ç»Ÿ MCP Server")
args = parser.parse_args()


class FileSystemError(Exception):
    """æ–‡ä»¶ç³»ç»Ÿå¼‚å¸¸"""

    pass


class SecurityValidator:
    """å®‰å…¨éªŒè¯å™¨"""

    # å±é™©çš„æ–‡ä»¶æ‰©å±•å
    DANGEROUS_EXTENSIONS = {
        ".exe",
        ".bat",
        ".cmd",
        ".com",
        ".pif",
        ".scr",
        ".vbs",
        ".js",
        ".jar",
        ".app",
        ".deb",
        ".pkg",
        ".rpm",
        ".dmg",
        ".iso",
    }

    # ç³»ç»Ÿå…³é”®ç›®å½•ï¼ˆç¦æ­¢æ“ä½œï¼‰
    PROTECTED_PATHS = {
        "/System",
        "/usr/bin",
        "/usr/sbin",
        "/bin",
        "/sbin",
        "/Windows/System32",
        "/Windows/SysWOW64",
        "/Program Files",
        "/Program Files (x86)",
    }

    @staticmethod
    def validate_path(file_path: str, allow_dangerous: bool = False) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶è·¯å¾„çš„å®‰å…¨æ€§"""
        try:
            # æ£€æŸ¥è·¯å¾„éå†æ”»å‡»ï¼ˆåœ¨è§£æå‰æ£€æŸ¥ï¼‰
            if ".." in file_path:
                return {"valid": False, "error": "è·¯å¾„åŒ…å«å±é™©çš„éå†å­—ç¬¦"}

            path = Path(file_path).resolve()

            # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
            if not path.is_absolute():
                return {"valid": False, "error": "å¿…é¡»æä¾›ç»å¯¹è·¯å¾„"}

            # æ£€æŸ¥ç³»ç»Ÿä¿æŠ¤ç›®å½•
            path_str = str(path)
            for protected in SecurityValidator.PROTECTED_PATHS:
                if path_str.startswith(protected):
                    return {
                        "valid": False,
                        "error": f"ç¦æ­¢è®¿é—®ç³»ç»Ÿä¿æŠ¤ç›®å½•: {protected}",
                    }

            # æ£€æŸ¥å±é™©æ–‡ä»¶æ‰©å±•å
            if (
                not allow_dangerous
                and path.suffix.lower() in SecurityValidator.DANGEROUS_EXTENSIONS
            ):
                return {"valid": False, "error": f"å±é™©çš„æ–‡ä»¶ç±»å‹: {path.suffix}"}

            return {"valid": True, "resolved_path": str(path)}

        except Exception as e:
            return {"valid": False, "error": f"è·¯å¾„éªŒè¯å¤±è´¥: {str(e)}"}


class FileMetadata:
    """æ–‡ä»¶å…ƒæ•°æ®ç®¡ç†å™¨"""

    @staticmethod
    def get_file_info(file_path: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯"""
        try:
            path = Path(file_path)

            if not path.exists():
                return {"exists": False}

            stat_info = path.stat()

            # åŸºç¡€ä¿¡æ¯
            info = {
                "exists": True,
                "name": path.name,
                "absolute_path": str(path.absolute()),
                "size_bytes": stat_info.st_size,
                "size_mb": round(stat_info.st_size / (1024 * 1024), 2),
                "is_file": path.is_file(),
                "is_dir": path.is_dir(),
                "is_symlink": path.is_symlink(),
                "created_time": datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                "accessed_time": datetime.fromtimestamp(stat_info.st_atime).isoformat(),
            }

            # æ–‡ä»¶ç‰¹å®šä¿¡æ¯
            if path.is_file():
                info.update(
                    {
                        "extension": path.suffix.lower(),
                        "mime_type": mimetypes.guess_type(str(path))[0] or "unknown",
                        "encoding": (
                            FileMetadata._detect_encoding(file_path)
                            if path.suffix.lower()
                            in [".txt", ".py", ".js", ".css", ".html", ".md"]
                            else None
                        ),
                    }
                )

            # æƒé™ä¿¡æ¯
            info["permissions"] = {
                "readable": os.access(file_path, os.R_OK),
                "writable": os.access(file_path, os.W_OK),
                "executable": os.access(file_path, os.X_OK),
                "mode": (
                    oct(stat_info.st_mode)[-3:]
                    if platform.system() != "Windows"
                    else None
                ),
            }

            return info

        except Exception as e:
            return {"exists": False, "error": str(e)}

    @staticmethod
    def _detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, "rb") as f:
                raw_data = f.read(10000)
                result = chardet.detect(raw_data)
                return result.get("encoding", "utf-8")
        except Exception:
            return "utf-8"


class CloudStorage:
    """äº‘å­˜å‚¨ç®¡ç†å™¨"""

    DEFAULT_UPLOAD_URL = "http://36.133.44.114:20034/askonce/api/v1/doc/upload"
    DEFAULT_HEADERS = {"User-Source": "AskOnce_bakend"}

    @staticmethod
    async def upload_file(
        file_path: str, upload_url: str = None, headers: Dict = None
    ) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨"""
        start_time = time.time()
        operation_id = hashlib.md5(
            f"upload_{file_path}_{time.time()}".encode()
        ).hexdigest()[:8]
        logger.info(
            f"â˜ï¸ CloudStorage.upload_fileå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}"
        )

        try:
            if not os.path.exists(file_path):
                error_time = time.time() - start_time
                logger.error(
                    f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ [{operation_id}] - æ–‡ä»¶: {file_path}, è€—æ—¶: {error_time:.2f}ç§’"
                )
                return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶100MBï¼‰
            file_size = os.path.getsize(file_path)
            file_size_mb = file_size / (1024 * 1024)
            logger.info(f"ğŸ“Š æ–‡ä»¶ä¿¡æ¯ [{operation_id}] - å¤§å°: {file_size_mb:.2f}MB")

            if file_size > 100 * 1024 * 1024:
                error_time = time.time() - start_time
                logger.error(
                    f"âŒ æ–‡ä»¶è¿‡å¤§ [{operation_id}] - å¤§å°: {file_size_mb:.2f}MB > 100MB, è€—æ—¶: {error_time:.2f}ç§’"
                )
                return {"status": "error", "message": "æ–‡ä»¶è¿‡å¤§ï¼Œè¶…è¿‡100MBé™åˆ¶"}

            file_name = os.path.basename(file_path)
            url = upload_url or CloudStorage.DEFAULT_UPLOAD_URL
            request_headers = headers or CloudStorage.DEFAULT_HEADERS

            logger.debug(f"ğŸ”— ä¸Šä¼ é…ç½® [{operation_id}] - URL: {url}")
            logger.debug(f"ğŸ“‹ è¯·æ±‚å¤´ [{operation_id}] - Headers: {request_headers}")
            logger.info(f"ğŸ“ å‡†å¤‡ä¸Šä¼ æ–‡ä»¶ [{operation_id}] - æ–‡ä»¶å: {file_name}")

            # å‘èµ·ä¸Šä¼ è¯·æ±‚
            upload_start_time = time.time()
            with open(file_path, "rb") as f:
                files = {"file": (file_name, f, "application/octet-stream")}
                logger.info(f"ğŸŒ å¼€å§‹ä¸Šä¼ è¯·æ±‚ [{operation_id}]")

                response = requests.post(
                    url, headers=request_headers, files=files, timeout=60
                )

            upload_time = time.time() - upload_start_time
            logger.info(
                f"â±ï¸ ä¸Šä¼ è¯·æ±‚å®Œæˆ [{operation_id}] - çŠ¶æ€ç : {response.status_code}, ä¸Šä¼ è€—æ—¶: {upload_time:.2f}ç§’"
            )

            response.raise_for_status()

            try:
                json_data = response.json()
                logger.debug(f"ğŸ“„ APIå“åº”å†…å®¹ [{operation_id}] - {json_data}")

                # è·å–æ–‡ä»¶URL - ç›´æ¥è·å–ï¼Œç±»ä¼¼åŸä»£ç é€»è¾‘
                file_url = json_data.get("data", {}).get("url")
                file_id = json_data.get("data", {}).get("fileId")

                if not file_url:
                    error_time = time.time() - start_time
                    logger.error(
                        f"âŒ APIå“åº”ä¸­ç¼ºå°‘URL [{operation_id}] - å®Œæ•´å“åº”: {json_data}, è€—æ—¶: {error_time:.2f}ç§’"
                    )
                    return {
                        "status": "error",
                        "message": "APIè¿”å›æˆåŠŸä½†ç¼ºå°‘æ–‡ä»¶URL",
                        "response": json_data,
                    }

                total_time = time.time() - start_time
                logger.info(
                    f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ [{operation_id}] - URL: {file_url}, æ–‡ä»¶ID: {file_id}, æ€»è€—æ—¶: {total_time:.2f}ç§’"
                )

                return {
                    "status": "success",
                    "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
                    "url": file_url,
                    "file_id": file_id,
                    "file_name": file_name,
                    "file_size": file_size,
                    "file_size_mb": file_size_mb,
                    "upload_time": upload_time,
                    "total_time": total_time,
                    "operation_id": operation_id,
                }

            except json.JSONDecodeError as e:
                error_time = time.time() - start_time
                logger.error(
                    f"âŒ JSONè§£æå¤±è´¥ [{operation_id}] - é”™è¯¯: {str(e)}, å“åº”: {response.text[:500]}, è€—æ—¶: {error_time:.2f}ç§’"
                )
                return {
                    "status": "error",
                    "message": "æœåŠ¡å™¨å“åº”æ ¼å¼é”™è¯¯",
                    "response": response.text,
                }

        except requests.exceptions.Timeout:
            error_time = time.time() - start_time
            logger.error(f"â° ä¸Šä¼ è¶…æ—¶ [{operation_id}] - è€—æ—¶: {error_time:.2f}ç§’")
            return {"status": "error", "message": "ä¸Šä¼ è¶…æ—¶"}
        except requests.exceptions.RequestException as e:
            error_time = time.time() - start_time
            logger.error(
                f"ğŸ’¥ ç½‘ç»œè¯·æ±‚å¤±è´¥ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            error_time = time.time() - start_time
            logger.error(
                f"ğŸ’¥ ä¸Šä¼ å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            return {"status": "error", "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}


# ==================== MCP å·¥å…·å‡½æ•° ====================


@mcp.tool()
async def file_read(
    file_path: str,
    start_line: int = 0,
    end_line: Optional[int] = None,
    encoding: str = "auto",
    max_size_mb: float = 10.0,
) -> Dict[str, Any]:
    """é«˜çº§æ–‡ä»¶è¯»å–å·¥å…·

    Args:
        file_path (str): æ–‡ä»¶ç»å¯¹è·¯å¾„
        start_line (int): å¼€å§‹è¡Œå·ï¼Œé»˜è®¤0
        end_line (Optional[int]): ç»“æŸè¡Œå·ï¼ˆä¸åŒ…å«ï¼‰ï¼ŒNoneè¡¨ç¤ºè¯»å–åˆ°æœ«å°¾
        encoding (str): æ–‡ä»¶ç¼–ç ï¼Œ'auto'è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        max_size_mb (float): æœ€å¤§è¯»å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤10MB

    Returns:
        Dict: åŒ…å«æ–‡ä»¶å†…å®¹å’Œå…ƒä¿¡æ¯
    """
    start_time = time.time()
    operation_id = hashlib.md5(f"read_{file_path}_{time.time()}".encode()).hexdigest()[
        :8
    ]
    logger.info(f"ğŸ“– file_readå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}")
    logger.info(
        f"ğŸ”§ å‚æ•°: start_line={start_line}, end_line={end_line}, encoding={encoding}, max_size_mb={max_size_mb}"
    )

    try:
        # å®‰å…¨éªŒè¯
        logger.debug(f"ğŸ”’ å¼€å§‹å®‰å…¨éªŒè¯ [{operation_id}]")
        validation = SecurityValidator.validate_path(file_path)
        if not validation["valid"]:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ å®‰å…¨éªŒè¯å¤±è´¥ [{operation_id}] - é”™è¯¯: {validation['error']}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": validation["error"]}

        file_path = validation["resolved_path"]
        logger.info(f"âœ… å®‰å…¨éªŒè¯é€šè¿‡ [{operation_id}] - è§£æè·¯å¾„: {file_path}")

        # è·å–æ–‡ä»¶ä¿¡æ¯
        logger.debug(f"ğŸ“Š è·å–æ–‡ä»¶ä¿¡æ¯ [{operation_id}]")
        file_info = FileMetadata.get_file_info(file_path)
        if not file_info["exists"]:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ [{operation_id}] - è·¯å¾„: {file_path}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}

        if not file_info["is_file"]:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ ä¸æ˜¯æ–‡ä»¶ [{operation_id}] - è·¯å¾„: {file_path}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": "æŒ‡å®šè·¯å¾„ä¸æ˜¯æ–‡ä»¶"}

        if not file_info["permissions"]["readable"]:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ æ–‡ä»¶ä¸å¯è¯» [{operation_id}] - è·¯å¾„: {file_path}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": "æ–‡ä»¶æ— è¯»å–æƒé™"}

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if file_info["size_mb"] > max_size_mb:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ æ–‡ä»¶è¿‡å¤§ [{operation_id}] - å¤§å°: {file_info['size_mb']:.2f}MB > {max_size_mb}MB, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {
                "status": "error",
                "message": f"æ–‡ä»¶è¿‡å¤§: {file_info['size_mb']:.2f}MB > {max_size_mb}MB",
            }

        logger.info(
            f"ğŸ“Š æ–‡ä»¶ä¿¡æ¯ [{operation_id}] - å¤§å°: {file_info['size_mb']:.2f}MB, æƒé™: å¯è¯»"
        )

        # æ£€æµ‹ç¼–ç 
        if encoding == "auto":
            encoding = file_info.get("encoding", "utf-8")
            logger.debug(f"ğŸ”¤ è‡ªåŠ¨æ£€æµ‹ç¼–ç  [{operation_id}] - ç¼–ç : {encoding}")

        # è¯»å–æ–‡ä»¶å†…å®¹
        read_start_time = time.time()
        logger.info(f"ğŸ“– å¼€å§‹è¯»å–æ–‡ä»¶å†…å®¹ [{operation_id}]")

        with open(file_path, "r", encoding=encoding) as f:
            lines = f.readlines()

        read_time = time.time() - read_start_time

        # å¤„ç†è¡ŒèŒƒå›´
        total_lines = len(lines)
        if end_line is None:
            end_line = total_lines

        start_line = max(0, start_line)
        end_line = min(total_lines, end_line)

        if start_line >= total_lines:
            content = ""
        else:
            content = "".join(lines[start_line:end_line])

        total_time = time.time() - start_time

        logger.info(
            f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ [{operation_id}] - æ€»è¡Œæ•°: {total_lines}, è¯»å–è¡Œæ•°: {end_line - start_line}, å†…å®¹é•¿åº¦: {len(content)}, è¯»å–è€—æ—¶: {read_time:.2f}ç§’, æ€»è€—æ—¶: {total_time:.2f}ç§’"
        )

        return {
            "status": "success",
            "message": f"æˆåŠŸè¯»å–æ–‡ä»¶ (è¡Œ {start_line}-{end_line})",
            "content": content,
            "file_info": {
                "path": file_path,
                "total_lines": total_lines,
                "read_lines": end_line - start_line,
                "encoding": encoding,
                "size_mb": file_info["size_mb"],
            },
            "line_range": {"start": start_line, "end": end_line, "total": total_lines},
            "execution_time": total_time,
            "operation_id": operation_id,
        }

    except UnicodeDecodeError as e:
        error_time = time.time() - start_time
        logger.error(
            f"âŒ ç¼–ç é”™è¯¯ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        return {
            "status": "error",
            "message": f"æ–‡ä»¶ç¼–ç é”™è¯¯: {str(e)}ï¼Œè¯·å°è¯•æŒ‡å®šæ­£ç¡®çš„ç¼–ç ",
        }
    except Exception as e:
        error_time = time.time() - start_time
        logger.error(
            f"ğŸ’¥ è¯»å–æ–‡ä»¶å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        return {"status": "error", "message": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"}


@mcp.tool()
async def file_write(
    file_path: str,
    content: str,
    mode: str = "overwrite",
    encoding: str = "utf-8",
    auto_upload: bool = True,
) -> Dict[str, Any]:
    """æ™ºèƒ½æ–‡ä»¶å†™å…¥å·¥å…·

    Args:
        file_path (str): æ–‡ä»¶ç»å¯¹è·¯å¾„
        content (str): è¦å†™å…¥çš„å†…å®¹
        mode (str): å†™å…¥æ¨¡å¼ - 'overwrite', 'append', 'prepend'
        encoding (str): æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤utf-8
        auto_upload (bool): æ˜¯å¦è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘ç«¯ï¼Œé»˜è®¤True

    Returns:
        Dict: æ“ä½œç»“æœå’Œæ–‡ä»¶ä¿¡æ¯
    """
    start_time = time.time()
    operation_id = hashlib.md5(f"write_{file_path}_{time.time()}".encode()).hexdigest()[
        :8
    ]
    logger.info(f"âœï¸ file_writeå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}")
    logger.info(
        f"ğŸ”§ å‚æ•°: mode={mode}, encoding={encoding}, auto_upload={auto_upload}, å†…å®¹é•¿åº¦: {len(content)}"
    )

    try:
        # å®‰å…¨éªŒè¯
        logger.debug(f"ğŸ”’ å¼€å§‹å®‰å…¨éªŒè¯ [{operation_id}]")
        validation = SecurityValidator.validate_path(file_path)
        if not validation["valid"]:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ å®‰å…¨éªŒè¯å¤±è´¥ [{operation_id}] - é”™è¯¯: {validation['error']}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": validation["error"]}

        file_path = validation["resolved_path"]
        path = Path(file_path)
        logger.info(f"âœ… å®‰å…¨éªŒè¯é€šè¿‡ [{operation_id}] - è§£æè·¯å¾„: {file_path}")

        # åˆ›å»ºç›®å½•ç»“æ„
        if not path.parent.exists():
            logger.info(f"ğŸ“ åˆ›å»ºç›®å½•ç»“æ„ [{operation_id}] - ç›®å½•: {path.parent}")
            path.parent.mkdir(parents=True, exist_ok=True)

        # å¤„ç†å†™å…¥æ¨¡å¼
        logger.debug(f"ğŸ“ å¤„ç†å†™å…¥æ¨¡å¼ [{operation_id}] - æ¨¡å¼: {mode}")
        if mode == "overwrite":
            write_mode = "w"
            final_content = content
        elif mode == "append":
            write_mode = "a"
            final_content = content
        elif mode == "prepend":
            write_mode = "w"
            if path.exists():
                logger.debug(f"ğŸ“– è¯»å–ç°æœ‰å†…å®¹ç”¨äºprepend [{operation_id}]")
                with open(file_path, "r", encoding=encoding) as f:
                    existing_content = f.read()
                final_content = content + existing_content
                logger.debug(
                    f"ğŸ“ åˆå¹¶å†…å®¹ [{operation_id}] - æ–°å†…å®¹é•¿åº¦: {len(content)}, åŸå†…å®¹é•¿åº¦: {len(existing_content)}, æœ€ç»ˆé•¿åº¦: {len(final_content)}"
                )
            else:
                final_content = content
        else:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ ä¸æ”¯æŒçš„å†™å…¥æ¨¡å¼ [{operation_id}] - æ¨¡å¼: {mode}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": f"ä¸æ”¯æŒçš„å†™å…¥æ¨¡å¼: {mode}"}

        # å†™å…¥æ–‡ä»¶
        write_start_time = time.time()
        logger.info(
            f"âœï¸ å¼€å§‹å†™å…¥æ–‡ä»¶ [{operation_id}] - æœ€ç»ˆå†…å®¹é•¿åº¦: {len(final_content)}"
        )

        with open(file_path, write_mode, encoding=encoding) as f:
            f.write(final_content)

        write_time = time.time() - write_start_time
        logger.info(f"âœ… æ–‡ä»¶å†™å…¥å®Œæˆ [{operation_id}] - å†™å…¥è€—æ—¶: {write_time:.2f}ç§’")

        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = FileMetadata.get_file_info(file_path)
        logger.debug(
            f"ğŸ“Š æ–‡ä»¶ä¿¡æ¯ [{operation_id}] - å¤§å°: {file_info['size_mb']:.2f}MB"
        )

        result = {
            "status": "success",
            "message": f"æ–‡ä»¶å†™å…¥æˆåŠŸ ({mode}æ¨¡å¼)",
            "file_info": {
                "path": file_path,
                "size_mb": file_info["size_mb"],
                "encoding": encoding,
            },
            "operation": {
                "mode": mode,
                "content_length": len(content),
                "final_content_length": len(final_content),
                "write_time": write_time,
                "timestamp": datetime.now().isoformat(),
            },
            "operation_id": operation_id,
        }

        # è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘ç«¯
        if auto_upload:
            logger.info(f"â˜ï¸ å¼€å§‹è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘ç«¯ [{operation_id}]")
            try:
                upload_result = await update_file_to_cloud_drive(file_path)
                if upload_result["status"] == "success":
                    result["cloud_url"] = upload_result["url"]
                    result["file_id"] = upload_result.get("file_id")
                    result["message"] += "ï¼Œå·²ä¸Šä¼ åˆ°äº‘ç«¯"
                    logger.info(
                        f"âœ… äº‘ç«¯ä¸Šä¼ æˆåŠŸ [{operation_id}] - URL: {upload_result['url']}"
                    )
                else:
                    result["upload_error"] = upload_result["message"]
                    logger.warning(
                        f"âš ï¸ äº‘ç«¯ä¸Šä¼ å¤±è´¥ [{operation_id}] - é”™è¯¯: {upload_result['message']}"
                    )
            except Exception as e:
                result["upload_error"] = f"äº‘ç«¯ä¸Šä¼ å¤±è´¥: {str(e)}"
                logger.error(f"ğŸ’¥ äº‘ç«¯ä¸Šä¼ å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}")
                logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

        total_time = time.time() - start_time
        result["execution_time"] = total_time

        logger.info(
            f"âœ… file_writeæ‰§è¡Œå®Œæˆ [{operation_id}] - æ€»è€—æ—¶: {total_time:.2f}ç§’"
        )

        return result

    except Exception as e:
        error_time = time.time() - start_time
        logger.error(
            f"ğŸ’¥ æ–‡ä»¶å†™å…¥å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        return {
            "status": "error",
            "message": f"æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}",
            "operation_id": operation_id,
        }


@mcp.tool()
async def update_file_to_cloud_drive(file_path: str) -> dict:
    """Upload a file to a cloud drive. It is useful for uploading files to cloud drive for later use. For example, show files in markdown format.

    Args:
        file_path (str): The path of the file to upload.

    Returns:
        dict: Status message and file url
    """
    start_time = time.time()
    operation_id = hashlib.md5(
        f"upload_cloud_drive_{file_path}_{time.time()}".encode()
    ).hexdigest()[:8]
    logger.info(
        f"â˜ï¸ update_file_to_cloud_driveå¼€å§‹æ‰§è¡Œ [{operation_id}] - æ–‡ä»¶: {file_path}"
    )

    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        logger.debug(f"ğŸ“‚ æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§ [{operation_id}] - è·¯å¾„: {file_path}")
        if not os.path.exists(file_path):
            error_time = time.time() - start_time
            logger.error(
                f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ [{operation_id}] - è·¯å¾„: {file_path}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return {"status": "error", "message": "File not found."}

        # è·å–æ–‡ä»¶å
        file_name = os.path.basename(file_path)
        logger.info(f"ğŸ“ æ–‡ä»¶å: {file_name} [{operation_id}]")
        print(f"[DEBUG] File name: {file_name}")

        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        file_size_mb = file_size / (1024 * 1024)
        logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb:.2f}MB [{operation_id}]")

        # è®¾ç½®ä¸Šä¼ URLå’Œheaders
        url = "http://36.133.44.114:20034/askonce/api/v1/doc/upload"
        headers = {"User-Source": "AskOnce_bakend"}
        logger.debug(f"ğŸ”— ä¸Šä¼ URL: {url} [{operation_id}]")
        logger.debug(f"ğŸ“‹ è¯·æ±‚å¤´: {headers} [{operation_id}]")

        payload = {}

        # å‡†å¤‡æ–‡ä»¶ä¸Šä¼ 
        logger.info(f"ğŸ“¤ å‡†å¤‡ä¸Šä¼ æ–‡ä»¶ [{operation_id}] - å¼€å§‹æ‰“å¼€æ–‡ä»¶")
        with open(file_path, "rb") as file_obj:
            files = {"file": (file_name, file_obj, "application/octet-stream")}
            logger.debug(f"ğŸ“‹ æ–‡ä»¶å‚æ•°å‡†å¤‡å®Œæˆ [{operation_id}]")

            # å‘èµ·HTTPè¯·æ±‚
            upload_start_time = time.time()
            logger.info(f"ğŸŒ å‘èµ·POSTè¯·æ±‚ [{operation_id}] - å¼€å§‹ä¸Šä¼ ")
            response = requests.request(
                "POST", url, headers=headers, data=payload, files=files, timeout=60
            )
            upload_time = time.time() - upload_start_time

            logger.info(
                f"ğŸ“¡ è¯·æ±‚å®Œæˆ [{operation_id}] - çŠ¶æ€ç : {response.status_code}, ä¸Šä¼ è€—æ—¶: {upload_time:.2f}ç§’"
            )
            print(f"[DEBUG] Response status: {response.status_code}")
            print(f"[DEBUG] Response headers: {dict(response.headers)}")

        # å¤„ç†å“åº”
        logger.debug(
            f"ğŸ“„ å¤„ç†å“åº”å†…å®¹ [{operation_id}] - å“åº”é•¿åº¦: {len(response.text)}"
        )
        try:
            json_data = json.loads(response.text)
            logger.info(f"âœ… JSONè§£ææˆåŠŸ [{operation_id}]")
            logger.debug(f"ğŸ“„ å“åº”å†…å®¹: {json_data} [{operation_id}]")
            print(f"[DEBUG] JSON response: {json_data}")

            # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦æœ‰URL
            if "data" in json_data and "url" in json_data["data"]:
                file_url = json_data["data"]["url"]
                total_time = time.time() - start_time
                logger.info(
                    f"ğŸ‰ æ–‡ä»¶ä¸Šä¼ æˆåŠŸ [{operation_id}] - URL: {file_url}, æ€»è€—æ—¶: {total_time:.2f}ç§’"
                )
                return {
                    "status": "success",
                    "message": "Successfully uploaded to cloud drive",
                    "url": file_url,
                    "file_name": file_name,
                    "file_size_mb": file_size_mb,
                    "upload_time": upload_time,
                    "total_time": total_time,
                    "operation_id": operation_id,
                }
            else:
                error_time = time.time() - start_time
                logger.error(
                    f"âŒ å“åº”ä¸­ç¼ºå°‘URL [{operation_id}] - å“åº”: {json_data}, è€—æ—¶: {error_time:.2f}ç§’"
                )
                return {
                    "status": "error",
                    "message": "Response missing file URL",
                    "response": json_data,
                }

        except json.JSONDecodeError as e:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ JSONè§£æå¤±è´¥ [{operation_id}] - é”™è¯¯: {str(e)}, å“åº”: {response.text[:500]}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            print(f"[DEBUG] JSON decode error: {str(e)}")
            print(f"[DEBUG] Raw response: {response.text}")
            return {
                "status": "error",
                "message": "Failed to decode JSON response.",
                "response": response.text,
            }

    except requests.exceptions.Timeout:
        error_time = time.time() - start_time
        logger.error(f"â° è¯·æ±‚è¶…æ—¶ [{operation_id}] - è€—æ—¶: {error_time:.2f}ç§’")
        return {"status": "error", "message": "Upload request timed out"}
    except requests.exceptions.RequestException as e:
        error_time = time.time() - start_time
        logger.error(
            f"ğŸ’¥ ç½‘ç»œè¯·æ±‚å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        print(f"[DEBUG] Request exception: {str(e)}")
        return {"status": "error", "message": f"Network request failed: {str(e)}"}
    except Exception as e:
        error_time = time.time() - start_time
        logger.error(
            f"ğŸ’¥ ä¸Šä¼ å¼‚å¸¸ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        print(f"[DEBUG] Unexpected error: {str(e)}")
        return {
            "status": "error",
            "message": f"An error occurred: {str(e)}",
            "operation_id": operation_id,
        }


@mcp.tool()
async def save_file_from_url(url: str, working_dir: str) -> str:
    """Download a file from a given URL and save it to the specified working directory.

    Args:
        url (str): The URL of the file to download.
        working_dir (str): The current working directory to save the file.

    Returns:
        str: The path of the downloaded file.
    """
    start_time = time.time()
    operation_id = hashlib.md5(f"download_{url}_{time.time()}".encode()).hexdigest()[:8]
    logger.info(f"ğŸ“¥ save_file_from_urlå¼€å§‹æ‰§è¡Œ [{operation_id}] - URL: {url}")
    logger.info(f"ğŸ”§ å‚æ•°: working_dir={working_dir}")

    try:
        # æ£€æŸ¥å·¥ä½œç›®å½•æ˜¯å¦å­˜åœ¨
        logger.debug(f"ğŸ“‚ æ£€æŸ¥å·¥ä½œç›®å½• [{operation_id}] - ç›®å½•: {working_dir}")
        if not os.path.exists(working_dir):
            error_time = time.time() - start_time
            logger.error(
                f"âŒ å·¥ä½œç›®å½•ä¸å­˜åœ¨ [{operation_id}] - ç›®å½•: {working_dir}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            print(f"[DEBUG] Working directory {working_dir} does not exist.")
            return f"Working directory {working_dir} does not exist."

        logger.info(f"âœ… å·¥ä½œç›®å½•å­˜åœ¨ [{operation_id}] - ç›®å½•: {working_dir}")

        # å‘èµ·HTTPè¯·æ±‚ä¸‹è½½æ–‡ä»¶
        download_start_time = time.time()
        logger.info(f"ğŸŒ å¼€å§‹ä¸‹è½½æ–‡ä»¶ [{operation_id}] - URL: {url}")
        print(f"[DEBUG] Starting download from: {url}")

        response = httpx.get(url, timeout=30)
        response.raise_for_status()
        download_time = time.time() - download_start_time

        # è·å–æ–‡ä»¶å¤§å°
        content_length = len(response.content)
        content_size_mb = content_length / (1024 * 1024)
        logger.info(
            f"ğŸ“Š ä¸‹è½½å®Œæˆ [{operation_id}] - å¤§å°: {content_size_mb:.2f}MB, ä¸‹è½½è€—æ—¶: {download_time:.2f}ç§’"
        )
        print(f"[DEBUG] Downloaded {content_size_mb:.2f}MB in {download_time:.2f}s")

        # è§£ç URLå¹¶è·å–æ–‡ä»¶å
        decoded_url = urllib.parse.unquote(url)
        logger.debug(f"ğŸ”¤ URLè§£ç  [{operation_id}] - åŸå§‹: {url}")
        logger.debug(f"ğŸ”¤ URLè§£ç  [{operation_id}] - è§£ç å: {decoded_url}")
        print(f"[DEBUG] Decoded URL: {decoded_url}")

        file_name = os.path.basename(decoded_url)
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–è€…åªæ˜¯è·¯å¾„åˆ†éš”ç¬¦ï¼Œä½¿ç”¨é»˜è®¤åç§°
        if not file_name or file_name in ["/", "\\"]:
            file_name = f"downloaded_file_{operation_id}"
            logger.warning(
                f"âš ï¸ æ— æ³•ä»URLè·å–æ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤åç§° [{operation_id}] - æ–‡ä»¶å: {file_name}"
            )

        logger.info(f"ğŸ“ æ–‡ä»¶å: {file_name} [{operation_id}]")
        print(f"[DEBUG] File name: {file_name}")

        # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
        file_path = os.path.join(working_dir, file_name)
        logger.debug(f"ğŸ“ æ–‡ä»¶ä¿å­˜è·¯å¾„ [{operation_id}] - è·¯å¾„: {file_path}")
        print(f"[DEBUG] File path: {file_path}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ åç¼€
        original_file_path = file_path
        counter = 1
        while os.path.exists(file_path):
            name, ext = os.path.splitext(original_file_path)
            file_path = f"{name}_{counter}{ext}"
            counter += 1
            logger.debug(
                f"ğŸ”„ æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°è¯•æ–°è·¯å¾„ [{operation_id}] - è·¯å¾„: {file_path}"
            )

        # ä¿å­˜æ–‡ä»¶
        save_start_time = time.time()
        logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ–‡ä»¶ [{operation_id}] - è·¯å¾„: {file_path}")
        with open(file_path, "wb") as f:
            f.write(response.content)
        save_time = time.time() - save_start_time

        # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
        if os.path.exists(file_path):
            saved_size = os.path.getsize(file_path)
            logger.info(
                f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ [{operation_id}] - è·¯å¾„: {file_path}, ä¿å­˜å¤§å°: {saved_size}, ä¿å­˜è€—æ—¶: {save_time:.2f}ç§’"
            )
            if saved_size != content_length:
                logger.warning(
                    f"âš ï¸ æ–‡ä»¶å¤§å°ä¸åŒ¹é… [{operation_id}] - ä¸‹è½½: {content_length}, ä¿å­˜: {saved_size}"
                )
        else:
            error_time = time.time() - start_time
            logger.error(
                f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ [{operation_id}] - è·¯å¾„: {file_path}, è€—æ—¶: {error_time:.2f}ç§’"
            )
            return f"Failed to save file to {file_path}"

        total_time = time.time() - start_time
        success_message = f"æ–‡ä»¶ä¿å­˜æˆåŠŸï¼Œåœ°å€ï¼š{file_path}"
        logger.info(
            f"ğŸ‰ save_file_from_urlæ‰§è¡Œå®Œæˆ [{operation_id}] - æ€»è€—æ—¶: {total_time:.2f}ç§’"
        )
        print(f"[DEBUG] {success_message}")
        return success_message

    except httpx.HTTPStatusError as e:
        error_time = time.time() - start_time
        error_msg = f"HTTP error occurred: {e.response.status_code} - {e.response.text}"
        logger.error(
            f"ğŸŒ HTTPé”™è¯¯ [{operation_id}] - çŠ¶æ€ç : {e.response.status_code}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        logger.error(
            f"ğŸŒ HTTPé”™è¯¯è¯¦æƒ… [{operation_id}] - å“åº”: {e.response.text[:500]}"
        )
        print(f"[DEBUG] {error_msg}")
        return error_msg
    except httpx.RequestError as e:
        error_time = time.time() - start_time
        error_msg = f"Error communicating with the server: {str(e)}"
        logger.error(
            f"ğŸ’¥ ç½‘ç»œé€šä¿¡é”™è¯¯ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        print(f"[DEBUG] Request error: {str(e)}")
        return error_msg
    except Exception as e:
        error_time = time.time() - start_time
        error_msg = f"An unexpected error occurred: {str(e)}"
        logger.error(
            f"ğŸ’¥ æœªé¢„æœŸé”™è¯¯ [{operation_id}] - é”™è¯¯: {str(e)}, è€—æ—¶: {error_time:.2f}ç§’"
        )
        logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        print(f"[DEBUG] Unexpected error: {str(e)}")
        return error_msg


@mcp.tool()
async def file_operations(operation: str, **kwargs) -> Dict[str, Any]:
    """å¤æ‚æ–‡ä»¶æ“ä½œå·¥å…·ï¼ˆä»…ç”¨äºå‘½ä»¤è¡Œæ— æ³•å¤„ç†çš„å¤æ‚åœºæ™¯ï¼‰

    Args:
        operation (str): æ“ä½œç±»å‹ - 'search_replace', 'get_info', 'batch_process'
        **kwargs: æ“ä½œç›¸å…³å‚æ•°

    Returns:
        Dict: æ“ä½œç»“æœ
    """
    try:
        if operation == "search_replace":
            return await _search_replace(**kwargs)
        elif operation == "get_info":
            return await _get_file_info(**kwargs)
        elif operation == "batch_process":
            return await _batch_process(**kwargs)
        else:
            return {"status": "error", "message": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}"}

    except Exception as e:
        return {"status": "error", "message": f"æ“ä½œå¤±è´¥: {str(e)}"}


async def _search_replace(
    file_path: str,
    search_pattern: str,
    replacement: str,
    use_regex: bool = False,
    case_sensitive: bool = True,
) -> Dict[str, Any]:
    """æœç´¢æ›¿æ¢ï¼ˆä»…ç”¨äºå¤æ‚æ­£åˆ™è¡¨è¾¾å¼åœºæ™¯ï¼‰"""
    try:
        # å®‰å…¨éªŒè¯
        validation = SecurityValidator.validate_path(file_path)
        if not validation["valid"]:
            return {"status": "error", "message": validation["error"]}

        file_path = validation["resolved_path"]

        # æ£€æŸ¥æ–‡ä»¶
        file_info = FileMetadata.get_file_info(file_path)
        if not file_info["exists"] or not file_info["is_file"]:
            return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶"}

        # è¯»å–æ–‡ä»¶
        encoding = file_info.get("encoding", "utf-8")
        with open(file_path, "r", encoding=encoding) as f:
            original_content = f.read()

        # æ‰§è¡Œæœç´¢æ›¿æ¢
        if use_regex:
            flags = 0 if case_sensitive else re.IGNORECASE
            pattern = re.compile(search_pattern, flags)
            new_content, replace_count = pattern.subn(replacement, original_content)
        else:
            if case_sensitive:
                new_content = original_content.replace(search_pattern, replacement)
                replace_count = original_content.count(search_pattern)
            else:
                pattern = re.compile(re.escape(search_pattern), re.IGNORECASE)
                new_content, replace_count = pattern.subn(replacement, original_content)

        # å†™å…¥ä¿®æ”¹åçš„å†…å®¹
        with open(file_path, "w", encoding=encoding) as f:
            f.write(new_content)

        return {
            "status": "success",
            "message": f"æˆåŠŸæ›¿æ¢ {replace_count} å¤„åŒ¹é…é¡¹",
            "statistics": {
                "replacements": replace_count,
                "original_length": len(original_content),
                "new_length": len(new_content),
                "length_change": len(new_content) - len(original_content),
            },
        }

    except re.error as e:
        return {"status": "error", "message": f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {str(e)}"}
    except Exception as e:
        return {"status": "error", "message": f"æœç´¢æ›¿æ¢å¤±è´¥: {str(e)}"}


async def _get_file_info(file_path: str) -> Dict[str, Any]:
    """è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯ï¼ˆä»…ç”¨äºéœ€è¦å®Œæ•´å…ƒæ•°æ®çš„åœºæ™¯ï¼‰"""
    try:
        # å®‰å…¨éªŒè¯
        validation = SecurityValidator.validate_path(file_path)
        if not validation["valid"]:
            return {"status": "error", "message": validation["error"]}

        file_path = validation["resolved_path"]

        # è·å–åŸºç¡€ä¿¡æ¯
        info = FileMetadata.get_file_info(file_path)
        if not info["exists"]:
            return {"status": "error", "message": "æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨"}

        # ä¸ºå°æ–‡ä»¶æ·»åŠ æ ¡éªŒå’Œ
        if info["is_file"] and info["size_mb"] < 10:
            try:
                info["checksums"] = {
                    "md5": hashlib.md5(open(file_path, "rb").read()).hexdigest(),
                    "sha256": hashlib.sha256(open(file_path, "rb").read()).hexdigest(),
                }
            except Exception as e:
                info["checksum_error"] = str(e)

        return {"status": "success", "message": "æ–‡ä»¶ä¿¡æ¯è·å–æˆåŠŸ", "file_info": info}

    except Exception as e:
        return {"status": "error", "message": f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}"}


async def _batch_process(
    operation: str, file_paths: List[str], **params
) -> Dict[str, Any]:
    """æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆä»…ç”¨äºéœ€è¦åŸå­æ€§çš„æ‰¹é‡æ“ä½œï¼‰"""
    try:
        results = []
        errors = []

        for file_path in file_paths:
            try:
                if operation == "compress":
                    # åˆ›å»ºå‹ç¼©åŒ…
                    archive_path = params.get("archive_path")
                    archive_type = params.get("archive_type", "zip")

                    if archive_type == "zip":
                        with zipfile.ZipFile(
                            archive_path, "w", zipfile.ZIP_DEFLATED
                        ) as zipf:
                            for fp in file_paths:
                                if os.path.exists(fp):
                                    zipf.write(fp, os.path.basename(fp))

                    results.append({"file": file_path, "status": "success"})

            except Exception as e:
                errors.append({"file": file_path, "error": str(e)})

        return {
            "status": "success",
            "message": f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {len(results)}ï¼Œå¤±è´¥: {len(errors)}",
            "results": results,
            "errors": errors,
        }

    except Exception as e:
        return {"status": "error", "message": f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"}


if __name__ == "__main__":
    app = Starlette(
        routes=[
            Mount("/", app=mcp.sse_app()),
        ]
    )

    uvicorn.run(app, host="0.0.0.0", port=34100)

"""
Sage Stream Service

åŸºäº Sage æ¡†æ¶çš„æ™ºèƒ½ä½“æµå¼æœåŠ¡
æä¾›ç®€æ´çš„ HTTP API å’Œ Server-Sent Events (SSE) å®æ—¶é€šä¿¡
ä¸åšä»»ä½•çš„é…ç½®ä»¥åŠè®¾ç½®çš„ç¼“å­˜ï¼Œæ‰€æœ‰çš„é…ç½®éƒ½é€šè¿‡æ¥å£ä¼ å…¥
"""

import argparse
import asyncio
import json
import os
import sys
import time
import traceback
import uuid
import warnings
from contextlib import asynccontextmanager
from typing import Any, Dict, List, Optional, Union

import uvicorn
from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from openai import AsyncOpenAI
from pydantic import BaseModel
# æ·»åŠ  Sage é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
print(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sagents.context.session_context import get_session_context
from sagents.sagents import SAgent
from sagents.tool import ToolManager, ToolProxy
from sagents.skill import SkillManager, SkillProxy
from sagents.utils.auto_gen_agent import AutoGenAgentFunc
from sagents.utils.logger import logger
from sagents.utils.system_prompt_optimizer import SystemPromptOptimizer
from sagents.utils.evaluations.checkpoint_generation import CheckpointGenerationAgent
from sagents.utils.evaluations.score_evaluation import AgentScoreEvaluator


parser = argparse.ArgumentParser(description="Sage Stream Service")
# æ–°æ ¼å¼å‚æ•°ï¼ˆæ¨èä½¿ç”¨ï¼‰
parser.add_argument("--default_llm_api_key", help="é»˜è®¤LLM API Key")
parser.add_argument("--default_llm_api_base_url", help="é»˜è®¤LLM API Base")
parser.add_argument("--default_llm_model_name", help="é»˜è®¤LLM API Model")
parser.add_argument("--default_llm_max_tokens", default=4096, type=int, help="é»˜è®¤LLM API Max Tokens")
parser.add_argument("--default_llm_temperature", default=0.2, type=float, help="é»˜è®¤LLM API Temperature")
parser.add_argument("--default_llm_max_model_len", default=54000, type=int, help="é»˜è®¤LLM æœ€å¤§ä¸Šä¸‹æ–‡")
parser.add_argument("--default_llm_top_p", default=0.9, type=float, help="é»˜è®¤LLM Top P")
parser.add_argument("--default_llm_presence_penalty", default=0.0, type=float, help="é»˜è®¤LLM Presence Penalty")

parser.add_argument("--host", default="0.0.0.0", help="Server Host")
parser.add_argument("--port", default=8001, type=int, help="Server Port")

parser.add_argument("--mcp-config", default="mcp_setting.json", help="MCPé…ç½®æ–‡ä»¶è·¯å¾„")
parser.add_argument("--workspace", default="agent_workspace", help="å·¥ä½œç©ºé—´ç›®å½•")
parser.add_argument("--skills-path", default=None, help="æŠ€èƒ½ç›®å½•è·¯å¾„")
parser.add_argument("--logs-dir", default="logs", help="æ—¥å¿—ç›®å½•")
parser.add_argument("--preset_running_config", default="", help="é¢„è®¾é…ç½®ï¼Œsystem_contextï¼Œä»¥åŠworkflowï¼Œä¸æ¥å£ä¸­ä¼ è¿‡æ¥çš„åˆå¹¶ä½¿ç”¨")
parser.add_argument("--memory_root", default=None, help="è®°å¿†å­˜å‚¨æ ¹ç›®å½•ï¼ˆå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ --memory_typeï¼‰")
parser.add_argument("--memory_type", default="session", help="è®°å¿†ç±»å‹: session | user")
parser.add_argument("--daemon", action="store_true", help="ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œ")
parser.add_argument("--pid-file", default="sage_stream.pid", help="PIDæ–‡ä»¶è·¯å¾„")
parser.add_argument("--context_history_ratio", type=float, default=0.2,
                    help='ä¸Šä¸‹æ–‡é¢„ç®—ç®¡ç†å™¨ï¼šå†å²æ¶ˆæ¯çš„æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼‰')
parser.add_argument("--context_active_ratio", type=float, default=0.3,
                    help='ä¸Šä¸‹æ–‡é¢„ç®—ç®¡ç†å™¨ï¼šæ´»è·ƒæ¶ˆæ¯çš„æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼‰')
parser.add_argument("--context_max_new_message_ratio", type=float, default=0.5,
                    help='ä¸Šä¸‹æ–‡é¢„ç®—ç®¡ç†å™¨ï¼šæ–°æ¶ˆæ¯çš„æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼‰')
parser.add_argument("--context_recent_turns", type=int, default=0,
                    help='ä¸Šä¸‹æ–‡é¢„ç®—ç®¡ç†å™¨ï¼šé™åˆ¶æœ€è¿‘çš„å¯¹è¯è½®æ•°ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶')


server_args = parser.parse_args()

# éªŒè¯å¿…éœ€å‚æ•°
required_args = ['default_llm_api_key', 'default_llm_api_base_url', 'default_llm_model_name']
missing_args = [arg for arg in required_args if getattr(server_args, arg) is None]
if missing_args:
    raise ValueError(f"å¿…éœ€å‚æ•°ç¼ºå¤±: è¯·æä¾› {', '.join(['--' + arg for arg in missing_args])}")

# å¤„ç† default_llm_max_model_len é€»è¾‘
if server_args.default_llm_max_model_len is None:
    server_args.default_llm_max_model_len = 54000
elif server_args.default_llm_max_model_len < 8000:
    server_args.default_llm_max_model_len = 54000

if server_args.workspace:
    server_args.workspace = os.path.abspath(server_args.workspace)
os.environ['PREFIX_FILE_WORKSPACE'] = server_args.workspace if server_args.workspace.endswith('/') else server_args.workspace+'/'

# å¤„ç† memory_root å…¼å®¹æ€§
if server_args.memory_root:
    os.environ["MEMORY_ROOT_PATH"] = server_args.memory_root
    logger.warning("memory_root å‚æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ memory_type å‚æ•°ã€‚å·²è‡ªåŠ¨è®¾ç½® MEMORY_ROOT_PATH ç¯å¢ƒå˜é‡ã€‚")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    await initialize_system(server_args)
    yield
    # å…³é—­æ—¶æ¸…ç†
    await cleanup_system()

# è®¾ç½®é…ç½®æ–‡ä»¶è·¯å¾„ç¯å¢ƒå˜é‡
os.environ['SAGE_MCP_CONFIG_PATH'] = server_args.mcp_config
# FastAPI åº”ç”¨
app = FastAPI(
    title="Sage Stream Service",
    description="åŸºäº Sage æ¡†æ¶çš„æ™ºèƒ½ä½“æµå¼æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# æ ¸å¿ƒæœåŠ¡ç±»
class SageStreamService:
    """
    åŸºäº Sage æ¡†æ¶çš„æµå¼æœåŠ¡
    æä¾›æ™ºèƒ½ä½“å¯¹è¯çš„æµå¼å¤„ç†èƒ½åŠ›
    """
    
    def __init__(self, model: Optional[AsyncOpenAI] = None, 
                        model_config: Optional[Dict[str, Any]] = None, 
                        tool_manager: Optional[Union[ToolManager, ToolProxy]] = None, 
                        skill_manager: Optional[Union[SkillManager, SkillProxy]] = None,
                        preset_running_config: Optional[Dict[str, Any]] = None,
                        workspace: Optional[str] = None,
                        memory_type: Optional[str] = "session",
                        context_budget_config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æœåŠ¡
        
        Args:
            model: OpenAI å®¢æˆ·ç«¯å®ä¾‹
            model_config: æ¨¡å‹é…ç½®å­—å…¸
            tool_manager: å·¥å…·ç®¡ç†å™¨å®ä¾‹
        """
        self.preset_running_config = preset_running_config or {}
        # è®¾ç½®system_prefix
        if "system_prefix" in self.preset_running_config:
            self.preset_system_prefix = self.preset_running_config['system_prefix']
            logger.debug(f"ä½¿ç”¨é¢„è®¾system_prefix: {self.preset_system_prefix}")
        elif "systemPrefix" in self.preset_running_config:
            self.preset_system_prefix = self.preset_running_config['systemPrefix']
            logger.debug(f"ä½¿ç”¨é¢„è®¾systemPrefix: {self.preset_system_prefix}")
        else:
            self.preset_system_prefix = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾system_prefix")
        
        # è®¾ç½®system_context
        if "system_context" in self.preset_running_config:
            self.preset_system_context = self.preset_running_config['system_context']
            logger.debug("ä½¿ç”¨é¢„è®¾system_context")
        elif "systemContext" in self.preset_running_config:
            self.preset_system_context = self.preset_running_config['systemContext']
            logger.debug("ä½¿ç”¨é¢„è®¾systemContext")
        else:
            self.preset_system_context = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾system_context")
        
        # è®¾ç½®available_workflows
        if "available_workflows" in self.preset_running_config:
            self.preset_available_workflows = self.preset_running_config['available_workflows']
            logger.debug("ä½¿ç”¨é¢„è®¾available_workflows")
        elif "availableWorkflows" in self.preset_running_config:
            self.preset_available_workflows = self.preset_running_config['availableWorkflows']
            logger.debug("ä½¿ç”¨é¢„è®¾availableWorkflows")
        else:
            self.preset_available_workflows = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾available_workflows")
        
        # è®¾ç½®available_tools
        if "available_tools" in self.preset_running_config:
            self.preset_available_tools = self.preset_running_config['available_tools']
            logger.debug("ä½¿ç”¨é¢„è®¾available_tools")
        elif "availableTools" in self.preset_running_config:
            self.preset_available_tools = self.preset_running_config['availableTools']
            logger.debug("ä½¿ç”¨é¢„è®¾availableTools")
        else:
            self.preset_available_tools = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾available_tools")
        
        # è®¾ç½®max_loop_count
        if "max_loop_count" in self.preset_running_config:
            self.preset_max_loop_count = self.preset_running_config['max_loop_count']
            logger.debug(f"ä½¿ç”¨é¢„è®¾max_loop_count: {self.preset_max_loop_count}")
        elif "maxLoopCount" in self.preset_running_config:
            self.preset_max_loop_count = self.preset_running_config['maxLoopCount']
            logger.debug(f"ä½¿ç”¨é¢„è®¾maxLoopCount: {self.preset_max_loop_count}")
        else:
            self.preset_max_loop_count = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾max_loop_count")

#         "deepThinking": false,
#   "multiAgent": false,
        # è®¾ç½®deepThinking
        if "deepThinking" in self.preset_running_config:
            self.preset_deep_thinking = self.preset_running_config['deepThinking']
            logger.debug(f"ä½¿ç”¨é¢„è®¾deepThinking: {self.preset_deep_thinking}")
        elif "deepThinking" in self.preset_running_config:
            self.preset_deep_thinking = self.preset_running_config['deepThinking']
            logger.debug(f"ä½¿ç”¨é¢„è®¾deepThinking: {self.preset_deep_thinking}")
        else:
            self.preset_deep_thinking = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾deepThinking")
        # è®¾ç½®multiAgent
        if "multiAgent" in self.preset_running_config:
            self.preset_multi_agent = self.preset_running_config['multiAgent']
            logger.debug(f"ä½¿ç”¨é¢„è®¾multiAgent: {self.preset_multi_agent}")
        elif "multiAgent" in self.preset_running_config:
            self.preset_multi_agent = self.preset_running_config['multiAgent']
            logger.debug(f"ä½¿ç”¨é¢„è®¾multiAgent: {self.preset_multi_agent}")
        else:
            self.preset_multi_agent = None
            logger.debug("æœªä½¿ç”¨é¢„è®¾multiAgent")

        # è®¾ç½®context_budget_config
        self.context_budget_config = context_budget_config

        # workspace æœ‰å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„
        if workspace:
            workspace = os.path.abspath(workspace)
        else:
            workspace = os.path.abspath("agent_workspace")

        # åˆ›å»º Sage AgentController å®ä¾‹
        self.sage_controller = SAgent(
            model=model,
            model_config=model_config,
            system_prefix=self.preset_system_prefix,
            workspace=workspace if workspace.endswith('/') else workspace+'/',
            memory_type=memory_type
        )
        self.tool_manager = tool_manager
        if self.preset_available_tools:
            if isinstance(self.tool_manager, ToolManager):
                self.tool_manager = ToolProxy(self.tool_manager, self.preset_available_tools)    
        
        self.skill_manager = skill_manager
        
        logger.info("SageStreamService åˆå§‹åŒ–å®Œæˆ")
    
    async def process_stream(self, messages, session_id=None, user_id=None, deep_thinking=None, 
                           max_loop_count=None, multi_agent=None,more_suggest=False,
                            system_context: Optional[Dict] = None, 
                           available_workflows: Optional[Dict] = None,
                           force_summary: bool=False):
        """å¤„ç†æµå¼èŠå¤©è¯·æ±‚"""
        logger.info(f"ğŸš€ SageStreamService.process_stream å¼€å§‹ï¼Œä¼šè¯ID: {session_id}")
        logger.info(f"ğŸ“ å‚æ•°: deep_thinking={deep_thinking}, multi_agent={multi_agent}, messages_count={len(messages)}")
        if isinstance(deep_thinking, str):
            if deep_thinking == 'auto':
                deep_thinking = None
            if deep_thinking == 'off':
                deep_thinking = False
            if deep_thinking == 'on':
                deep_thinking = True
        if isinstance(multi_agent, str):
            if multi_agent == 'auto':
                multi_agent = None
            if multi_agent == 'off':
                multi_agent = False
            if multi_agent == 'on':
                multi_agent = True
        
        
        # å¦‚æœ self.preset_system_context ä¸æ˜¯ç©ºï¼Œå°†self.preset_system_context çš„å†…å®¹ï¼Œæ›´æ–°åˆ° system_contextï¼Œä¸æ˜¯èµ‹å€¼ï¼Œè¦æ£€æŸ¥ä¸€ä¸‹system_context æ˜¯ä¸æ˜¯ç©º
        if self.preset_system_context:
            if system_context:
                system_context.update(self.preset_system_context)
            else:
                system_context = self.preset_system_context
        # å¦‚æœ self.preset_available_workflows ä¸æ˜¯ç©ºï¼Œå°†self.preset_available_workflows çš„å†…å®¹ï¼Œæ›´æ–°åˆ° available_workflowsï¼Œä¸æ˜¯èµ‹å€¼
        if self.preset_available_workflows:
            if available_workflows:
                available_workflows.update(self.preset_available_workflows)
            else:
                available_workflows = self.preset_available_workflows

        try:
            logger.info("ğŸ”„ å‡†å¤‡è°ƒç”¨ sage_controller.run_stream...")
            
            # ç›´æ¥è°ƒç”¨åŒæ­¥çš„ run_stream æ–¹æ³•
            stream_result = self.sage_controller.run_stream(
                input_messages=messages,
                tool_manager=self.tool_manager,
                skill_manager=self.skill_manager,
                session_id=session_id,
                user_id=user_id,
                deep_thinking=deep_thinking if deep_thinking is not None else self.preset_deep_thinking,
                max_loop_count = max_loop_count if max_loop_count is not None else self.preset_max_loop_count ,
                multi_agent=multi_agent if multi_agent is not None else self.preset_multi_agent,
                more_suggest = more_suggest,
                system_context=system_context,
                available_workflows=available_workflows,
                force_summary=force_summary,
                context_budget_config=self.context_budget_config
            )
            
            logger.info("âœ… run_stream è°ƒç”¨æˆåŠŸï¼Œå¼€å§‹å¤„ç†ç»“æœ...")
            
            # å¤„ç†è¿”å›çš„ç”Ÿæˆå™¨
            chunk_count = 0
            async for chunk in stream_result:
                chunk_count += 1
                # logger.info(f"ğŸ“¦ å¤„ç†ç¬¬ {chunk_count} ä¸ªå—ï¼ŒåŒ…å« {len(chunk)} æ¡æ¶ˆæ¯")
                
                # ç›´æ¥ä½¿ç”¨æ¶ˆæ¯çš„åŸå§‹å†…å®¹ï¼Œä¸é‡æ–°æ•´ç†æ ¼å¼
                for message in chunk:
                    # æ·±æ‹·è´åŸå§‹æ¶ˆæ¯ï¼Œä¿æŒæ‰€æœ‰å­—æ®µ
                    result = message.to_dict()
                    
                    # åªæ·»åŠ å¿…è¦çš„ä¼šè¯ä¿¡æ¯
                    result['session_id'] = session_id
                    result['timestamp'] = time.time()
                    
                    # å¤„ç†å¤§å†…å®¹çš„ç‰¹æ®Šæƒ…å†µ
                    content = result.get('content', '')
                    show_content = result.get('show_content', '')
                    
                    # æ¸…ç†show_contentä¸­çš„base64å›¾ç‰‡æ•°æ®ï¼Œé¿å…JSONè¿‡å¤§ï¼Œä½†ä¿ç•™contentä¸­çš„base64
                    if isinstance(show_content, str) and 'data:image' in show_content:
                        try:
                            # å¦‚æœshow_contentæ˜¯JSONå­—ç¬¦ä¸²ï¼Œè§£æå¹¶æ¸…ç†
                            if show_content.strip().startswith('{'):
                                show_content_data = json.loads(show_content)
                                if isinstance(show_content_data, dict) and 'results' in show_content_data:
                                    if isinstance(show_content_data['results'], list):
                                        for item in show_content_data['results']:
                                            if isinstance(item, dict) and 'image' in item:
                                                if item['image'] and isinstance(item['image'], str) and item['image'].startswith('data:image'):
                                                    item['image'] = '[BASE64_IMAGE_REMOVED_FOR_DISPLAY]'
                                result['show_content'] = json.dumps(show_content_data, ensure_ascii=False)
                            else:
                                # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†
                                import re
                                result['show_content'] = re.sub(r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+', '[BASE64_IMAGE_REMOVED_FOR_DISPLAY]', show_content)
                        except (json.JSONDecodeError, Exception) as e:
                            logger.warning(f"æ¸…ç† show_content å¤±è´¥: {e}")
                            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤base64æ•°æ®
                            import re
                            result['show_content'] = re.sub(r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+', '[BASE64_IMAGE_REMOVED_FOR_DISPLAY]', show_content)
                    
                    # ç‰¹æ®Šå¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼Œé¿å…JSONåµŒå¥—é—®é¢˜
                    if result.get('role') == 'tool' and isinstance(content, str):
                        try:
                            # å°è¯•è§£æcontentä¸­çš„JSONæ•°æ®
                            if content.strip().startswith('{'):
                                parsed_content = json.loads(content)
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—çš„JSONç»“æ„
                                if isinstance(parsed_content, dict) and 'content' in parsed_content:
                                    inner_content = parsed_content['content']
                                    if isinstance(inner_content, str) and inner_content.strip().startswith('{'):
                                        try:
                                            # è§£æå†…å±‚JSONï¼Œè¿™é€šå¸¸æ˜¯å®é™…çš„å·¥å…·ç»“æœ
                                            tool_data = json.loads(inner_content)
                                            
                                            # æ¸…ç†å·¥å…·ç»“æœä¸­çš„å¤§æ•°æ®ï¼Œé¿å…JSONè¿‡å¤§
                                            if isinstance(tool_data, dict) and 'results' in tool_data:
                                                if isinstance(tool_data['results'], list):
                                                    for item in tool_data['results']:
                                                        if isinstance(item, dict):
                                                            # é™åˆ¶æ–‡æœ¬å­—æ®µé•¿åº¦ï¼Œä½†ä¿ç•™æ‰€æœ‰å­—æ®µ
                                                            for field in ['snippet', 'description', 'content']:
                                                                if field in item and isinstance(item[field], str):
                                                                    if len(item[field]) > 1000:
                                                                        item[field] = item[field][:1000] + '...[TRUNCATED]'
                                            
                                            # ç›´æ¥ä½¿ç”¨è§£æåçš„æ•°æ®
                                            result['content'] = tool_data
                                        except json.JSONDecodeError:
                                            # å†…å±‚è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤–å±‚æ•°æ®
                                            result['content'] = parsed_content
                                    else:
                                        # å†…å±‚ä¸æ˜¯JSONå­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                                        result['content'] = parsed_content
                                else:
                                    # ä¸æ˜¯åµŒå¥—ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨
                                    result['content'] = parsed_content
                                    
                        except json.JSONDecodeError as e:
                            logger.warning(f"è§£æå·¥å…·ç»“æœJSONå¤±è´¥: {e}")
                            # ä¿æŒåŸå§‹å­—ç¬¦ä¸²
                            pass
                    
                    # ç›´æ¥yieldåŸå§‹æ¶ˆæ¯ï¼Œä¸è¿›è¡Œå¤æ‚çš„åºåˆ—åŒ–å¤„ç†
                    yield result
                    await asyncio.sleep(0.01)  # é¿å…è¿‡å¿«å‘é€
                
                # åœ¨æ¯ä¸ªå—ä¹‹åè®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                await asyncio.sleep(0)
            
            logger.info(f"ğŸ æµå¼å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç†äº† {chunk_count} ä¸ªå—")
                
        except GeneratorExit:
            logger.warning(f"ğŸ”Œ process_stream: å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œä¼šè¯ID: {session_id}")
            logger.warning("ğŸ” GeneratorExit è¯¦æƒ…: å®¢æˆ·ç«¯åœ¨æµå¼å¤„ç†è¿‡ç¨‹ä¸­æ–­å¼€äº†è¿æ¥")
            logger.warning(f"ğŸ“‹ GeneratorExit å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
            # é‡æ–°æŠ›å‡ºGeneratorExitï¼Œè®©ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            logger.error(f"âŒ æµå¼å¤„ç†å¼‚å¸¸: {e}")
            logger.error(f"ğŸ” å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"ğŸ“‹ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            error_result = {
                'type': 'error',
                'content': f"å¤„ç†å¤±è´¥: {str(e)}",
                'role': 'assistant',
                'message_id': str(uuid.uuid4()),
                'session_id': session_id,
                'show_content': f"å¤„ç†å¤±è´¥: {str(e)}"
            }
            yield error_result
    

    
    # ä¼šè¯ç®¡ç†æ–¹æ³•
    def interrupt_session(self, session_id: str, message: str = "ç”¨æˆ·è¯·æ±‚ä¸­æ–­") -> bool:
        """ä¸­æ–­æŒ‡å®šä¼šè¯"""
        return self.sage_controller.interrupt_session(session_id, message)
    
    def save_session(self, session_id: str) -> bool:
        """ä¿å­˜ä¼šè¯çŠ¶æ€"""
        return self.sage_controller.save_session(session_id)

    def get_session_status(self, session_id: str):
        """è·å–ä¼šè¯çŠ¶æ€"""
        return self.sage_controller.get_session_status(session_id)
    
    def list_active_sessions(self):
        """åˆ—å‡ºæ‰€æœ‰æ´»è·ƒä¼šè¯"""
        return self.sage_controller.list_active_sessions()

# å…¨å±€å˜é‡
default_stream_service: Optional[SageStreamService] = None
all_active_sessions_service_map: Dict[str, Dict[str, Any]] = {}
tool_manager: Optional[ToolManager] = None
default_model_client: Optional[AsyncOpenAI] = None


async def initialize_tool_manager():
    """å¼‚æ­¥åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨"""
    # åˆ›å»ºå·¥å…·ç®¡ç†å™¨å®ä¾‹ï¼Œä½†ä¸è‡ªåŠ¨å‘ç°å·¥å…·
    manager = ToolManager.get_instance(is_auto_discover=False)

    # æ‰‹åŠ¨è¿›è¡ŒåŸºç¡€å·¥å…·å‘ç°
    manager.discover_tools_from_path()

    # è®¾ç½® MCP é…ç½®è·¯å¾„
    manager._mcp_setting_path = os.environ.get('SAGE_MCP_CONFIG_PATH', 'mcp_setting.json')

    # å¼‚æ­¥å‘ç° MCP å·¥å…·
    await manager._discover_mcp_tools(mcp_setting_path=manager._mcp_setting_path)

    return manager

async def initialize_system(server_args):
    """åˆå§‹åŒ–ç³»ç»Ÿ"""
    global default_stream_service, tool_manager, skill_manager, default_model_client
    
    logger.info("æ­£åœ¨åˆå§‹åŒ– Sage Stream Service...")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
        if server_args.default_llm_api_key:
            logger.info(f"é»˜è®¤ API å¯†é’¥: {server_args.default_llm_api_key}...")
            logger.info(f"é»˜è®¤ API åŸºç¡€ URL: {server_args.default_llm_api_base_url}...")
            default_model_client = AsyncOpenAI(
                api_key=server_args.default_llm_api_key,
                base_url=server_args.default_llm_api_base_url
            )
            default_model_client.model = server_args.default_llm_model_name
            logger.info(f"é»˜è®¤æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {server_args.default_llm_model_name}")
        else:
            logger.warning("æœªé…ç½®é»˜è®¤ API å¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        
        # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
        try:
            tool_manager = await initialize_tool_manager()
            logger.info("å·¥å…·ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"å·¥å…·ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            tool_manager = None
        
        # åˆå§‹åŒ–æŠ€èƒ½ç®¡ç†å™¨
        try:
            skill_dirs = [server_args.skills_path] if server_args.skills_path else None
            skill_manager = SkillManager(skill_dirs=skill_dirs)
            logger.info("æŠ€èƒ½ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æŠ€èƒ½ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            skill_manager = None

        # åˆå§‹åŒ–æµå¼æœåŠ¡
        if default_model_client:
            # ä»é…ç½®ä¸­æ„å»ºæ¨¡å‹é…ç½®å­—å…¸
            model_config_dict = {
                'model': server_args.default_llm_model_name,
                'max_tokens': server_args.default_llm_max_tokens,
                'temperature': server_args.default_llm_temperature,
                'top_p': server_args.default_llm_top_p,
                'presence_penalty': server_args.default_llm_presence_penalty
            }

            if server_args.preset_running_config:
                if os.path.exists(server_args.preset_running_config):
                    with open(server_args.preset_running_config, 'r') as f:
                        preset_running_config = json.load(f)
                else:
                    preset_running_config = {}
            else:
                preset_running_config = {}

            # æ„å»ºcontext_budget_configå­—å…¸
            # max_model_lenç»Ÿä¸€ä½¿ç”¨default_llm_max_model_len
            context_budget_config = {
                'max_model_len': server_args.default_llm_max_model_len
            }
            if server_args.context_history_ratio is not None:
                context_budget_config['history_ratio'] = server_args.context_history_ratio
            if server_args.context_active_ratio is not None:
                context_budget_config['active_ratio'] = server_args.context_active_ratio
            if server_args.context_max_new_message_ratio is not None:
                context_budget_config['max_new_message_ratio'] = server_args.context_max_new_message_ratio
            if server_args.context_recent_turns is not None:
                context_budget_config['recent_turns'] = server_args.context_recent_turns
            
            logger.info(f"ä½¿ç”¨context_budget_config: {context_budget_config}")

            default_stream_service = SageStreamService(
                model=default_model_client,
                model_config=model_config_dict,
                tool_manager=tool_manager,
                skill_manager=skill_manager,
                preset_running_config=preset_running_config,
                workspace=server_args.workspace,
                memory_type=server_args.memory_type,
                context_budget_config=context_budget_config
            )
            logger.info("é»˜è®¤ SageStreamService åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("æ¨¡å‹å®¢æˆ·ç«¯æœªé…ç½®ï¼Œæµå¼æœåŠ¡ä¸å¯ç”¨")
            
    except Exception as e:
        logger.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error(traceback.format_exc())

def add_cors_headers(response):
    """æ·»åŠ  CORS å¤´"""
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "*"

async def cleanup_system():
    """æ¸…ç†ç³»ç»Ÿèµ„æº"""
    global default_stream_service, tool_manager, default_model_client
    
    logger.info("æ­£åœ¨æ¸…ç†ç³»ç»Ÿèµ„æº...")
    
    try:
        if tool_manager:
            # æ¸…ç†å·¥å…·ç®¡ç†å™¨èµ„æº
            logger.info("æ¸…ç†å·¥å…·ç®¡ç†å™¨èµ„æº")
            
        default_stream_service = None
        tool_manager = None
        default_model_client = None
        
        logger.info("ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"ç³»ç»Ÿèµ„æºæ¸…ç†å¤±è´¥: {e}")

# Pydantic æ¨¡å‹å®šä¹‰
class ChatMessage(BaseModel):
    role: Optional[str] = None
    content: Optional[Any] = None
    message_id: Optional[str] = None
    type: Optional[str] = "normal"
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    show_content: Optional[str] = None
    # æ·»åŠ å†å²å¯¹è¯ä¸­å¯èƒ½å­˜åœ¨çš„å­—æ®µ
    message_type: Optional[str] = None
    timestamp: Optional[Union[float, str]] = None
    chunk_id: Optional[str] = None
    is_final: Optional[bool] = None
    is_chunk: Optional[bool] = None
    metadata: Optional[Dict[str, Any]] = None
    session_id: Optional[str] = None

class StreamRequest(BaseModel):
    messages: List[ChatMessage]
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    deep_thinking: Optional[Union[bool, str]] = None
    max_loop_count: int = 10
    multi_agent: Optional[Union[bool, str]] = None
    summary : bool =True  # è¿‡æ—¶å­—æ®µ
    deep_research: bool = True # è¿‡æ—¶å­—æ®µï¼Œä¸multi_agentä¸€è‡´
    more_suggest: bool = False
    force_summary: bool = False
    system_context: Optional[Dict[str, Any]] = None
    available_workflows: Optional[Dict[str, List[str]]] = None
    llm_model_config: Optional[Dict[str, Any]] = None
    system_prefix: Optional[str] = None
    available_tools: Optional[List[str]] = None
    available_skills: Optional[List[str]] = None # Added for skill restriction
    
    def __init__(self, **data):
        # å¤„ç†å­—æ®µå…¼å®¹æ€§
        if 'deep_research' in data and 'multi_agent' not in data:
            data['multi_agent'] = data['deep_research']
            warnings.warn("deep_researchå­—æ®µå·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨multi_agent", DeprecationWarning)
        
        if 'summary' in data:
            warnings.warn("summaryå­—æ®µå·²è¿‡æ—¶ï¼Œå°†è¢«å¿½ç•¥", DeprecationWarning)
            
        super().__init__(**data)


def get_local_ip() -> str:
    """
    è·å–æœ¬æœºçš„å®é™…IPåœ°å€
    """
    import socket
    try:
        # åˆ›å»ºä¸€ä¸ªUDP socketè¿æ¥åˆ°å¤–éƒ¨åœ°å€æ¥è·å–æœ¬æœºIP
        # è¿™é‡Œä½¿ç”¨8.8.8.8ä½œä¸ºç›®æ ‡ï¼Œä½†å®é™…ä¸ä¼šå‘é€æ•°æ®
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
            return local_ip
    except Exception as e:
        logger.warning(f"æ— æ³•è·å–æœ¬æœºIPåœ°å€ï¼Œä½¿ç”¨localhost: {e}")
        return "localhost"


def generate_curl_command(request: StreamRequest, host: str = "localhost", port: int = 8001) -> str:
    """
    æ ¹æ®StreamRequestç”Ÿæˆå¯¹åº”çš„curlå‘½ä»¤
    """
    import json
    
    # æ„å»ºè¯·æ±‚ä½“
    request_data = request.dict()
    
    # æ„å»ºcurlå‘½ä»¤
    curl_command = f"""curl -X POST "http://{host}:{port}/api/stream" \\
  -H "Content-Type: application/json" \\
  -d '{json.dumps(request_data, ensure_ascii=False, indent=2)}'"""
    
    return curl_command


def save_curl_command_to_session(curl_command: str, session_id: str, workspace_root: str):
    """
    å°†curlå‘½ä»¤ä¿å­˜åˆ°æŒ‡å®šsessionçš„å·¥ä½œç©ºé—´æ–‡ä»¶å¤¹ä¸­
    """
    import os
    from datetime import datetime
    
    try:
        # æ„å»ºsessionæ–‡ä»¶å¤¹è·¯å¾„
        session_folder = os.path.join(workspace_root, session_id)
        
        # ç¡®ä¿sessionæ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs(session_folder, exist_ok=True)
        
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        curl_file_path = os.path.join(session_folder, f"curl_command_{timestamp}.txt")
        
        # ä¿å­˜curlå‘½ä»¤åˆ°æ–‡ä»¶
        with open(curl_file_path, 'w', encoding='utf-8') as f:
            f.write(curl_command)
        
        logger.info(f"Curl command saved to: {curl_file_path}")
        return curl_file_path
        
    except Exception as e:
        logger.error(f"Failed to save curl command: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


class ConfigRequest(BaseModel):
    api_key: str
    model_name: str = "deepseek-chat"
    base_url: str = "https://api.deepseek.com/v1"
    max_tokens: Optional[int] = 4096
    temperature: Optional[float] = 0.7

class ToolInfo(BaseModel):
    name: str
    description: str
    parameters: Dict[str, Any]
    type: str  # å·¥å…·ç±»å‹ï¼šbasic, mcp, agent
    source: str  # å·¥å…·æ¥æº

class SystemStatus(BaseModel):
    status: str
    service_name: str = "SageStreamService"
    tools_count: int
    active_sessions: int
    version: str = "1.0"

class InterruptRequest(BaseModel):
    message: str = "ç”¨æˆ·è¯·æ±‚ä¸­æ–­"

class AutoGenAgentRequest(BaseModel):
    """è‡ªåŠ¨ç”ŸæˆAgenté…ç½®çš„è¯·æ±‚æ¨¡å‹"""
    agent_description: str  # Agentæè¿°
    available_tools: Optional[List[str]] = None  # å¯é€‰çš„å·¥å…·åç§°åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™åªä½¿ç”¨è¿™äº›å·¥å…·

class AutoGenAgentResponse(BaseModel):
    """è‡ªåŠ¨ç”ŸæˆAgentå“åº”"""
    success: bool
    message: str
    agent_config: Optional[Dict[str, Any]] = None

class SystemPromptOptimizeRequest(BaseModel):
    """ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–è¯·æ±‚"""
    original_prompt: str  # åŸå§‹ç³»ç»Ÿæç¤ºè¯
    optimization_goal: Optional[str] = None  # ä¼˜åŒ–ç›®æ ‡ï¼ˆå¯é€‰ï¼‰

class SystemPromptOptimizeResponse(BaseModel):
    """ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–å“åº”"""
    success: bool
    message: str
    optimized_prompt: Optional[str] = None  # ä¼˜åŒ–åçš„æç¤ºè¯
    optimization_details: Optional[Dict[str, Any]] = None  # ä¼˜åŒ–è¯¦æƒ…


class ScoreEvaluationRequest(StreamRequest):
    """è¯„ä¼°æ‰“åˆ†è¯·æ±‚"""
    checkpoints: list| dict

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "service": "ReagentStreamService"
    }

@app.get("/api/tools", response_model=List[ToolInfo])
async def get_tools(response: Response):
    """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
    add_cors_headers(response)
    
    try:
        tools = []
        
        if tool_manager:
            available_tools = tool_manager.list_tools_with_type()
            
            for tool_info in available_tools:
                tools.append(ToolInfo(
                    name=tool_info.get("name", ""),
                    description=tool_info.get("description", ""),
                    parameters=tool_info.get("parameters", {}),
                    type=tool_info.get("type", "basic"),
                    source=tool_info.get("source", "internal")
                ))
        
        return tools
    except Exception as e:
        logger.error(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.post("/api/stream")
async def stream_chat(request: StreamRequest):
    """æµå¼èŠå¤©æ¥å£"""
    if not default_stream_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªé…ç½®æˆ–ä¸å¯ç”¨")
    # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´ï¼Œç”¨äºé¦–tokenè€—æ—¶ç»Ÿè®¡
    api_request_start_time = time.time()

    logger.info(f"Server: è¯·æ±‚å‚æ•°: {request}")
    # ç”Ÿæˆä¼šè¯ID
    # llm_model_config={'model': '', 'maxTokens': '', 'temperature': ''}
    # å¦‚æœæ˜¯value æ˜¯ç©ºï¼Œåˆ é™¤key
    if request.llm_model_config:
        request.llm_model_config = {k: v for k, v in request.llm_model_config.items() if v is not None and v != ''}

    session_id = request.session_id or str(uuid.uuid4())
    logger.info(f"ğŸ“¥ APIè¯·æ±‚å¼€å§‹: ä¼šè¯ID: {session_id}, å¼€å§‹æ—¶é—´æˆ³: {api_request_start_time:.3f}")
    
    # ç”Ÿæˆå¹¶ä¿å­˜curlå‘½ä»¤åˆ°sessionæ–‡ä»¶å¤¹
    try:
        # å¦‚æœhostæ˜¯0.0.0.0ï¼Œåˆ™ä½¿ç”¨æœ¬æœºå®é™…IPåœ°å€
        actual_host = get_local_ip() if server_args.host == "0.0.0.0" else server_args.host
        curl_command = generate_curl_command(request, actual_host, server_args.port)
        save_curl_command_to_session(curl_command,session_id , server_args.workspace)
        logger.info(f"å·²ä¿å­˜curlå‘½ä»¤åˆ°session {session_id}")
    except Exception as e:
        logger.error(f"ä¿å­˜curlå‘½ä»¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    # åˆ¤æ–­æ˜¯å¦è¦åˆå§‹åŒ–æ–°çš„ sage service è¿˜æ˜¯ä½¿ç”¨é»˜è®¤çš„
    # å–å†³äºæ˜¯å¦éœ€è¦è‡ªå®šä¹‰æ¨¡å‹ä»¥åŠ agent çš„system prefix ï¼Œä»¥åŠå¯¹tool çš„å·¥å…·æ˜¯å¦æœ‰é™åˆ¶
    if request.llm_model_config or request.system_prefix or request.available_tools or request.available_skills:
        llm_config_dict = request.llm_model_config or {}
        # æ ¹æ®model config åˆå§‹åŒ–æ–°çš„æ¨¡å‹å®¢æˆ·ç«¯
        logger.info(f"åˆå§‹åŒ–æ–°çš„æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ¨¡å‹é…ç½®api_key :{llm_config_dict.get('api_key', server_args.default_llm_api_key)}")
        logger.info(f"åˆå§‹åŒ–æ–°çš„æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ¨¡å‹é…ç½®base_url :{llm_config_dict.get('base_url', server_args.default_llm_api_base_url)}")
        logger.info(f"åˆå§‹åŒ–æ–°çš„æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ¨¡å‹é…ç½®model :{llm_config_dict.get('model', server_args.default_llm_model_name)}")
        model_client = AsyncOpenAI(
            api_key=llm_config_dict.get('api_key', server_args.default_llm_api_key),
            base_url=llm_config_dict.get('base_url', server_args.default_llm_api_base_url),
        )
        llm_model_config = {
            'model': llm_config_dict.get('model', server_args.default_llm_model_name)
        }
        
        # åªæœ‰åœ¨æœ‰æœ‰æ•ˆçš„max_tokenså€¼æ—¶æ‰æ·»åŠ è¯¥é”®ï¼Œé¿å…Noneå€¼å¯¼è‡´é”™è¯¯
        max_tokens_value = llm_config_dict.get('max_tokens', server_args.default_llm_max_tokens)
        max_model_len = llm_config_dict.get('max_model_len', server_args.default_llm_max_model_len)
        if max_tokens_value is not None:
            llm_model_config['max_tokens'] = int(max_tokens_value)
            
        # åªæœ‰åœ¨æœ‰æœ‰æ•ˆçš„temperatureå€¼æ—¶æ‰æ·»åŠ è¯¥é”®ï¼Œé¿å…Noneå€¼å¯¼è‡´é”™è¯¯
        temperature_value = llm_config_dict.get('temperature', server_args.default_llm_temperature)
        if temperature_value is not None:
            llm_model_config['temperature'] = float(temperature_value)
        
        top_p_value = llm_config_dict.get('top_p', server_args.default_llm_top_p)
        if top_p_value is not None:
            llm_model_config['top_p'] = float(top_p_value)
        
        presence_penalty_value = llm_config_dict.get('presence_penalty', server_args.default_llm_presence_penalty)
        if presence_penalty_value is not None:
            llm_model_config['presence_penalty'] = float(presence_penalty_value)
        
        
        
        logger.info(f"åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ¨¡å‹é…ç½®: {llm_model_config}")

        if request.available_tools is not None:
            logger.info(f"åˆå§‹åŒ–å·¥å…·ä»£ç†ï¼Œå¯ç”¨å·¥å…·: {request.available_tools}")
            start_tool_proxy = time.time()
            # å¦‚æœrequest.multi_agent æ˜¯trueï¼Œè¦ç¡®ä¿request.available_toolsæ²¡æœ‰ complete_task è¿™ä¸ªå·¥å…·
            if request.multi_agent and 'complete_task' in request.available_tools:
                request.available_tools.remove('complete_task')
            tool_proxy = ToolProxy(tool_manager,request.available_tools)
            end_tool_proxy = time.time()
            logger.info(f"åˆå§‹åŒ–å·¥å…·ä»£ç†è€—æ—¶: {end_tool_proxy - start_tool_proxy} ç§’")
        else:
            tool_proxy = tool_manager

        if request.available_skills is not None:
            logger.info(f"åˆå§‹åŒ–æŠ€èƒ½ä»£ç†ï¼Œå¯ç”¨æŠ€èƒ½: {request.available_skills}")
            start_skill_proxy = time.time()
            skill_proxy = SkillProxy(skill_manager, request.available_skills)
            end_skill_proxy = time.time()
            logger.info(f"åˆå§‹åŒ–æŠ€èƒ½ä»£ç†è€—æ—¶: {end_skill_proxy - start_skill_proxy} ç§’")
        else:
            skill_proxy = skill_manager

        start_stream_service = time.time()
        # æ„å»ºcontext_budget_configå­—å…¸
        # max_model_lenç»Ÿä¸€ä½¿ç”¨è¯·æ±‚ä¸­çš„max_model_lenï¼ˆå¦‚æœæä¾›ï¼‰æˆ–default_llm_max_model_len
        context_budget_config = {
            'max_model_len': max_model_len
        }
        if server_args.context_history_ratio is not None:
            context_budget_config['history_ratio'] = server_args.context_history_ratio
        if server_args.context_active_ratio is not None:
            context_budget_config['active_ratio'] = server_args.context_active_ratio
        if server_args.context_max_new_message_ratio is not None:
            context_budget_config['max_new_message_ratio'] = server_args.context_max_new_message_ratio
        if server_args.context_recent_turns is not None:
            context_budget_config['recent_turns'] = server_args.context_recent_turns
        
        # åˆå§‹åŒ–æ–°çš„ sage service
        stream_service = SageStreamService(
            model=model_client,
            model_config=llm_model_config,
            tool_manager=tool_proxy,
            skill_manager=skill_proxy,
            preset_running_config={
                "system_prefix": request.system_prefix
            },
            workspace=server_args.workspace,
            memory_type=server_args.memory_type,
            context_budget_config=context_budget_config
        )
        end_stream_service = time.time()
        logger.info(f"åˆå§‹åŒ–æµå¼æœåŠ¡è€—æ—¶: {end_stream_service - start_stream_service} ç§’")
        all_active_sessions_service_map[session_id] = {
            'stream_service': stream_service,
            'session_id': session_id,
            "is_default": False
        }
    else:
        logger.info(f"ä½¿ç”¨é»˜è®¤çš„æµå¼æœåŠ¡ï¼Œä¼šè¯ID: {session_id}")
        # ä½¿ç”¨é»˜è®¤çš„ sage service
        stream_service = default_stream_service
        # è®°å½•ä¼šè¯ID
        all_active_sessions_service_map[session_id] = {
            'stream_service': stream_service,
            'session_id': session_id,
            "is_default": True
        }

    async def generate_stream(stream_service):
        """ç”ŸæˆSSEæµ"""
        try:
            # ç›´æ¥è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼Œä¸è¿›è¡Œå†…å®¹è°ƒæ•´
            messages = []
            for msg in request.messages:
                # ä¿æŒåŸå§‹æ¶ˆæ¯çš„æ‰€æœ‰å­—æ®µ
                message_dict = msg.model_dump()
                # å¦‚æœæœ‰content ä¸€å®šè¦è½¬åŒ–æˆstr
                if message_dict.get('content'):
                    message_dict['content'] = str(message_dict['content'])
                messages.append(message_dict)
            
            logger.info(f"å¼€å§‹æµå¼å¤„ç†ï¼Œä¼šè¯ID: {session_id}")
            
            # æ‰“å°è¯·æ±‚ä½“å†…å®¹
            logger.info(f"è¯·æ±‚ä½“å†…å®¹: {request}")
            
            # æ·»åŠ æµå¤„ç†è®¡æ•°å™¨å’Œè¿æ¥çŠ¶æ€è·Ÿè¸ª
            stream_counter = 0
            last_activity_time = time.time()
            # é¦–ä¸ªtokenè¿”å›è€—æ—¶æ ‡è®°
            first_token_logged = False
            
            # å¤„ç†æµå¼å“åº”ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°
            async for result in stream_service.process_stream(
                messages=messages, 
                session_id=session_id,
                user_id=request.user_id,
                deep_thinking=request.deep_thinking,
                max_loop_count=request.max_loop_count,
                multi_agent=request.multi_agent,
                more_suggest=request.more_suggest,
                system_context=request.system_context,
                available_workflows=request.available_workflows,
                force_summary=request.force_summary
            ):
                # æ›´æ–°æµå¤„ç†è®¡æ•°å™¨å’Œæ´»åŠ¨æ—¶é—´
                stream_counter += 1
                current_time = time.time()
                time_since_last = current_time - last_activity_time
                last_activity_time = current_time
                
                # æ¯100ä¸ªç»“æœè®°å½•ä¸€æ¬¡è¿æ¥çŠ¶æ€
                if stream_counter % 100 == 0:
                    logger.info(f"ğŸ“Š æµå¤„ç†çŠ¶æ€ - ä¼šè¯: {session_id}, è®¡æ•°: {stream_counter}, é—´éš”: {time_since_last:.3f}s")
                
                # å¤„ç†å¤§JSONçš„åˆ†å—ä¼ è¾“
                try:
                    json_str = json.dumps(result, ensure_ascii=False)
                    json_size = len(json_str)
                    
                    # å¯¹äºè¶…å¤§JSONï¼Œä½¿ç”¨åˆ†å—å‘é€ç¡®ä¿å®Œæ•´æ€§
                    if json_size > 32768:  # 32KBä»¥ä¸Šä½¿ç”¨åˆ†å—å‘é€
                        logger.info(f"ğŸ”„ å¤§JSONåˆ†å—å‘é€: {json_size} å­—ç¬¦")
                        
                        # åˆ†å—å‘é€å¤§JSON
                        chunk_size = 8192  # 8KB per chunk
                        total_chunks = (json_size + chunk_size - 1) // chunk_size
                        
                        # å‘é€åˆ†å—å¼€å§‹æ ‡è®°
                        start_marker = {
                            'type': 'chunk_start',
                            'message_id': result.get('message_id', 'unknown'),
                            'total_size': json_size,
                            'total_chunks': total_chunks,
                            'chunk_size': chunk_size,
                            'original_type': result.get('type', 'unknown')
                        }
                        # é¦–ä¸ªtokenè€—æ—¶æ—¥å¿—ï¼ˆé¦–æ¬¡yieldå‰ï¼‰
                        if not first_token_logged:
                            first_latency = time.time() - api_request_start_time
                            logger.info(f"â±ï¸ é¦–tokenå“åº”è€—æ—¶: {first_latency:.3f}sï¼Œä¼šè¯ID: {session_id}")
                            first_token_logged = True
                        yield json.dumps(start_marker, ensure_ascii=False) + "\n"
                        await asyncio.sleep(0.01)  # å»¶è¿Ÿç¡®ä¿å‰ç«¯å‡†å¤‡å¥½
                        
                        for i in range(total_chunks):
                            start = i * chunk_size
                            end = min(start + chunk_size, json_size)
                            chunk_data = json_str[start:end]
                            
                            # åˆ›å»ºåˆ†å—æ¶ˆæ¯
                            chunk_message = {
                                'type': 'json_chunk',
                                'message_id': result.get('message_id', 'unknown'),  # æ·»åŠ message_idå­—æ®µ
                                'chunk_id': f"{result.get('message_id', 'unknown')}_{i}",
                                'chunk_index': i,
                                'total_chunks': total_chunks,
                                'chunk_data': chunk_data,
                                'chunk_size': len(chunk_data),
                                'is_final': i == total_chunks - 1,
                                'checksum': hash(chunk_data) % 1000000
                            }
                            # é¦–ä¸ªtokenè€—æ—¶æ—¥å¿—ï¼ˆé¦–æ¬¡yieldå‰ï¼‰
                            if not first_token_logged:
                                first_latency = time.time() - api_request_start_time
                                logger.info(f"â±ï¸ é¦–tokenå“åº”è€—æ—¶: {first_latency:.3f}sï¼Œä¼šè¯ID: {session_id}")
                                first_token_logged = True
                            yield json.dumps(chunk_message, ensure_ascii=False) + "\n"
                            await asyncio.sleep(0.005)  # é€‚ä¸­å»¶è¿Ÿç¡®ä¿é¡ºåº
                        
                        # å‘é€åˆ†å—ç»“æŸæ ‡è®°
                        end_marker = {
                            'type': 'chunk_end',
                            'message_id': result.get('message_id', 'unknown'),
                            'total_chunks': total_chunks,
                            'expected_size': json_size,
                            'original_type': result.get('type', 'unknown')
                        }
                        yield json.dumps(end_marker, ensure_ascii=False) + "\n"
                        
                        logger.info(f"âœ… å®Œæˆåˆ†å—å‘é€: {total_chunks} å—")
                    else:
                        # å°JSONç›´æ¥å‘é€
                        # é¦–ä¸ªtokenè€—æ—¶æ—¥å¿—ï¼ˆé¦–æ¬¡yieldå‰ï¼‰
                        if not first_token_logged:
                            first_latency = time.time() - api_request_start_time
                            logger.info(f"â±ï¸ é¦–tokenå“åº”è€—æ—¶: {first_latency:.3f}sï¼Œä¼šè¯ID: {session_id}")
                            first_token_logged = True
                        yield json.dumps(result, ensure_ascii=False) + "\n"
                        
                except Exception as e:
                    logger.error(f"JSONåºåˆ—åŒ–å¤±è´¥: {e}")
                    # åˆ›å»ºé”™è¯¯å“åº”
                    error_data = {
                        'type': 'error',
                        'message_id': result.get('message_id', 'error'),
                        'content': f'æ•°æ®å¤„ç†é”™è¯¯: {str(e)}',
                        'original_size': len(str(result)),
                        'error': True
                    }
                    # é¦–ä¸ªtokenè€—æ—¶æ—¥å¿—ï¼ˆé¦–æ¬¡yieldå‰ï¼‰
                    if not first_token_logged:
                        first_latency = time.time() - api_request_start_time
                        logger.info(f"â±ï¸ é¦–tokenå“åº”è€—æ—¶: {first_latency:.3f}sï¼Œä¼šè¯ID: {session_id}")
                        first_token_logged = True
                    yield json.dumps(error_data, ensure_ascii=False) + "\n"
                    
                await asyncio.sleep(0.01)  # é¿å…è¿‡å¿«å‘é€
            # å‘é€æµç»“æŸæ ‡è®°
            end_data = {
                'type': 'stream_end',
                'session_id': session_id,
                'timestamp': time.time(),
                'total_stream_count': stream_counter
            }
            # token_usage ç°åœ¨é€šè¿‡ç‰¹æ®Šçš„ MessageChunk åœ¨ run_stream çš„ finally å—ä¸­è¿”å›
            # è¿™é‡Œä¸å†éœ€è¦é¢å¤–å¤„ç† token_usage
            total_duration = time.time() - (last_activity_time - time_since_last if 'time_since_last' in locals() else last_activity_time)
            logger.info(f"âœ… å®Œæˆæµå¼å¤„ç†: ä¼šè¯ {session_id}, æ€»è®¡ {stream_counter} ä¸ªæµç»“æœ, è€—æ—¶ {total_duration:.3f}s")
            logger.info(f"âœ… æµç»“æŸæ•°æ®: {end_data}")
            yield json.dumps(end_data, ensure_ascii=False) + "\n"
        
        except GeneratorExit as ge:
            import sys
            disconnect_msg = f"ğŸ”Œ [GENERATOR_EXIT] å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œç”Ÿæˆå™¨è¢«å…³é—­ - ä¼šè¯ID: {session_id}, æ—¶é—´: {time.time()}"
            logger.error(disconnect_msg)
            logger.error(f"ğŸ” [GENERATOR_EXIT] GeneratorExitè¯¦æƒ…: {type(ge).__name__} - {str(ge)}")
            logger.error(f"ğŸ“‹ [GENERATOR_EXIT] å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
            logger.error(f"ğŸ“Š [GENERATOR_EXIT] æµå¤„ç†ç»Ÿè®¡: å·²å¤„ç† {stream_counter if 'stream_counter' in locals() else 0} ä¸ªæµç»“æœ")
            # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—ç¼“å†²åŒº
            sys.stdout.flush()
            sys.stderr.flush()
            
        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¼‚å¸¸: {e}")
            logger.error(traceback.format_exc())
            error_data = {
                'type': 'error',
                'message': str(e),
                'session_id': session_id
            }
            yield json.dumps(error_data, ensure_ascii=False) + "\n"
        finally:
            logger.info('server generate_stream finally save info and delete')
            # æ¸…ç†ä¼šè¯èµ„æº
            if session_id in all_active_sessions_service_map:
                stream_service = all_active_sessions_service_map[session_id]['stream_service']
                if stream_service:
                    if stream_service.save_session(session_id):
                        logger.info(f"ä¼šè¯ {session_id} çŠ¶æ€å·²ä¿å­˜")
                    else:
                        logger.error(f"ä¼šè¯ {session_id} ä¿å­˜å¤±è´¥ï¼Œå·²ç»ä¿å­˜")
                del all_active_sessions_service_map[session_id]
                logger.info(f"ä¼šè¯ {session_id} èµ„æºå·²æ¸…ç†")
    
    return StreamingResponse(
        generate_stream(stream_service),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.post("/api/sessions/{session_id}/interrupt")
async def interrupt_session(session_id: str, request: Optional[InterruptRequest] = None):
    """ä¸­æ–­æŒ‡å®šä¼šè¯"""
    session_info = all_active_sessions_service_map.get(session_id)
    if not session_info:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

    stream_service = session_info['stream_service']

    if not stream_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªé…ç½®æˆ–ä¸å¯ç”¨")
    try:
        message = request.message if request else "ç”¨æˆ·è¯·æ±‚ä¸­æ–­"
        success = stream_service.interrupt_session(session_id, message)
        
        if success:
            logger.info(f"ä¼šè¯ {session_id} ä¸­æ–­æˆåŠŸ")
            return {
                "status": "success",
                "message": f"ä¼šè¯ {session_id} å·²ä¸­æ–­",
                "session_id": session_id
            }
        else:
            return {
                "status": "not_found",
                "message": f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²ç»“æŸ",
                "session_id": session_id
            }
    except Exception as e:
        logger.error(f"ä¸­æ–­ä¼šè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¸­æ–­ä¼šè¯å¤±è´¥: {str(e)}")

# è·å–æŒ‡å®šseesion id çš„å½“å‰çš„ä»»åŠ¡ç®¡ç†å™¨ä¸­çš„ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
@app.post("/api/sessions/{session_id}/tasks_status")
async def get_session_status(session_id: str):
    """è·å–æŒ‡å®šä¼šè¯çš„çŠ¶æ€"""
    session_info = all_active_sessions_service_map.get(session_id)
    if not session_info:
        return {
            "status": "not_found",
            "message": f"ä¼šè¯ {session_id} å·²å®Œæˆæˆ–è€…ä¸å­˜åœ¨",
            "session_id": session_id,
            "tasks_status": None
        }
    stream_service = session_info['stream_service']
    tasks_status = stream_service.sage_controller.get_tasks_status(session_id)
    tasks_status['tasks']
    logger.info(f"è·å–ä¼šè¯ {session_id} ä»»åŠ¡æ•°é‡ï¼š{len(tasks_status['tasks'])}")
    return {
        "status": "success",
        "message": f"ä¼šè¯ {session_id} çŠ¶æ€è·å–æˆåŠŸ",
        "session_id": session_id,
        "tasks_status": tasks_status
    }


@app.post("/api/sessions/{session_id}/file_workspace")
async def get_file_workspace(session_id: str):
    session_info = all_active_sessions_service_map.get(session_id)
    if not session_info:
        return {
            "status": "success",
            "message": f"ä¼šè¯ {session_id} å·²å®Œæˆæˆ–è€…ä¸å­˜åœ¨",
            "session_id": session_id,
            "files": []
        }
    try:
        session_context = get_session_context(session_id)
    except Exception:
        return {
            "status": "success",
            "message": f"ä¼šè¯ {session_id} å·²å®Œæˆæˆ–è€…ä¸å­˜åœ¨",
            "session_id": session_id,
            "files": []
        }
    # è¿™ä¸ªä¼šè¯çš„å·¥ä½œç©ºé—´çš„ï¼Œç»å¯¹è·¯å¾„
    workspace_path = session_context.agent_workspace
    
    if not os.path.exists(workspace_path):
        return {
            "status": "success",
            "message": "å·¥ä½œç©ºé—´ä¸ºç©º",
            "session_id": session_id,
            "files": []
        }
    
    files = []
    try:
        for root, dirs, filenames in os.walk(workspace_path):
            for filename in filenames:
                file_path = os.path.join(root, filename)
                # è®¡ç®—ç›¸å¯¹äºå·¥ä½œç©ºé—´çš„è·¯å¾„
                relative_path = os.path.relpath(file_path, workspace_path)
                file_stat = os.stat(file_path)
                files.append({
                    "name": filename,
                    "path": relative_path,
                    "size": file_stat.st_size,
                    "modified_time": file_stat.st_mtime,
                    "is_directory": False
                })
            
            for dirname in dirs:
                dir_path = os.path.join(root, dirname)
                relative_path = os.path.relpath(dir_path, workspace_path)
                files.append({
                    "name": dirname,
                    "path": relative_path,
                    "size": 0,
                    "modified_time": os.stat(dir_path).st_mtime,
                    "is_directory": True
                })
        logger.info(f"è·å–ä¼šè¯ {session_id} å·¥ä½œç©ºé—´æ–‡ä»¶æ•°é‡ï¼š{len(files)}")
        return {
            "status": "success",
            "message": "è·å–æ–‡ä»¶åˆ—è¡¨æˆåŠŸ",
            "session_id": session_id,
            "files": files,
            "agent_workspace": session_context.agent_workspace
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}",
            "session_id": session_id,
            "files": []
        }

# æŒ‡å®šagent workspace ä»¥åŠfile è¿›è¡Œä¸‹è½½
@app.get("/api/sessions/file_workspace/download")
async def download_file(request: Request):
    """ä¸‹è½½å·¥ä½œç©ºé—´ä¸­çš„æŒ‡å®šæ–‡ä»¶"""
    file_path = request.query_params.get("file_path")
    workspace_path = request.query_params.get("workspace_path")
    
    if not file_path or not workspace_path:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦çš„å‚æ•°: file_path æˆ– workspace_path")
    
    # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    full_file_path = os.path.join(workspace_path, file_path)
    
    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨å·¥ä½œç©ºé—´å†…
    if not os.path.abspath(full_file_path).startswith(os.path.abspath(workspace_path)):
        raise HTTPException(status_code=403, detail="è®¿é—®è¢«æ‹’ç»ï¼šæ–‡ä»¶è·¯å¾„è¶…å‡ºå·¥ä½œç©ºé—´èŒƒå›´")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(full_file_path):
        raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆä¸æ˜¯ç›®å½•ï¼‰
    if not os.path.isfile(full_file_path):
        raise HTTPException(status_code=400, detail=f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}")
    
    try:
        # è¿”å›æ–‡ä»¶å†…å®¹
        return FileResponse(
            path=full_file_path,
            filename=os.path.basename(file_path),
            media_type='application/octet-stream'
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

class ExecToolRequest(BaseModel):
    tool_name: str
    tool_params: Dict[str, Any]

@app.post("/api/tools/exec")
async def exec_tool(request: ExecToolRequest):
    """æ‰§è¡Œå·¥å…·"""
    logger.info(f"æ‰§è¡Œå·¥å…·è¯·æ±‚: {request}")
    try:
        if not tool_manager:
            logger.error("å·¥å…·ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return {"status": "error", "message": "å·¥å…·ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

        # æ£€æµ‹å·¥å…·æ˜¯å¦å­˜åœ¨
        if request.tool_name not in tool_manager.tools.keys():
            logger.error(f"æ‰§è¡Œå·¥å…·å¤±è´¥: {request.tool_name}")
            return {"status": "error", "message": "å·¥å…·ä¸å­˜åœ¨"}

        tool_response = tool_manager.run_tool(
                tool_name=request.tool_name,
                session_context=None,
                session_id="",
                **request.tool_params
            )
        if tool_response:
            logger.info(f"æ‰§è¡Œå·¥å…·æˆåŠŸ: {request.tool_name}")
            return {"status": "success", "message": "å·¥å…·æ‰§è¡ŒæˆåŠŸ", "data": tool_response}
        else:
            logger.error(f"æ‰§è¡Œå·¥å…·å¤±è´¥: {request.tool_name}")
            return {"status": "error", "message": "å·¥å…·æ‰§è¡Œå¤±è´¥"}
    except Exception as e:
        logger.error(f"æ‰§è¡Œå·¥å…·å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰§è¡Œå·¥å…·å¤±è´¥: {str(e)}")


class MCPServerRequest(BaseModel):
    name: str
    streaming_http_url: Optional[str] = None
    sse_url: Optional[str] = None
    api_key: Optional[str] = None
    disabled: bool = False

@app.post("/api/agent/auto-generate", response_model=AutoGenAgentResponse)
async def auto_generate_agent(request: AutoGenAgentRequest):
    """
    è‡ªåŠ¨ç”ŸæˆAgenté…ç½®çš„APIæ¥å£
    
    æ ¹æ®Agentæè¿°å’Œå·¥å…·ç®¡ç†å™¨è‡ªåŠ¨ç”ŸæˆAgenté…ç½®
    """
    start_time = time.time()
    logger.info(f"å¼€å§‹å¤„ç†Agentè‡ªåŠ¨ç”Ÿæˆè¯·æ±‚ï¼Œæè¿°é•¿åº¦: {len(request.agent_description)}")
    
    try:
        # ä½¿ç”¨æœåŠ¡å™¨é»˜è®¤çš„LLMå®¢æˆ·ç«¯
        global default_model_client, tool_manager
        
        if default_model_client is None:
            logger.error("é»˜è®¤LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return AutoGenAgentResponse(
                success=False,
                message="é»˜è®¤LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            )
        
        if tool_manager is None:
            logger.error("å·¥å…·ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return AutoGenAgentResponse(
                success=False,
                message="å·¥å…·ç®¡ç†å™¨æœªåˆå§‹åŒ–"
            )
        
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {server_args.default_llm_model_name}")
        logger.info(f"å¯ç”¨å·¥å…·æ•°é‡: {len(tool_manager.tools)}")
        
        # åˆ›å»ºAutoGenAgentFuncå®ä¾‹
        auto_gen_agent = AutoGenAgentFunc()
        
        # æ ¹æ®æ˜¯å¦æä¾›å·¥å…·åˆ—è¡¨å†³å®šä½¿ç”¨ToolManagerè¿˜æ˜¯ToolProxy
        if request.available_tools:
            logger.info(f"ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åˆ—è¡¨: {request.available_tools}")
            # åˆ›å»ºToolProxyï¼ŒåªåŒ…å«æŒ‡å®šçš„å·¥å…·
            tool_proxy = ToolProxy(tool_manager, request.available_tools)
            tool_manager_or_proxy = tool_proxy
        else:
            logger.info("ä½¿ç”¨å®Œæ•´çš„å·¥å…·ç®¡ç†å™¨")
            tool_manager_or_proxy = tool_manager
        
        # ç”ŸæˆAgenté…ç½®ï¼Œä½¿ç”¨æœåŠ¡å™¨é»˜è®¤é…ç½®
        logger.info("å¼€å§‹è°ƒç”¨AutoGenAgentFuncç”Ÿæˆé…ç½®")
        agent_config = await auto_gen_agent.generate_agent_config(
            agent_description=request.agent_description,
            tool_manager=tool_manager_or_proxy,
            llm_client=default_model_client,
            model=server_args.default_llm_model_name
        )
        
        if agent_config is None:
            logger.error("AutoGenAgentFuncè¿”å›None")
            return AutoGenAgentResponse(
                success=False,
                message="ç”ŸæˆAgenté…ç½®å¤±è´¥"
            )
        
        elapsed_time = time.time() - start_time
        logger.info(f"Agenté…ç½®ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return AutoGenAgentResponse(
            success=True,
            message="Agenté…ç½®ç”ŸæˆæˆåŠŸ",
            agent_config=agent_config
        )
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"è‡ªåŠ¨ç”ŸæˆAgenté…ç½®å¤±è´¥ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œé”™è¯¯: {str(e)}")
        logger.error(traceback.format_exc())
        return AutoGenAgentResponse(
            success=False,
            message=f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        )

@app.post("/api/mcp/add")
async def add_mcp_server(request: MCPServerRequest, response: Response):
    """æ·»åŠ MCP serveråˆ°tool manager"""
    add_cors_headers(response)
    
    try:
        global tool_manager, default_stream_service
        
        if not tool_manager:
            raise HTTPException(status_code=503, detail="å·¥å…·ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        
        logger.info(f"å¼€å§‹æ·»åŠ MCP server: {request.name}")
        
        # æ·»åŠ æ–°çš„MCP serveré…ç½®
        server_config: Dict[str, Any] = {
            "disabled": request.disabled
        }
        if request.streaming_http_url:
            server_config["streaming_http_url"] = request.streaming_http_url
        if request.sse_url:
            server_config["sse_url"] = request.sse_url
        if request.api_key:
            server_config["api_key"] = request.api_key
        
        # æ·»åŠ æ–°çš„MCP serveråˆ°å·¥å…·ç®¡ç†å™¨
        success = tool_manager.register_mcp_server(request.name, server_config)
        if success:
            # è¯»å–ç°æœ‰çš„MCPé…ç½®
            mcp_config_path = server_args.mcp_config
            if os.path.exists(mcp_config_path):
                with open(mcp_config_path, 'r', encoding='utf-8') as f:
                    mcp_config = json.load(f)
            else:
                mcp_config = {"mcpServers": {}}
            
            mcp_config["mcpServers"][request.name] = server_config
            
            # ä¿å­˜æ›´æ–°åçš„é…ç½®
            with open(mcp_config_path, 'w', encoding='utf-8') as f:
                json.dump(mcp_config, f, indent=4, ensure_ascii=False)

            # ä¹‹åè¦é€šè¿‡è¿™ä¸ªæ¥å£è·å–åˆ°æ³¨å†Œæƒ…å†µçš„è¯¦ç»†ä¿¡æ¯ï¼Œé‚£äº›tool æ³¨å†ŒæˆåŠŸäº†ï¼Œé‚£äº›toolæ²¡æœ‰æˆåŠŸã€‚        
        
            return {
                "status": "success",
                "message": f"MCP server {request.name} æ·»åŠ æˆåŠŸ",
                "server_name": request.name,
                "timestamp": time.time()
            }
        else:
            return {
                "status": "error",
                "message": f"MCP server {request.name} æ·»åŠ å¤±è´¥",
                "server_name": request.name,
                "timestamp": time.time()
            }
        
    except Exception as e:
        logger.error(f"æ·»åŠ MCP serverå¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"æ·»åŠ MCP serverå¤±è´¥: {str(e)}")

@app.post("/api/system-prompt/optimize", response_model=SystemPromptOptimizeResponse)
async def optimize_system_prompt(request: SystemPromptOptimizeRequest, response: Response):
    """ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰é»˜è®¤çš„æ¨¡å‹å®¢æˆ·ç«¯
        if not default_model_client:
            add_cors_headers(response)
            return SystemPromptOptimizeResponse(
                success=False,
                message="ç³»ç»Ÿæœªé…ç½®é»˜è®¤LLMæ¨¡å‹ï¼Œæ— æ³•è¿›è¡Œæç¤ºè¯ä¼˜åŒ–"
            )
        
        # åˆ›å»ºSystemPromptOptimizerå®ä¾‹
        optimizer = SystemPromptOptimizer()
        
        # æ‰§è¡Œä¼˜åŒ–
        result = await asyncio.to_thread(
            optimizer.optimize_system_prompt,
            request.original_prompt,
            default_model_client,
            server_args.default_llm_model_name,
            request.optimization_goal
        )
        
        # æå–ä¼˜åŒ–åçš„æç¤ºè¯
        optimized_prompt = result.get('optimized_prompt', '')
        
        add_cors_headers(response)
        return SystemPromptOptimizeResponse(
            success=True,
            message="ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–æˆåŠŸ",
            optimized_prompt=optimized_prompt,
            optimization_details={
                "original_length": len(request.original_prompt),
                "optimized_length": len(optimized_prompt),
                "optimization_goal": request.optimization_goal
            }
        )
        
    except Exception as e:
        logger.error(f"ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        add_cors_headers(response)
        return SystemPromptOptimizeResponse(
            success=False,
            message=f"ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–å¤±è´¥: {str(e)}"
        )

def get_agent_config_tools(availableTools):
    tools = []
    for tool_name in availableTools:
        for tool in tool_manager.list_tools():
            if tool['name'] == tool_name:
                tools.append(tool)
    return tools

@app.post("/api/evaluations/checkpoint_generation")
async def generate_checkpoints(request: StreamRequest, response: Response):
    """è°ƒç”¨CheckpointGenerationAgentç”Ÿæˆè¯„ä¼°æ£€æŸ¥ç‚¹"""
    add_cors_headers(response)
    if not request.messages:
        raise HTTPException(status_code=400, detail="user_messagesä¸èƒ½ä¸ºç©º")
    
    llm_config = request.llm_model_config or {}
    api_key = llm_config.get("api_key") or server_args.default_llm_api_key
    base_url = llm_config.get("base_url") or server_args.default_llm_api_base_url
    model_name = llm_config.get("model") or server_args.default_llm_model_name
    
    if not api_key or not base_url or not model_name:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦çš„æ¨¡å‹é…ç½®ï¼ˆapi_key/base_url/model_nameï¼‰")
    
    checkpoint_agent = CheckpointGenerationAgent(
        api_key=api_key,
        base_url=base_url,
    )
    try:
        result = await checkpoint_agent.workflow(
            user_messages=[
                            {"role": msg.role, "content": msg.content}
                            for msg in request.messages
                        ],
            agent_config=json.dumps(request.model_dump(), ensure_ascii=False),
            tools_description=json.dumps(get_agent_config_tools(request.available_tools), ensure_ascii=False),
            model_name=model_name,
        )
        return {
            "status": "success",
            "data": json.loads(result),
            "total_tokens": checkpoint_agent.get_total_tokens()
        }
    except json.JSONDecodeError:
        logger.warning(f"æ£€æŸ¥ç‚¹ç”Ÿæˆç»“æœä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {result}")
        raise HTTPException(status_code=400, detail="æ£€æŸ¥ç‚¹ç”Ÿæˆç»“æœä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œè¯·é‡æ–°è¯·æ±‚")
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")  


@app.post("/api/evaluations/score")
async def evaluate_agent_result(request: ScoreEvaluationRequest, response: Response):
    """è°ƒç”¨AgentScoreEvaluatorå¯¹Agentç»“æœè¿›è¡Œæ‰“åˆ†"""
    add_cors_headers(response)
    if not request.messages:
        raise HTTPException(status_code=400, detail="messagesä¸èƒ½ä¸ºç©º")
    
    llm_config = request.llm_model_config or {}
    api_key = llm_config.get("api_key") or server_args.default_llm_api_key
    base_url = llm_config.get("base_url") or server_args.default_llm_api_base_url
    model_name = llm_config.get("model") or server_args.default_llm_model_name
    
    if not api_key or not base_url or not model_name:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦çš„æ¨¡å‹é…ç½®ï¼ˆapi_key/base_url/model_nameï¼‰")
    
    evaluator = AgentScoreEvaluator(
        api_key=api_key,
        base_url=base_url,
    )
    
    try:
        evaluation_result = await evaluator.evaluate(
            agent_result=[
                            {"role": msg.role, "content": msg.content}
                            for msg in request.messages
                        ],
            agent_config=json.dumps(request.model_dump(), ensure_ascii=False),
            checkpoint=json.dumps(request.checkpoints, ensure_ascii=False),
            model_name=model_name,
        )
        return {
            "status": "success",
            "data": json.loads(evaluation_result),
            "total_tokens": evaluator.get_total_tokens()
        }
    except json.JSONDecodeError:
        logger.warning(f"è¯„ä¼°ç»“æœä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {evaluation_result}")
        raise HTTPException(status_code=400, detail="è¯„ä¼°ç»“æœä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œè¯·é‡æ–°è¯·æ±‚")

    except Exception as e:
        logger.error(f"è¯„ä¼°Agentç»“æœå¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"è¯„ä¼°Agentç»“æœå¤±è´¥: {str(e)}")


try:
    from fastapi.middleware.wsgi import WSGIMiddleware
    from wsgidav.wsgidav_app import WsgiDAVApp

    # é…ç½®æ–‡ä»¶å­˜å‚¨è·¯å¾„
    STORAGE_PATH = os.environ.get("HOST_WEBDAV_SERVER_ROOT") or './'
    os.makedirs(STORAGE_PATH, exist_ok=True)

    # é…ç½® WsgiDAV
    config = {
        "provider_mapping": {"/": STORAGE_PATH},
        "simple_dc": {"user_mapping": {"*": {"admin": {"password": "password"}}}},
        "verbose": 1,
        "lock_storage": True,
        "property_manager": True,
    }

    # åˆ›å»º WsgiDAV åº”ç”¨
    webdav_app = WsgiDAVApp(config)


    # å°† WebDAV æŒ‚è½½åˆ° /webdav è·¯å¾„
    if os.environ.get("ENABLE_DEBUG_WEBDAV"):
        app.mount("/webdav", WSGIMiddleware(webdav_app))
except Exception as e:
    logger.warning(f"WebDAV æŒ‚è½½å¤±è´¥: {str(e)}, è¯·æ£€æŸ¥ENABLE_DEBUG_WEBDAVç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®ä¸ºTrue")


if __name__ == "__main__":
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(server_args.logs_dir, exist_ok=True)
    os.makedirs(server_args.workspace, exist_ok=True)
    
    # å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼
    if server_args.daemon:
        import daemon
        import daemon.pidfile
        
        context = daemon.DaemonContext(
            working_directory=os.getcwd(),
            umask=0o002,
            pidfile=daemon.pidfile.TimeoutPIDLockFile(server_args.pid_file),
        )
        
        with context:
            uvicorn.run(
                app,
                host=server_args.host,
                port=server_args.port,
                log_level="debug",
                reload=False
            )
    else:
        uvicorn.run(
            app,
            host=server_args.host,
            port=server_args.port,
            log_level="debug",
            reload=False
        )

"""
æµå¼èŠå¤©æ¥å£è·¯ç”±æ¨¡å—
"""

import json
import uuid
import asyncio
import traceback
import time
from typing import Dict, Any, Optional, List, Union
from fastapi import APIRouter
from openai import OpenAI
from common.exceptions import SageHTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from service.sage_stream_service import SageStreamService
from config.settings import StartupConfig
from sagents.utils.logger import logger
import core.globals as global_vars
from models.conversation import ConversationDao

# åˆ›å»ºè·¯ç”±å™¨
stream_router = APIRouter()


class Message(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]]]


class StreamRequest(BaseModel):
    messages: List[Message]
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    deep_thinking: Optional[bool] = None
    max_loop_count: Optional[int] = None
    multi_agent: Optional[bool] = None
    more_suggest: Optional[bool] = None
    system_context: Optional[Dict[str, Any]] = None
    available_workflows: Optional[Dict[str, List[str]]] = None
    llm_model_config: Optional[Dict[str, Any]] = None
    system_prefix: Optional[str] = None
    available_tools: Optional[List[str]] = None
    force_summary: Optional[bool] = False
    agent_id: Optional[str] = None
    agent_name: Optional[str] = None

    def __init__(self, **data):
        super().__init__(**data)
        # ç¡®ä¿ messages ä¸­çš„æ¯ä¸ªæ¶ˆæ¯éƒ½æœ‰ role å’Œ content
        if self.messages:
            for i, msg in enumerate(self.messages):
                if isinstance(msg, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º Message å¯¹è±¡
                    self.messages[i] = Message(**msg)
                elif not hasattr(msg, "role") or not hasattr(msg, "content"):
                    raise ValueError(f"æ¶ˆæ¯ {i} ç¼ºå°‘å¿…è¦çš„ 'role' æˆ– 'content' å­—æ®µ")


def _clean_llm_model_config(llm_model_config: dict) -> dict:
    """æ¸…ç†LLMæ¨¡å‹é…ç½®ï¼Œç§»é™¤ç©ºå€¼"""
    if not llm_model_config:
        return {}
    return {k: v for k, v in llm_model_config.items() if v is not None and v != ""}


def _build_llm_model_config(request_config: dict, server_args: StartupConfig) -> dict:
    """æ„å»ºLLMæ¨¡å‹é…ç½®"""
    llm_model_config = {
        "model": request_config.get("model", server_args.default_llm_model_name)
    }

    # åªæœ‰åœ¨æœ‰æœ‰æ•ˆçš„max_tokenså€¼æ—¶æ‰æ·»åŠ è¯¥é”®ï¼Œé¿å…Noneå€¼å¯¼è‡´é”™è¯¯
    max_tokens_value = request_config.get(
        "max_tokens", server_args.default_llm_max_tokens
    )
    if max_tokens_value is not None:
        llm_model_config["max_tokens"] = int(max_tokens_value)

    # åªæœ‰åœ¨æœ‰æœ‰æ•ˆçš„temperatureå€¼æ—¶æ‰æ·»åŠ è¯¥é”®ï¼Œé¿å…Noneå€¼å¯¼è‡´é”™è¯¯
    temperature_value = request_config.get(
        "temperature", server_args.default_llm_temperature
    )
    if temperature_value is not None:
        llm_model_config["temperature"] = float(temperature_value)

    return llm_model_config


def _create_model_client(request_config: dict, server_args: StartupConfig):
    model_client = global_vars.get_default_model_client()
    if request_config:
        api_key = request_config.get("api_key", server_args.default_llm_api_key)
        base_url = request_config.get("base_url", server_args.default_llm_api_base_url)
        model_name = request_config.get("model", server_args.default_llm_model_name)
        logger.info(
            f"åˆå§‹åŒ–æ–°çš„æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ¨¡å‹é…ç½®api_key: {api_key}, base_url: {base_url}, model: {model_name}"
        )
        model_client = OpenAI(api_key=api_key, base_url=base_url)
        model_client.model = model_name
    return model_client


def _create_tool_proxy(request: StreamRequest):
    """åˆ›å»ºå·¥å…·ä»£ç†"""
    if not request.available_tools:
        return global_vars.get_tool_manager()

    logger.info(f"åˆå§‹åŒ–å·¥å…·ä»£ç†ï¼Œå¯ç”¨å·¥å…·: {request.available_tools}")

    # å¦‚æœrequest.multi_agent æ˜¯trueï¼Œè¦ç¡®ä¿request.available_toolsæ²¡æœ‰ complete_task è¿™ä¸ªå·¥å…·
    if request.multi_agent and "complete_task" in request.available_tools:
        request.available_tools.remove("complete_task")
    from sagents.tool.tool_proxy import ToolProxy

    tool_proxy = ToolProxy(global_vars.get_tool_manager(), request.available_tools)
    return tool_proxy


def _setup_stream_service(request: StreamRequest):
    """è®¾ç½®æµå¼æœåŠ¡ï¼Œè¿”å›(stream_service, session_id)"""
    session_id = request.session_id or str(uuid.uuid4())
    if session_id in global_vars.get_all_active_sessions_service_map():
        return SageHTTPException(
            status_code=500, detail="ä¼šè¯æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„ä¼šè¯ID"
        )
    # æ¸…ç†LLMæ¨¡å‹é…ç½®
    if request.llm_model_config:
        request.llm_model_config = _clean_llm_model_config(request.llm_model_config)
    server_args = global_vars.get_startup_config()
    model_client = _create_model_client(request.llm_model_config, server_args)
    llm_model_config = _build_llm_model_config(request.llm_model_config, server_args)
    max_model_len = request.llm_model_config.get(
        "max_model_len", server_args.default_llm_max_model_len
    )
    # åˆ›å»ºå·¥å…·ä»£ç†
    tool_proxy = _create_tool_proxy(request)
    """åˆ›å»ºæµå¼æœåŠ¡"""
    stream_service = SageStreamService(
        model=model_client,
        model_config=llm_model_config,
        tool_manager=tool_proxy,
        preset_running_config={"system_prefix": request.system_prefix},
        workspace=server_args.workspace,
        memory_root=server_args.memory_root,
        max_model_len=max_model_len,
    )

    all_active_sessions_service_map = global_vars.get_all_active_sessions_service_map()
    all_active_sessions_service_map[session_id] = {
        "stream_service": stream_service,
        "session_id": session_id,
    }
    global_vars.set_all_active_sessions_service_map(all_active_sessions_service_map)
    return stream_service, session_id


def _prepare_messages(request_messages):
    """å‡†å¤‡å’Œæ ¼å¼åŒ–æ¶ˆæ¯"""
    messages = []
    for msg in request_messages:
        # ä¿æŒåŸå§‹æ¶ˆæ¯çš„æ‰€æœ‰å­—æ®µ
        message_dict = msg.model_dump()
        # å…ˆåˆ¤æ–­åŸæ¶ˆæ¯æ˜¯å¦å­˜åœ¨message_idå­—æ®µï¼Œ ä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ä¸€ä¸ª
        if "message_id" not in message_dict or not message_dict["message_id"]:
            message_dict["message_id"] = str(uuid.uuid4())  # ä¸ºæ¯ä¸ªæ¶ˆæ¯ç”Ÿæˆå”¯ä¸€ID
        # å¦‚æœæœ‰content ä¸€å®šè¦è½¬åŒ–æˆstr
        if message_dict.get("content"):
            message_dict["content"] = str(message_dict["content"])
        messages.append(message_dict)
    return messages


def _initialize_message_collector(messages):
    """åˆå§‹åŒ–æ¶ˆæ¯æ”¶é›†å™¨"""
    message_collector = {}  # {message_id: merged_message}
    message_order = []  # ä¿æŒæ¶ˆæ¯çš„åŸå§‹é¡ºåº

    # å°†è¯·æ±‚çš„messagesæ·»åŠ åˆ°åˆå§‹åŒ–ä¸­
    for msg in messages:
        msg_id = msg["message_id"]
        message_collector[msg_id] = msg
        message_order.append(msg_id)

    return message_collector, message_order


def _update_message_collector(message_collector, message_order, result):
    """æ›´æ–°æ¶ˆæ¯æ”¶é›†å™¨"""
    if not isinstance(result, dict) or not result.get("message_id"):
        return

    message_id = result["message_id"]
    # å¦‚æœæ˜¯æ–°æ¶ˆæ¯ï¼Œåˆå§‹åŒ–
    if message_id not in message_collector:
        message_collector[message_id] = result
        message_order.append(message_id)
    else:
        # å¯¹äºå·¥å…·è°ƒç”¨ç»“æœæ¶ˆæ¯ï¼Œå®Œæ•´æ›¿æ¢è€Œä¸æ˜¯åˆå¹¶
        if result.get("role") != "tool":
            # åˆå¹¶contentå’Œshow_contentå­—æ®µï¼ˆè¿½åŠ ï¼‰
            if result.get("content"):
                message_collector[message_id]["content"] += str(result["content"])
            if result.get("show_content"):
                message_collector[message_id]["show_content"] += str(
                    result["show_content"]
                )


async def _send_chunked_json(result):
    """å‘é€åˆ†å—JSONæ•°æ®"""
    json_str = json.dumps(result, ensure_ascii=False)
    json_size = len(json_str)

    # å¯¹äºè¶…å¤§JSONï¼Œä½¿ç”¨åˆ†å—å‘é€ç¡®ä¿å®Œæ•´æ€§
    if json_size > 32768:  # 32KBä»¥ä¸Šä½¿ç”¨åˆ†å—å‘é€
        logger.info(f"ğŸ”„ å¤§JSONåˆ†å—å‘é€: {json_size} å­—ç¬¦")

        # åˆ†å—å‘é€å¤§JSON
        chunk_size = 8192  # 8KB per chunk
        total_chunks = (json_size + chunk_size - 1) // chunk_size

        # å‘é€åˆ†å—å¼€å§‹æ ‡è®°
        start_marker = {
            "type": "chunk_start",
            "message_id": result.get("message_id", "unknown"),
            "total_size": json_size,
            "total_chunks": total_chunks,
            "chunk_size": chunk_size,
            "original_type": result.get("type", "unknown"),
        }
        yield json.dumps(start_marker, ensure_ascii=False) + "\n"
        await asyncio.sleep(0.01)  # å»¶è¿Ÿç¡®ä¿å‰ç«¯å‡†å¤‡å¥½

        for i in range(total_chunks):
            start = i * chunk_size
            end = min(start + chunk_size, json_size)
            chunk_data = json_str[start:end]

            # åˆ›å»ºåˆ†å—æ¶ˆæ¯
            chunk_message = {
                "type": "json_chunk",
                "message_id": result.get("message_id", "unknown"),  # æ·»åŠ message_idå­—æ®µ
                "chunk_id": f"{result.get('message_id', 'unknown')}_{i}",
                "chunk_index": i,
                "total_chunks": total_chunks,
                "chunk_data": chunk_data,
                "chunk_size": len(chunk_data),
                "is_final": i == total_chunks - 1,
                "checksum": hash(chunk_data) % 1000000,
            }

            yield json.dumps(chunk_message, ensure_ascii=False) + "\n"
            await asyncio.sleep(0.005)  # é€‚ä¸­å»¶è¿Ÿç¡®ä¿é¡ºåº

        # å‘é€åˆ†å—ç»“æŸæ ‡è®°
        end_marker = {
            "type": "chunk_end",
            "message_id": result.get("message_id", "unknown"),
            "total_chunks": total_chunks,
            "expected_size": json_size,
            "original_type": result.get("type", "unknown"),
        }
        yield json.dumps(end_marker, ensure_ascii=False) + "\n"

        logger.info(f"âœ… å®Œæˆåˆ†å—å‘é€: {total_chunks} å—")
    else:
        # å°JSONç›´æ¥å‘é€
        yield json.dumps(result, ensure_ascii=False) + "\n"


async def _create_conversation_title(request):
    """åˆ›å»ºä¼šè¯æ ‡é¢˜"""
    if not request.messages or len(request.messages) == 0:
        return "æ–°ä¼šè¯"

    # ä½¿ç”¨ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
    first_message = request.messages[0].content
    if isinstance(first_message, str):
        conversation_title = (
            first_message[:50] + "..." if len(first_message) > 50 else first_message
        )
    elif isinstance(first_message, list) and len(first_message) > 0:
        # å¦‚æœæ˜¯å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œå°è¯•æå–æ–‡æœ¬å†…å®¹
        for item in first_message:
            if isinstance(item, dict) and item.get("type") == "text":
                text_content = item.get("text", "")
                conversation_title = (
                    text_content[:50] + "..."
                    if len(text_content) > 50
                    else text_content
                )
                break
        else:
            conversation_title = "å¤šæ¨¡æ€æ¶ˆæ¯"
    else:
        conversation_title = "æ–°ä¼šè¯"

    return conversation_title


async def _save_conversation_if_needed(
    session_id, request, message_collector, message_order
):
    conversation_dao = ConversationDao()
    """å¦‚æœéœ€è¦ï¼Œä¿å­˜æ–°ä¼šè¯"""
    messages = []
    existing_conversation = await conversation_dao.get_by_session_id(session_id)
    if not existing_conversation:
        conversation_title = await _create_conversation_title(request)
        await conversation_dao.save_conversation(
            user_id=request.user_id or "default_user",
            agent_id=request.agent_id or "default_agent",
            agent_name=request.agent_name or "Sage Assistant",
            messages=[],
            session_id=session_id,
            title=conversation_title,
        )
        logger.info(f"åˆ›å»ºæ–°ä¼šè¯: {session_id}, æ ‡é¢˜: {conversation_title}")
    else:
        messages = existing_conversation.messages
    for message_id in message_order:
        if message_id in message_collector:
            merged_message = message_collector[message_id]
            # æ·»åŠ æ¶ˆæ¯åˆ°conversation
            messages.append(merged_message)
    await conversation_dao.update_conversation_messages(session_id, messages)
    logger.info(
        f"æˆåŠŸæŒ‰é¡ºåºä¿å­˜ {len(message_collector)} æ¡æ¶ˆæ¯åˆ°ç°æœ‰conversation {session_id}"
    )


@stream_router.post("/api/stream")
async def stream_chat(request: StreamRequest):
    """æµå¼èŠå¤©æ¥å£"""
    if not global_vars.get_default_model_client():
        raise SageHTTPException(
            status_code=503,
            detail="æ¨¡å‹å®¢æˆ·ç«¯æœªé…ç½®æˆ–ä¸å¯ç”¨",
            error_detail="Model client is not configured or unavailable",
        )
    # éªŒè¯è¯·æ±‚å‚æ•°
    if not request.messages or len(request.messages) == 0:
        raise SageHTTPException(status_code=400, detail="æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

    logger.info(f"Server: è¯·æ±‚å‚æ•°: {request}")
    # è®¾ç½®æµå¼æœåŠ¡
    stream_service, session_id = _setup_stream_service(request)

    # ç”Ÿæˆæµå¼å“åº”
    async def generate_stream():
        """ç”ŸæˆSSEæµ"""
        try:
            # å‡†å¤‡å’Œæ ¼å¼åŒ–æ¶ˆæ¯
            messages = _prepare_messages(request.messages)

            logger.info(f"å¼€å§‹æµå¼å¤„ç†ï¼Œä¼šè¯ID: {session_id}")

            # æ·»åŠ æµå¤„ç†è®¡æ•°å™¨å’Œè¿æ¥çŠ¶æ€è·Ÿè¸ª
            stream_counter = 0
            last_activity_time = time.time()

            # åˆå§‹åŒ–æ¶ˆæ¯æ”¶é›†å™¨
            message_collector, message_order = _initialize_message_collector(messages)

            # å¤„ç†æµå¼å“åº”ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°
            async for result in stream_service.process_stream(
                messages=messages,
                session_id=session_id,
                user_id=request.user_id,
                deep_thinking=request.deep_thinking,
                max_loop_count=request.max_loop_count,
                multi_agent=request.multi_agent,
                more_suggest=request.more_suggest,
                system_context=request.system_context,
                available_workflows=request.available_workflows,
                force_summary=request.force_summary,
            ):
                # æ›´æ–°æµå¤„ç†è®¡æ•°å™¨å’Œæ´»åŠ¨æ—¶é—´
                stream_counter += 1
                current_time = time.time()
                time_since_last = current_time - last_activity_time
                last_activity_time = current_time

                # æ¯100ä¸ªç»“æœè®°å½•ä¸€æ¬¡è¿æ¥çŠ¶æ€
                if stream_counter % 100 == 0:
                    logger.info(
                        f"ğŸ“Š æµå¤„ç†çŠ¶æ€ - ä¼šè¯: {session_id}, è®¡æ•°: {stream_counter}, é—´éš”: {time_since_last:.3f}s"
                    )

                # æ›´æ–°æ¶ˆæ¯æ”¶é›†å™¨
                _update_message_collector(message_collector, message_order, result)

                # å¤„ç†JSONä¼ è¾“ï¼ˆåˆ†å—æˆ–ç›´æ¥å‘é€ï¼‰
                try:
                    async for chunk in _send_chunked_json(result):
                        yield chunk
                except Exception as e:
                    logger.error(f"JSONåºåˆ—åŒ–å¤±è´¥: {e}")
                    # åˆ›å»ºé”™è¯¯å“åº”
                    error_data = {
                        "type": "error",
                        "message_id": result.get("message_id", "error"),
                        "content": f"æ•°æ®å¤„ç†é”™è¯¯: {str(e)}",
                        "original_size": len(str(result)),
                        "error": True,
                    }
                    yield json.dumps(error_data, ensure_ascii=False) + "\n"

                await asyncio.sleep(0.01)  # é¿å…è¿‡å¿«å‘é€

            # å‘é€æµç»“æŸæ ‡è®°
            end_data = {
                "type": "stream_end",
                "session_id": session_id,
                "timestamp": time.time(),
                "total_stream_count": stream_counter,
            }
            total_duration = time.time() - (
                last_activity_time - time_since_last
                if "time_since_last" in locals()
                else last_activity_time
            )
            logger.info(
                f"âœ… å®Œæˆæµå¼å¤„ç†: ä¼šè¯ {session_id}, æ€»è®¡ {stream_counter} ä¸ªæµç»“æœ, è€—æ—¶ {total_duration:.3f}s"
            )
            yield json.dumps(end_data, ensure_ascii=False) + "\n"

            # ä¿å­˜ä¼šè¯å’Œæ¶ˆæ¯åˆ°æ•°æ®åº“
            await _save_conversation_if_needed(
                session_id, request, message_collector, message_order
            )

        except GeneratorExit as ge:
            import sys

            disconnect_msg = f"ğŸ”Œ [GENERATOR_EXIT] å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œç”Ÿæˆå™¨è¢«å…³é—­ - ä¼šè¯ID: {session_id}, æ—¶é—´: {time.time()}"
            logger.error(disconnect_msg)
            logger.error(
                f"ğŸ“Š [GENERATOR_EXIT] æµå¤„ç†ç»Ÿè®¡: å·²å¤„ç† {stream_counter if 'stream_counter' in locals() else 0} ä¸ªæµç»“æœ"
            )
            # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—ç¼“å†²åŒº
            sys.stderr.flush()

        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¼‚å¸¸: {e}")
            logger.error(traceback.format_exc())
            error_data = {"type": "error", "message": str(e), "session_id": session_id}
            yield json.dumps(error_data, ensure_ascii=False) + "\n"
        finally:
            logger.info("æµå¤„ç†ç»“æŸï¼Œæ¸…ç†ä¼šè¯èµ„æº")
            # æ¸…ç†ä¼šè¯èµ„æº
            all_active_sessions_service_map = (
                global_vars.get_all_active_sessions_service_map()
            )
            if session_id in all_active_sessions_service_map:
                del all_active_sessions_service_map[session_id]
            logger.info(f"ä¼šè¯ {session_id} èµ„æºå·²æ¸…ç†")

    return StreamingResponse(generate_stream(), media_type="text/plain")
